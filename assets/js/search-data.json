{
  
    
        "post0": {
            "title": "Improving quantization by normalizing weights",
            "content": "Introduction . I have hidden all the code from the previous post so we can focus on the experiments but you can find it if you download the notebook or open it in colab. . Collecting the Dataset . Visualize the dataset . Remember the model class definition. . class CNN(nn.Module): def __init__(self, dropout_rate=0.0, norm_w=True): super(CNN, self).__init__() self.norm_w = norm_w # Create the layers normalizing or not the weigths if self.norm_w: self.conv_layer1 = nn.utils.weight_norm(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3), name=&#39;weight&#39;) self.conv_layer2 = nn.utils.weight_norm(nn.Conv2d(32, 64, 3), name=&#39;weight&#39;) self.fc = nn.utils.weight_norm(nn.Linear(in_features= 64*5*5, out_features=10), name=&#39;weight&#39;) else: self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) self.conv_layer2 = nn.Conv2d(32, 64, 3) self.fc = nn.Linear(in_features= 64*5*5, out_features=10) # input dimensions are Bx1x28x28 (BxCxHxW) self.batch_norm1 = nn.BatchNorm2d(32) self.relu = nn.ReLU() self.pool = nn.MaxPool2d(kernel_size=2,stride=2) self.batch_norm2 = nn.BatchNorm2d(64) self.flat = nn.Flatten() self.drop = nn.Dropout(p=dropout_rate) def forward(self, x): # Block 1 out = self.conv_layer1(x) out = self.batch_norm1(out) out = self.relu(out) out = self.pool(out) # Block 2 out = self.conv_layer2(out) out = self.batch_norm2(out) out = self.relu(out) out = self.pool(out) # Flatten the output using BxC*H*W out = self.flat(out) out = self.drop(out) out = self.fc(out) return out def add_quant(self): &#39;&#39;&#39; Returns a new model with added quantization layers &#39;&#39;&#39; return nn.Sequential(torch.quantization.QuantStub(), self, torch.quantization.DeQuantStub()) . Training without PyTorch Model Quantization . Helper functions for Pytorch Quantizantion and evaluation . Chain everything together in a single train function . Experiments . Let&#39;s simulate that it is our first time facing this problem, so we haven&#39;t thought about normalizing the weights. . model_15, _ = train_model_and_quantize(epochs=15, norm_w=False) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 2.53 s - Loss: 0.2702 - Categorical_Accuracy: 0.9447 - Val_Loss: 0.0466 - Categorical_Val_Accuracy: 0.9848 Epoch 2 Time: 2.21 s - Loss: 0.0523 - Categorical_Accuracy: 0.9837 - Val_Loss: 0.0416 - Categorical_Val_Accuracy: 0.9869 Epoch 3 Time: 2.21 s - Loss: 0.0358 - Categorical_Accuracy: 0.9885 - Val_Loss: 0.0286 - Categorical_Val_Accuracy: 0.9904 Epoch 4 Time: 2.20 s - Loss: 0.0291 - Categorical_Accuracy: 0.9907 - Val_Loss: 0.0337 - Categorical_Val_Accuracy: 0.9888 Epoch 5 Time: 2.20 s - Loss: 0.0250 - Categorical_Accuracy: 0.9920 - Val_Loss: 0.0335 - Categorical_Val_Accuracy: 0.9909 Epoch 6 Time: 2.22 s - Loss: 0.0204 - Categorical_Accuracy: 0.9937 - Val_Loss: 0.0306 - Categorical_Val_Accuracy: 0.9898 Epoch 7 Time: 2.26 s - Loss: 0.0183 - Categorical_Accuracy: 0.9945 - Val_Loss: 0.0419 - Categorical_Val_Accuracy: 0.9863 Epoch 8 Time: 2.23 s - Loss: 0.0168 - Categorical_Accuracy: 0.9946 - Val_Loss: 0.0379 - Categorical_Val_Accuracy: 0.9900 Epoch 9 Time: 2.23 s - Loss: 0.0176 - Categorical_Accuracy: 0.9941 - Val_Loss: 0.0436 - Categorical_Val_Accuracy: 0.9882 Epoch 10 Time: 2.23 s - Loss: 0.0159 - Categorical_Accuracy: 0.9945 - Val_Loss: 0.0376 - Categorical_Val_Accuracy: 0.9889 Epoch 11 Time: 2.21 s - Loss: 0.0159 - Categorical_Accuracy: 0.9949 - Val_Loss: 0.0433 - Categorical_Val_Accuracy: 0.9881 Epoch 12 Time: 2.21 s - Loss: 0.0132 - Categorical_Accuracy: 0.9954 - Val_Loss: 0.0428 - Categorical_Val_Accuracy: 0.9894 Epoch 13 Time: 2.21 s - Loss: 0.0143 - Categorical_Accuracy: 0.9956 - Val_Loss: 0.0443 - Categorical_Val_Accuracy: 0.9884 Epoch 14 Time: 2.22 s - Loss: 0.0113 - Categorical_Accuracy: 0.9962 - Val_Loss: 0.0397 - Categorical_Val_Accuracy: 0.9896 Epoch 15 Time: 2.23 s - Loss: 0.0132 - Categorical_Accuracy: 0.9961 - Val_Loss: 0.0371 - Categorical_Val_Accuracy: 0.9895 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9895 Eval time: 5.13s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.6951 Eval time: 6.34s . As we can see, the drop in the performance of the quantized model is remarkable. What might have happened? Why the results are way better when using Keras? . We could try training for just 1 epoch and see what happend . model_1, _ = train_model_and_quantize(epochs=1, norm_w=False) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 2.34 s - Loss: 0.2260 - Categorical_Accuracy: 0.9505 - Val_Loss: 0.0466 - Categorical_Val_Accuracy: 0.9838 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9838 Eval time: 5.25s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.9424 Eval time: 6.03s . The result of the quantization is better now. What is happening? . The first thing that comes to my mind is taking a look at the weightsüîç . import matplotlib.pyplot as plt import numpy as np %matplotlib inline def vis_model(model): w = [] for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): w.extend(np.array(m.weight.data.cpu()).flatten()) plt.hist(w, density=True, bins=128) plt.ylabel(&#39;Ocurrences&#39;) plt.xlabel(&#39;Weight&#39;) . vis_model(model_15) . vis_model(model_1) . So the weights of the model trained for 15 epochs are larger, may that be what is causing the problem? . We can see how weights are distributed in the Keras model and we will see the wights are smaller (more similar to the model trained for one epoch). . . The explanation may be that with larger weights the minimum and the maximum are further apart so the &quot;resolution&quot; of the quantization is worse. One quick fix could be clamping the weights, so they can not be larger or smaller than a certain threshold. But this will make that our model just learn in the few first iterations. It will produce a histogram like this one: . . A better option would be normalizing the weights so let&#39;s try it. . model_15_norm, _ = train_model_and_quantize(epochs=15, norm_w=True) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 2.44 s - Loss: 0.1988 - Categorical_Accuracy: 0.9498 - Val_Loss: 0.0593 - Categorical_Val_Accuracy: 0.9806 Epoch 2 Time: 2.36 s - Loss: 0.0509 - Categorical_Accuracy: 0.9848 - Val_Loss: 0.0391 - Categorical_Val_Accuracy: 0.9870 Epoch 3 Time: 2.51 s - Loss: 0.0383 - Categorical_Accuracy: 0.9880 - Val_Loss: 0.0392 - Categorical_Val_Accuracy: 0.9871 Epoch 4 Time: 2.34 s - Loss: 0.0325 - Categorical_Accuracy: 0.9901 - Val_Loss: 0.0304 - Categorical_Val_Accuracy: 0.9897 Epoch 5 Time: 2.37 s - Loss: 0.0279 - Categorical_Accuracy: 0.9909 - Val_Loss: 0.0394 - Categorical_Val_Accuracy: 0.9877 Epoch 6 Time: 2.39 s - Loss: 0.0237 - Categorical_Accuracy: 0.9928 - Val_Loss: 0.0339 - Categorical_Val_Accuracy: 0.9896 Epoch 7 Time: 2.33 s - Loss: 0.0208 - Categorical_Accuracy: 0.9933 - Val_Loss: 0.0284 - Categorical_Val_Accuracy: 0.9899 Epoch 8 Time: 2.34 s - Loss: 0.0171 - Categorical_Accuracy: 0.9948 - Val_Loss: 0.0386 - Categorical_Val_Accuracy: 0.9869 Epoch 9 Time: 2.35 s - Loss: 0.0154 - Categorical_Accuracy: 0.9950 - Val_Loss: 0.0476 - Categorical_Val_Accuracy: 0.9849 Epoch 10 Time: 2.34 s - Loss: 0.0147 - Categorical_Accuracy: 0.9952 - Val_Loss: 0.0313 - Categorical_Val_Accuracy: 0.9903 Epoch 11 Time: 2.34 s - Loss: 0.0116 - Categorical_Accuracy: 0.9961 - Val_Loss: 0.0375 - Categorical_Val_Accuracy: 0.9885 Epoch 12 Time: 2.35 s - Loss: 0.0124 - Categorical_Accuracy: 0.9960 - Val_Loss: 0.0399 - Categorical_Val_Accuracy: 0.9896 Epoch 13 Time: 2.34 s - Loss: 0.0099 - Categorical_Accuracy: 0.9969 - Val_Loss: 0.0283 - Categorical_Val_Accuracy: 0.9918 Epoch 14 Time: 2.36 s - Loss: 0.0083 - Categorical_Accuracy: 0.9973 - Val_Loss: 0.0290 - Categorical_Val_Accuracy: 0.9913 Epoch 15 Time: 2.43 s - Loss: 0.0067 - Categorical_Accuracy: 0.9977 - Val_Loss: 0.0337 - Categorical_Val_Accuracy: 0.9907 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9907 Eval time: 5.51s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.9838 Eval time: 6.13s . We get better results! Lower loss in performance while being able to reduce significantly the size of the model. . And as we can see the weights are now normalized. . vis_model(model_15_norm) . Conclusion . After these experiments, the main takeaways would be the following: . We can look at the network&#39;s weights and see what is happening. It is a good practice and provides some understanding of how the model is being/has being trained. . | The values that our weight take are really important in quantization. . | We have to be careful with the weights not getting too large in our network. As we reward the outputs with higher values is feasible that the model tries to make its weights as big as possible. . | Normalizing the weight can lead to a better quantized models. Nevertheless, it can increase the training time with the same number of epochs. But it often converges faster, so fewer epochs are needed. You will have to study your problem carefully. No one said it was easy! . | .",
            "url": "https://xanaga.github.io/posts_en/quantization/efficient_deeplearning/2022/05/30/Improving_quantization_by_normalizing_weights.html",
            "relUrl": "/quantization/efficient_deeplearning/2022/05/30/Improving_quantization_by_normalizing_weights.html",
            "date": " ‚Ä¢ May 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Quantizing a Deep Learning Model",
            "content": "Introduction . As the description says introduced, this is a version of a codelab from the &quot;Efficient Deep Learning&quot; book. The book uses TensorFlow so I thought it would be a good idea to try to implement the same ideas with PyTorch. This has the two main objectives: . Encourage others who are more familiar with PyTorch to read the book, as I find efficient deep learning a very important topic. Also, I think the work done by the authors is outstanding so I recommend it to everyone. . | Allow me to practice the concepts of chapter 2. . | . So I will try to make things as similar as possible to the original notebook in case anyone wants to use it to follow the book. I will try not to repeat the explanations made in that notebook and focuse on explaining the changes regarding the PyTorch implementation. . Moreover, there is another post that explores some difficulties I found while doing the PyTorch implementation, so I encourage you to read it as well. . Collecting the Dataset . import numpy as np import time import os import torch import torchvision from torch import nn, optim from torchsummary import summary from tqdm.notebook import tqdm import torch.utils.data as data_utils def process_x(x): &quot;&quot;&quot;Process the given tensors of images.&quot;&quot;&quot; x = x.type(torch.float32) # The original data is in [0.0, 255.0]. # This normalization helps in making them lie between [-1.0, 1.0]. x /= 127.5 x -= 1.0 # Now we have [NxWxH], add one dimension for the channels # We add it to dim 1 because Pytorch image format is [CxWxH] x = torch.unsqueeze(x, dim=1) return x def load_data(ds=torchvision.datasets.MNIST): &quot;&quot;&quot;Returns the processed dataset.&quot;&quot;&quot; training, test = ds(&#39;/data/mnist&#39;, download=True), ds(&#39;/data/mnist&#39;, download=True, train=False) train_images, train_labels = process_x(training.data), training.targets test_images, test_labels = process_x(test.data), test.targets return (train_images, train_labels), (test_images, test_labels) (train_x, train_y), (test_x, test_y) = load_data() . We will have to specify which device we want to use: . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; . Visualize the dataset . The part of visualising the dataset will be the same as in the original notebook. . %matplotlib inline import matplotlib.pyplot as plt def collect_samples(x, y, num_classes=10, num_per_class=5): sampled_x = None sampled_x_idx = 0 for class_idx in range(num_classes): num_collected = 0 for idx in range(len(x)): if num_collected == num_per_class: break if y[idx] != class_idx: continue if sampled_x is not None: sampled_x = np.concatenate((sampled_x, np.expand_dims(x[idx], 0))) else: sampled_x = np.expand_dims(x[idx], 0) sampled_x_idx = sampled_x_idx + 1 num_collected = num_collected + 1 return sampled_x def show_images(images, num_rows = 1, titles = None): &quot;&quot;&quot;Display a list of images in a single figure with matplotlib. &quot;&quot;&quot; # assert((titles is None)or (len(images) == len(titles))) num_images = len(images) num_cols = num_images // num_rows # plot images fig, axes = plt.subplots(num_rows, num_cols, figsize=(1.25 * num_rows, 1.75 * num_cols)) for i in range(num_images): ax = axes[i%num_cols, i//num_cols] ax.axis(&#39;off&#39;) ax.imshow(np.squeeze(images[i]), cmap=&#39;Greys&#39;) # ax.add_axes((0, 0, 1, 1)) if i % num_cols == 0: ax.set_title(&#39;{}&#39;.format(int(i / num_rows))) # plt.tight_layout() plt.show() show_images(collect_samples(train_x, train_y, num_per_class=10), num_rows=10) . . For creating the model we will use a class. Using Sequential() would be more similar to how it is done with Keras, but I think using the class will allow for more flexibility. . You will see a norm_w argument passed to the constructor. It is used for normalizing the weights of the layers, as it improves significantly the effectivity of the quantization. This is further detailed in the complementary post. . class CNN(nn.Module): def __init__(self, dropout_rate=0.0, norm_w=True): super(CNN, self).__init__() self.norm_w = norm_w # Create the layers normalizing or not the weigths if self.norm_w: self.conv_layer1 = nn.utils.weight_norm(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3), name=&#39;weight&#39;) self.conv_layer2 = nn.utils.weight_norm(nn.Conv2d(32, 64, 3), name=&#39;weight&#39;) self.fc = nn.utils.weight_norm(nn.Linear(in_features= 64*5*5, out_features=10), name=&#39;weight&#39;) else: self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) self.conv_layer2 = nn.Conv2d(32, 64, 3) self.fc = nn.Linear(in_features= 64*5*5, out_features=10) # input dimensions are Bx1x28x28 (BxCxHxW) self.batch_norm1 = nn.BatchNorm2d(32) self.relu = nn.ReLU() self.pool = nn.MaxPool2d(kernel_size=2,stride=2) self.batch_norm2 = nn.BatchNorm2d(64) self.flat = nn.Flatten() self.drop = nn.Dropout(p=dropout_rate) def forward(self, x): # Block 1 out = self.conv_layer1(x) out = self.batch_norm1(out) out = self.relu(out) out = self.pool(out) # Block 2 out = self.conv_layer2(out) out = self.batch_norm2(out) out = self.relu(out) out = self.pool(out) # Flatten the output using BxC*H*W out = self.flat(out) out = self.drop(out) out = self.fc(out) return out def add_quant(self): &#39;&#39;&#39; Returns a new model with added quantization layers &#39;&#39;&#39; return nn.Sequential(torch.quantization.QuantStub(), self, torch.quantization.DeQuantStub()) . toy_model = CNN() summary(toy_model, input_size=(1,28,28),device=&quot;cpu&quot;) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - . Quick Notes: . The BatchNorm2d in PyTorch appears as it has half of the parameters than the Keras one. This is because Keras count the accumulated means and accumulated std as non-trainable parameters, while Pytorch simply hide them. So PyTorch only counts as Batch Normalization parmeters the scale and the offset. This also explains the difference in the 192 non-trainable parameters. | Forward/backward pass size (MB) refears to the expected memory to run the model. | . Training without PyTorch Model Quantization . As PyTorch does not have a training loop using .fit() like Keras does, we will have to implement our own. It is not a plain PyTorch traiining loop as we will print some metrics and save them for the later plots. . Note:This could be done in fewer lines of code using Fastai or PyTorch Lightning, and for tracking is better to use platforms such as Weights &amp; Biases. But as is not the point of this post we will continue as it is. . def train_model(model, epochs, bz=128, device=&#39;cuda&#39;): # Save the model training information model_history = {&quot;loss&quot;:[], &quot;sparse_categorical_accuracy&quot;: [], &quot;val_loss&quot;:[], &quot;val_sparse_categorical_accuracy&quot;:[]} # Show the model summary model.to(device) summary(model, input_size=(1,28,28), device=device) # Define the training hyperparameters optimizer = optim.Adam(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() # Create the training and validation dataloders train = data_utils.TensorDataset(train_x, train_y) train_loader = data_utils.DataLoader(train, batch_size=bz, shuffle=True) val = data_utils.TensorDataset(test_x, test_y) val_loader = data_utils.DataLoader(val, batch_size=bz, shuffle=False) for epoch in range(epochs): # loop over the number of epochs start = time.time() print(f&#39;Epoch {epoch+1}&#39;) train_loss = 0.0 train_correct = 0.0 val_loss = 0.0 val_correct = 0.0 # Loop over the trainig set for data in tqdm(train_loader): # Move the data to the corresponding device x, y = data x, y = x.to(device), y.to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(x) loss = criterion(outputs, y) loss.backward() optimizer.step() train_loss += outputs.shape[0] * loss.item() train_correct += (outputs.argmax(1) == y).detach().cpu().sum() # Log the training loss and metrics train_loss = train_loss/float(len(train_x)) train_correct = train_correct/float(len(train_x)) model_history[&quot;loss&quot;].append(train_loss) model_history[&quot;sparse_categorical_accuracy&quot;].append(train_correct) #Loop over the validation set: model.eval() with torch.inference_mode(): for data in val_loader: x, y = data x, y = x.to(device), y.to(device) outputs = model(x) loss = criterion(outputs, y) val_loss += outputs.shape[0] * loss.item() val_correct += (outputs.argmax(1) == y).detach().cpu().sum() # Log the training loss and metrics val_loss = val_loss/float(len(test_x)) val_correct = val_correct/float(len(test_x)) model_history[&quot;val_loss&quot;].append(val_loss) model_history[&quot;val_sparse_categorical_accuracy&quot;].append(val_correct) model.train() end = time.time() # Print information for the epoch print(f&quot;Time: {(end-start):.2f} s - &quot;, f&quot;Loss: {train_loss:.4f} - Categorical_Accuracy: {train_correct:.4f} - &quot;, f&quot;Val_Loss: {val_loss:.4f} - Categorical_Val_Accuracy: {val_correct:.4f}&quot;) print(&#39;Finished Training&#39;) return model, model_history . no_quant = CNN() basic_mnist_model, basic_mnist_model_history = train_model(no_quant, epochs=15, device=device) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 4.67 s - Loss: 0.1699 - Categorical_Accuracy: 0.9557 - Val_Loss: 0.0609 - Categorical_Val_Accuracy: 0.9810 Epoch 2 Time: 4.54 s - Loss: 0.0498 - Categorical_Accuracy: 0.9845 - Val_Loss: 0.0487 - Categorical_Val_Accuracy: 0.9865 Epoch 3 Time: 4.56 s - Loss: 0.0388 - Categorical_Accuracy: 0.9876 - Val_Loss: 0.0431 - Categorical_Val_Accuracy: 0.9877 Epoch 4 Time: 4.55 s - Loss: 0.0300 - Categorical_Accuracy: 0.9909 - Val_Loss: 0.0472 - Categorical_Val_Accuracy: 0.9855 Epoch 5 Time: 4.57 s - Loss: 0.0245 - Categorical_Accuracy: 0.9925 - Val_Loss: 0.0317 - Categorical_Val_Accuracy: 0.9903 Epoch 6 Time: 4.55 s - Loss: 0.0226 - Categorical_Accuracy: 0.9930 - Val_Loss: 0.0365 - Categorical_Val_Accuracy: 0.9879 Epoch 7 Time: 4.52 s - Loss: 0.0186 - Categorical_Accuracy: 0.9943 - Val_Loss: 0.0333 - Categorical_Val_Accuracy: 0.9891 Epoch 8 Time: 4.52 s - Loss: 0.0165 - Categorical_Accuracy: 0.9949 - Val_Loss: 0.0339 - Categorical_Val_Accuracy: 0.9899 Epoch 9 Time: 4.55 s - Loss: 0.0143 - Categorical_Accuracy: 0.9952 - Val_Loss: 0.0352 - Categorical_Val_Accuracy: 0.9893 Epoch 10 Time: 4.55 s - Loss: 0.0122 - Categorical_Accuracy: 0.9964 - Val_Loss: 0.0273 - Categorical_Val_Accuracy: 0.9915 Epoch 11 Time: 4.62 s - Loss: 0.0109 - Categorical_Accuracy: 0.9964 - Val_Loss: 0.0298 - Categorical_Val_Accuracy: 0.9908 Epoch 12 Time: 4.54 s - Loss: 0.0089 - Categorical_Accuracy: 0.9971 - Val_Loss: 0.0278 - Categorical_Val_Accuracy: 0.9919 Epoch 13 Time: 4.69 s - Loss: 0.0082 - Categorical_Accuracy: 0.9973 - Val_Loss: 0.0365 - Categorical_Val_Accuracy: 0.9893 Epoch 14 Time: 5.18 s - Loss: 0.0082 - Categorical_Accuracy: 0.9972 - Val_Loss: 0.0389 - Categorical_Val_Accuracy: 0.9889 Epoch 15 Time: 5.89 s - Loss: 0.0075 - Categorical_Accuracy: 0.9975 - Val_Loss: 0.0309 - Categorical_Val_Accuracy: 0.9915 Finished Training . plt.plot( basic_mnist_model_history[&#39;sparse_categorical_accuracy&#39;], linestyle=&#39;dashed&#39;) plt.plot( basic_mnist_model_history[&#39;val_sparse_categorical_accuracy&#39;], linestyle=&#39;dotted&#39;) plt.title(&#39;Model Accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;Train Accuracy&#39;, &#39;Test Accuracy&#39;], loc=&#39;lower right&#39;) plt.show() . Helper functions for Pytorch Quantizantion and evaluation . These two functions will compute the size of our model. As it is mentioned in the book, it is an important metric to take into account when evaluanting models (not just the score). . Note:We won&#39;t use the first one, it is just informative, you can use it for non-quantized models. It is an aproximation and will be a bit lower than the real size. . def model_size(model): param_size = 0 for param in model.parameters(): param_size += param.nelement() * param.element_size() buffer_size = 0 for buffer in model.buffers(): buffer_size += buffer.nelement() * buffer.element_size() model_size = (param_size + buffer_size) / 1024 def real_model_size(model): torch.save(model.state_dict(), &quot;temp.p&quot;) size = os.path.getsize(&quot;temp.p&quot;)/1024 os.remove(&#39;temp.p&#39;) return size . The following evaluation functions use the same logic as the ones presented in the book. . def pytorch_model_eval(model, test_images, test_labels, quantized): &quot;&quot;&quot;Evaluate the generated model.&quot;&quot;&quot; num_correct = 0 num_total = 0 model.eval() with torch.inference_mode(): for idx in range(len(test_images)): num_total = num_total + 1 if quantized: input = test_images[idx:idx+1].to(&quot;cpu&quot;) else: input = test_images[idx:idx+1].to(device) output = model(input) # The returned output is a tensor of logits, so we find the maximum in that # tensor, and see if it matches the label. if output.argmax(1) == test_labels[idx]: num_correct = num_correct + 1 print(&#39;Accuracy:&#39;, num_correct * 1.0 / num_total) . The quantize_and_eval function includes the code needed for quantizing the model, which is inspired in the official PyTorch post which I recommend reading. . To do so we do the following steps: . If present, remove weight normalization from the layers as is not supported in quantization. . | Change the model to the CPU and put it into evaluation mode. . | Set the backend to fbgemm to run the model on a computer with x86 architecture. If we want the model to run on mobile devices which normally have arm architecture, you need to use qnnpack for the backend. . | Fuse: this consist in converting a list of modules into a single module. The output will be the same, but PyTorch will optimize some operations for us. This will save memory and make the model run faster. But it is optional and not all layers are supported. In our case we will fuse our two convolutional blocks, conv+batch_norm+(relu). You can read more about fusion and which layers can be fused here. . | Add the stubs layers at the beginning and the end of the model. Those will be converted after calibration to quantize the inputs and outputs. . | Set the configuration, we will use the default. . | Calibrate: We take a sample of the training data that represents the data on which we will evaluate, to teach the model how to quantize the inputs and activations. Although, here we are using directly the test data so it could be considered a bit like cheating. But as is not the point of the notebook we will leave it as it is and keep that in mind for real-world applications. . | Convert: where the actual quantization is made. . | . ! mkdir -p &#39;pytorch_models&#39; def quantize_and_eval(model, model_name, quantized_export, test_dataset_x, test_dataset_y): &quot;&quot;&quot;Helper method to eval a model (quantizing it or not).&quot;&quot;&quot; # Remove the normalization from the weighted layers as is not suported in quantization if model.norm_w: for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): nn.utils.remove_weight_norm(m) if quantized_export: # Some operations not suported on GPU model.to(&quot;cpu&quot;) model.eval() # Backend for using a serever backend = &quot;fbgemm&quot; &quot;&quot;&quot;Fuse - Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules &quot;&quot;&quot; torch.quantization.fuse_modules(model, [[&#39;conv_layer1&#39;,&#39;batch_norm1&#39;,&#39;relu&#39;], [&#39;conv_layer2&#39;, &#39;batch_norm2&#39;]], inplace=True) &quot;&quot;&quot;Insert stubs&quot;&quot;&quot; model = model.add_quant() &quot;&quot;&quot;Prepare&quot;&quot;&quot; model.qconfig = torch.quantization.get_default_qconfig(backend) torch.quantization.prepare(model, inplace=True) &quot;&quot;&quot;Calibrate&quot;&quot;&quot; with torch.inference_mode(): for idx in range(min(len(test_dataset_x), 1000)): x = test_dataset_x[idx:idx+1].to(&quot;cpu&quot;) model(x) &quot;&quot;&quot;Convert&quot;&quot;&quot; torch.quantization.convert(model, inplace=True) model_name = &#39;{}_{}.pt&#39;.format( model_name, (&#39;quantized&#39; if quantized_export else &#39;float&#39;)) print(&#39;Model Name: {}, Quantized: {}&#39;.format(model_name, quantized_export)) print(&#39;Model Size: {:.2f} KB&#39;.format(real_model_size(model))) torch.save(model.state_dict(), os.path.join(&#39;pytorch_models&#39;, model_name)) # Evaluate the model. start = time.time() pytorch_model_eval(model, test_dataset_x, test_dataset_y, quantized_export) end = time.time() print(&#39;Eval time: {:.2f}s&#39;.format(end-start)) . Chain everything together in a single train function . Now we only need to put all the pieces together!üéâüéâ . import os import numpy as np from copy import deepcopy def train_model_and_quantize(batch_size=128, epochs=100, model_name=&#39;mnist_model&#39;, norm_w=True): model = CNN(norm_w=norm_w).to(device) model, model_history = train_model(model, epochs, bz=batch_size) print(&quot;Running Final Evaluation&quot;) # Convert and evaluate both (floating point and quantized models). q_model = deepcopy(model) quantize_and_eval(model, model_name, False, test_x, test_y) quantize_and_eval(q_model, model_name, True, test_x, test_y) return model, model_history . mnist_model, mnist_model_history = train_model_and_quantize(epochs=15) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 4.73 s - Loss: 0.1884 - Categorical_Accuracy: 0.9522 - Val_Loss: 0.0486 - Categorical_Val_Accuracy: 0.9840 Epoch 2 Time: 4.57 s - Loss: 0.0508 - Categorical_Accuracy: 0.9849 - Val_Loss: 0.0457 - Categorical_Val_Accuracy: 0.9854 Epoch 3 Time: 4.63 s - Loss: 0.0391 - Categorical_Accuracy: 0.9880 - Val_Loss: 0.0392 - Categorical_Val_Accuracy: 0.9858 Epoch 4 Time: 4.63 s - Loss: 0.0321 - Categorical_Accuracy: 0.9898 - Val_Loss: 0.0359 - Categorical_Val_Accuracy: 0.9885 Epoch 5 Time: 4.61 s - Loss: 0.0276 - Categorical_Accuracy: 0.9915 - Val_Loss: 0.0286 - Categorical_Val_Accuracy: 0.9912 Epoch 6 Time: 4.65 s - Loss: 0.0224 - Categorical_Accuracy: 0.9936 - Val_Loss: 0.0275 - Categorical_Val_Accuracy: 0.9912 Epoch 7 Time: 4.64 s - Loss: 0.0200 - Categorical_Accuracy: 0.9938 - Val_Loss: 0.0337 - Categorical_Val_Accuracy: 0.9899 Epoch 8 Time: 4.69 s - Loss: 0.0171 - Categorical_Accuracy: 0.9945 - Val_Loss: 0.0331 - Categorical_Val_Accuracy: 0.9883 Epoch 9 Time: 4.69 s - Loss: 0.0153 - Categorical_Accuracy: 0.9951 - Val_Loss: 0.0272 - Categorical_Val_Accuracy: 0.9911 Epoch 10 Time: 4.60 s - Loss: 0.0149 - Categorical_Accuracy: 0.9951 - Val_Loss: 0.0310 - Categorical_Val_Accuracy: 0.9901 Epoch 11 Time: 4.61 s - Loss: 0.0128 - Categorical_Accuracy: 0.9958 - Val_Loss: 0.0267 - Categorical_Val_Accuracy: 0.9916 Epoch 12 Time: 4.61 s - Loss: 0.0094 - Categorical_Accuracy: 0.9971 - Val_Loss: 0.0315 - Categorical_Val_Accuracy: 0.9904 Epoch 13 Time: 4.60 s - Loss: 0.0086 - Categorical_Accuracy: 0.9975 - Val_Loss: 0.0277 - Categorical_Val_Accuracy: 0.9910 Epoch 14 Time: 4.62 s - Loss: 0.0081 - Categorical_Accuracy: 0.9974 - Val_Loss: 0.0358 - Categorical_Val_Accuracy: 0.9899 Epoch 15 Time: 4.63 s - Loss: 0.0081 - Categorical_Accuracy: 0.9973 - Val_Loss: 0.0318 - Categorical_Val_Accuracy: 0.9906 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9906 Eval time: 7.14s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.9845 Eval time: 8.91s . plt.plot( mnist_model_history[&#39;sparse_categorical_accuracy&#39;], linestyle=&#39;dashed&#39;) plt.plot( mnist_model_history[&#39;val_sparse_categorical_accuracy&#39;], linestyle=&#39;dotted&#39;) plt.title(&#39;Model Accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;Train Accuracy&#39;, &#39;Test Accuracy&#39;], loc=&#39;lower right&#39;) plt.show() .",
            "url": "https://xanaga.github.io/posts_en/quantization/efficient_deeplearning/2022/05/29/Quantizing_A_Deep_Learning_Model_PyTorch.html",
            "relUrl": "/quantization/efficient_deeplearning/2022/05/29/Quantizing_A_Deep_Learning_Model_PyTorch.html",
            "date": " ‚Ä¢ May 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Knowledge Distillation for Traffic Sign Recognition",
            "content": ". Introduction . It seems that the current trend in Deep Learning is to have bigger and more models. This makes it difficult for users to use them, fit them into small devices, get fast results etc. This is why I see model compression, knowledge distillation, and these kinds of techniques as one of the more interesting and useful topics in deep learning. After all, if you want to apply deep learning in VR/AR you need to fit them into small devices. Also for simulations and computer graphics is better to have optimized and fast models. In addition, this makes AI more affordable to everyone, democratizing access to this technology. With this objective in mind I tried to get similar results as in my previous post, but using a resnet18 instead of a resnet34. Before continuing reading this post I encourage you to take a look at the previous one. Nevertheless, I was not successful at all. Looking for some solutions I discovered FasterAI by Nathan Hubens, which is an awesome library that implements those compression techniques based on Fastai. So my objectives with this notebook are: . Try to implement Knowledge Distillation technique using FasterAI to start getting familiar with the library. I would like to apply this model compression to other projects too (but that will not be covered in this notebook) | Encourage anyone who is thinking about implementing these types of approaches that may seem complicated to try this library. | . Imports . First we will upgrade the fastai version used in Colab, by deafult it is the first version. . ! pip install -Uqq fastai # upgrade fastai on colab . import fastai fastai.__version__ . &#39;2.5.6&#39; . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.data.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . This will be analogous to the Download section in the previous post. Despite that, I have added the possibility to only train on some classes of the dataset. Because of using Knowledge Distillation we should be able to classify signals that never have been seen in the training set (or at least give an informed guess). For this notebook, we will use all the classes, but you are encouraged to try to eliminate some and see how it behaves! . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . c=list(range(43)) # List the classes you want to train on . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. It is analogous to the process made in the resnet34 notebook, with some minor changes to handle the cases where not all the classes are selected. A more detailed explanation can be found in the previous post. . We have changed the read_annotations function to allow us to filter and read only the annotations of the classes we are interested in. This is done by adding the if inside the for loop. So the classes will have to be passed as an argument in all the following functions. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename, classes=None): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column if classes is None or label in classes: # We only read the annotations of the classes we are interested annotations.append(Annotation(filename, label)) return annotations def load_training_annotations(source_path, classes=None): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename, classes)) return annotations def copy_files(label, filenames, source, destination, classes=None, move=False): func = os.rename if move else shutil.copyfile label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) for filename in filenames: if classes is None or int(os.path.basename(label_path)) in classes: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) def split_train_validation_sets(source_path, train_path, validation_path, all_path, classes, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) annotations = load_training_annotations(source_path) filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) for label, filenames in filenames.items(): filenames = sorted(filenames) validation_size =int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, classes, move=False) copy_files(label, train_filenames, source_path, train_path, classes, move=True) copy_files(label, validation_filenames, source_path, validation_path, classes, move=True) . Due to the changes we have made in the previous functions we only have to add the information about the classes in the last step, passing it to the split_train_validation_sets function. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, c, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Prepare the data . Data preparation is even more similar to the previous post. We will only add a vocab when creating the dataset. This is because if we don&#39;t specify it Fastai will take as vocab the classes that appear in our data, which would be problematic if we want to restrict the problem to only a subset of classes. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], vocab=[str(i) for i in range(43)]) . We can see that our vocab will have all the classes, no matter which classes we use to train. . data.vocab . [&#39;0&#39;, &#39;1&#39;, &#39;10&#39;, &#39;11&#39;, &#39;12&#39;, &#39;13&#39;, &#39;14&#39;, &#39;15&#39;, &#39;16&#39;, &#39;17&#39;, &#39;18&#39;, &#39;19&#39;, &#39;2&#39;, &#39;20&#39;, &#39;21&#39;, &#39;22&#39;, &#39;23&#39;, &#39;24&#39;, &#39;25&#39;, &#39;26&#39;, &#39;27&#39;, &#39;28&#39;, &#39;29&#39;, &#39;3&#39;, &#39;30&#39;, &#39;31&#39;, &#39;32&#39;, &#39;33&#39;, &#39;34&#39;, &#39;35&#39;, &#39;36&#39;, &#39;37&#39;, &#39;38&#39;, &#39;39&#39;, &#39;4&#39;, &#39;40&#39;, &#39;41&#39;, &#39;42&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . Knowledge Distillation . This is a technique proposed by Geoffrey E. Hinton as a way to extract the knowledge from big models and use it to train simpler architectures. Moreover, it also has proven to provide what seems to be more robust and general learning which is always desirable. We have considered this approach because of the increasing size of the models not only in computer vision tasks but also in Natural Language Processing (NLP), which makes those difficult to be used in inference for very time demanding tasks or even impossible to fit in embedded systems. . The high-level idea is to, instead of training the models with one-hot encoded vectors, train them using a ‚Äúsofter version‚Äù of those that stores some information about the similarity between images. Is a concept similar to word embeddings in NLP. These labels are obtained from (normally) a bigger model which is already trained and used only for inference. . Here is where we start using FasterAI library. Nevertheless, I found problems loading a model into a learner with Fastai. As KnowledgeDistillation function expects to receive a learner I had to copy and paste this function and modify it by adding the from_learner parameter. In this way, I can set it to false and use as a teacher a plain PyTorch model. Nevertheless, the correct way of using it would be as straightforward as installing the library: . pip install fasterai . And import the things you need: . from fasterai import ... . sz = 96 bs = 256 wd = 5e-3 f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) . To act as the teacher we will use the model we trained in the previous post which can be found in this repo. You can download it from there or train your own model using the previous notebook. Once you have the model trained you will have to set the path_to_model variable. . If you are working in Colab you can drag and drop the model into the Colab&#39;s file explorer or save the model in your drive and mount it (just by clicking the Mount Drive button in the files tab, on the left in Colab). By mounting the drive you will be able to reach the model file by navigating across your drive directories. . path_to_model = #your_path . The teacher model &#128188; &#128207; . First, we prepare the teacher model. To do so, we will get the architecture of our model with random weights and then we will fill it with the trained weights. . Important: To do that, is necessary that the random filled architecture has exactly the same parameters as the ones in the pretrained ones. This is why we use create_cnn_model fuction of Fastai, which given a model architecture it creates a model (which is a Sequential object) with the head and the body that we choose. In the previous notebook we created the Learner with the default settings, so now we will use those defaults too. For each architecture, Fastai has a way of cutting the model, and for all of them it uses the same default head, you can check the source code! . If you would want to create the model with a different head and body you would have to use the cut and custom_head arguments. You can find more information about it in the documentation. . The to_device method will place it into GPU or CPU depending on the hardware available. . teacher = create_cnn_model(resnet34, 43, pretrained=False).to(device) #get the model architecture teacher.load_state_dict(torch.load(path_to_model+&#39;resnet34_weights.pth&#39;)) #load the trained wheights teacher . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=43, bias=False) ) ) . . . Note: The teacher is a Sequential object with two modules. The module (0) is the body, and it is the original architecture of the model (in our case resnet34), which can be pretrained (not in our case). The second module, the (1): Sequential, is the head which is initialized using kaiming_normal initialization by default(it can be changed using the init argument), so it is not pretrained. The layers of this head are the default used in Fastai. The decision about where to cut the original model is taken according to Fastai metadata for each architecture, but as we have said it is customizable using the cut argument. . Our teacher model is almost ready! The only thing that last is that, as we don&#39;t want to propagate gradients through our teacher (we only want it for inference) we should make sure it is frozen. . The following PyTorch code does exactly that. . for param in teacher.parameters(): param.requires_grad = False . . Note: Inside the KnowledgeDistillation callback we use the teacher in eval() mode, so the gradients would not be calculated even if the model is not frozen, but is better to make sure. . The student Learner &#128215; . Now let&#39;s prepare our student network. As before we use the create_cnn_model, but with a pretrained resnet18 architecture, to build the model that we will pass to the Learner constructor. . student_model=create_cnn_model(resnet18, 43).to(device) #get the model architecture and pretrained wheights . Then, we could do the following: . bad_student = Learner(data, student_model, metrics=[accuracy,f1_score_mult]) bad_student.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 11,725,888 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . But this would unfreeze the whole student! We don&#39;t want that, we would like to have the earlier pretrained layers of the model (the body) frozen and the rest (the head) to be trainable. The reason is that, by default, Learner does not freeze any parameter group. . So let&#39;s freeze the body. To do that we use the freeze_to(n) method where n is the number of parameter groups we want to freeze. We take n=1 as we only want the body to be frozen. . bad_student.freeze_to(1) bad_student.summary() . /usr/local/lib/python3.7/dist-packages/fastai/optimizer.py:22: UserWarning: Freezing 1 groups; model has 1; whole model is frozen. warn(f&#34;Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.&#34;) . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 False ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 False ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 12,672 Total non-trainable params: 11,713,216 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have frozen the entire model! ü•∂ . Note: The BatchNorm2d layers are never frozen, this is why we have some trainable parameters. If we look at the warning at the top of the output, we can see the problem. We only have one parameter group which is the entire model. . So, how are those parameters groups chosen? By the argument splitter, which by default is trainable_parameters. This will return all the trainable parameters of the model. In our case, they are all trainable, so we will only get one parameter group, hence the whole student will be frozen. . The solution is to pass splitter=default_split. This will split the parameter groups in body and head, just as we want! . . Important: In the Explore Training section we will use lr_find(), which does not accept any weight decay nor callback arguments (it takes the ones defined when creating the Learner). Then the correct thing to do is to pass as an argument to the learner the KnowledgeDistillation callback, as we do with the weight decay. By doing so we have thesame training scenario when doing lr_find() and fit_one_cycle(). . loss = partial(SoftTarget, T=20) kd = KnowledgeDistillation(teacher, loss, from_learner=False) . student = Learner(data, student_model, metrics=[accuracy,f1_score_mult], splitter=default_split, wd=wd, cbs=kd) student.freeze_to(1) #student.summary() . . Warning: If you use student.summary() it will raise the following error: Exception occured in KnowledgeDistillation when calling event after_loss: cross_entropy_loss(): argument &#8217;target&#8217; (position 2) must be Tensor, not NoneType. . Although, it only affect to the summary method. So we can continue and if we really want to check the learner we can remove and add back the callback, as the collapsed code shows below. . print(&#39;See in which position the KnowledgeDistillation is:&#39;) print(student.cbs) print() student.remove_cb(cb=student.cbs[3]) # We could also do cb=kd print(student.summary()) # Obviously you won&#39;t see the KnowledgeDistillation callback at the end student.add_cb(cb=kd) # Add back the callback print() print(&#39;Check that you have the same list as before the summary:&#39;) print(student.cbs) . See in which position the KnowledgeDistillation is: [TrainEvalCallback, Recorder, ProgressCallback, KnowledgeDistillation] . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 558,976 Total non-trainable params: 11,166,912 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback Check that you have the same list as before the summary: [TrainEvalCallback, Recorder, ProgressCallback, KnowledgeDistillation] . . Training . In the training process is where FasterAI magic comes in üßô . Only by adding the KnowledgeDistillation callback when fitting the model, we will be able to use this technique with almost the same code as in the previous post. But, as we setted those parameters when creating the Learner(), we don&#39;t even need to do that!üéâ . Explore training . We will follow the same procedure as is the previous post for finding the hyperparameters that better train our model. . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . . Note: Remember that the weight decay and the Knowledge Distillation Callback where setted in the Learner() definition. . student.lr_find() # wd and callback included . SuggestedLRs(valley=0.004365158267319202) . student.fit_one_cycle(1, lr_max=0.001) # wd and cbs are setted in Learner() . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 15.465137 | 13.668188 | 0.605898 | 0.563930 | 01:35 | . student.unfreeze() . student.lr_find() . SuggestedLRs(valley=0.00019054606673307717) . student.fit_one_cycle(9, lr_max=slice(0.0001, 0.001)) #, wd=wd, cbs=kd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 9.225066 | 6.410910 | 0.833745 | 0.821246 | 01:41 | . 1 | 4.279030 | 1.822614 | 0.960905 | 0.957461 | 01:36 | . 2 | 2.453992 | 1.222131 | 0.974760 | 0.972952 | 01:37 | . 3 | 1.889181 | 0.925809 | 0.982579 | 0.981159 | 01:37 | . 4 | 1.645638 | 0.775705 | 0.988889 | 0.988514 | 01:37 | . 5 | 1.506031 | 0.679071 | 0.989575 | 0.989230 | 01:37 | . 6 | 1.422567 | 0.668275 | 0.990809 | 0.990585 | 01:36 | . 7 | 1.355428 | 0.612182 | 0.991632 | 0.991374 | 01:36 | . 8 | 1.323943 | 0.623958 | 0.991495 | 0.991259 | 01:37 | . student.lr_find() . SuggestedLRs(valley=1.5848931980144698e-06) . student.fit_one_cycle(6, lr_max=slice(0.00001, 0.0001))#, wd=wd, cbs=kd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 1.322057 | 0.606176 | 0.991770 | 0.991487 | 01:37 | . 1 | 1.320874 | 0.618872 | 0.990535 | 0.990247 | 01:36 | . 2 | 1.300413 | 0.615932 | 0.992730 | 0.992584 | 01:36 | . 3 | 1.300726 | 0.611663 | 0.992455 | 0.992312 | 01:36 | . 4 | 1.290397 | 0.592799 | 0.992455 | 0.992310 | 01:37 | . 5 | 1.286910 | 0.597255 | 0.992455 | 0.992296 | 01:37 | . Retrain on the whole dataset . Once we know which configuration is better we can train the model on all the data available (training+validation). . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], vocab=[str(i) for i in range(43)], train=&#39;all&#39;) . teacher = create_cnn_model(resnet34, 43, pretrained=False).to(device) teacher.load_state_dict(torch.load(path_to_model+&#39;resnet34_weights.pth&#39;)) student_model=create_cnn_model(resnet18, 43).to(device) student = Learner(data, student_model, metrics=[accuracy,f1_score_mult], splitter=default_split, wd=wd, cbs=kd) student.freeze_to(1) . student.fit_one_cycle(1, lr_max=0.001) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 13.978484 | 11.242065 | 0.715364 | 0.683073 | 01:49 | . student.unfreeze() student.fit_one_cycle(10, lr_max=slice(0.0001, 0.001)) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 9.187653 | 5.441027 | 0.893416 | 0.883363 | 01:54 | . 1 | 3.874919 | 1.078969 | 0.987106 | 0.987011 | 01:53 | . 2 | 2.228036 | 0.545888 | 0.997257 | 0.997255 | 01:53 | . 3 | 1.754988 | 0.441869 | 0.998080 | 0.998076 | 01:52 | . 4 | 1.543820 | 0.505094 | 0.998903 | 0.998900 | 01:53 | . 5 | 1.451698 | 0.324294 | 0.999726 | 0.999725 | 01:52 | . 6 | 1.336454 | 0.319012 | 0.999863 | 0.999863 | 01:51 | . 7 | 1.282818 | 0.278213 | 1.000000 | 1.000000 | 01:51 | . 8 | 1.239342 | 0.259717 | 1.000000 | 1.000000 | 01:50 | . 9 | 1.236298 | 0.250886 | 1.000000 | 1.000000 | 01:51 | . student.fit_one_cycle(6, lr_max=0.0001) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 1.270205 | 0.312592 | 0.999726 | 0.999725 | 01:50 | . 1 | 1.336736 | 0.343301 | 0.999863 | 0.999863 | 01:51 | . 2 | 1.285208 | 0.289410 | 0.999863 | 0.999863 | 01:50 | . 3 | 1.217801 | 0.267841 | 0.999863 | 0.999863 | 01:51 | . 4 | 1.179426 | 0.231982 | 1.000000 | 1.000000 | 01:51 | . 5 | 1.172143 | 0.221064 | 1.000000 | 1.000000 | 01:50 | . Test . Like we did in the previous post we will use Test Time Augmentations, and we will use the same function to plot how the performance and inference time behave. . Note: We have added to the test_time_aug function the mask argument. This will allow us to compute the F1-score only for some classes. For example, only for the ones we have trained on. . def test_time_aug(learner, test_dataloader, y_true, metric, n_augs=[10], beta=0.1,mask=None): res = [] if mask is None: mask=list(range(len(y_true))) learner.eval() for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learner.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds[mask], y_true[mask]) res.append([aug, score,infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . true_test_labels = {a.filename: a.label for a in test_annotations} class_indexes = data.vocab.o2i test_img=get_image_files(&#39;./data/test&#39;) filenames = [filepath.name for filepath in test_img] labels = [str(true_test_labels[filename]) for filename in filenames] y_true = np.array([class_indexes[label] for label in labels]) . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . The following code will create the mask used for computing the F1-score. You only have to specify in which classes you want to focus on the interest_classes variable. We will focus on all the classes that we trained with (which are all the classes). . interest_classes=c interest_idx=[data.vocab.o2i[str(cl)] for cl in interest_classes] mask=np.isin(y_true, interest_idx, invert=False) . . Note: If you set invert to True you will get the F1-score only on the images you have not seen in the training. If we try to use our learner for as it is we will get an error, produced by the KnowledgeDistallation callback. But we don&#39;t need it anymore! (sorry Nathan, you&#39;ve been a hero üòî). Let&#39;s remove it. . student.cbs . (#4) [TrainEvalCallback,Recorder,ProgressCallback,KnowledgeDistillation] . student.cbs[3] . KnowledgeDistillation . student.remove_cb(cb=student.cbs[3]) student.cbs . (#3) [TrainEvalCallback,Recorder,ProgressCallback] . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(student, test_dataloader, y_true, metric, n_augs=[0, 5, 10, 20, 30]) . N Augmentations: 0 F1-score: 0.9922140340549594 Time:24.077036380767822 . . N Augmentations: 5 F1-score: 0.9933502950176439 Time:117.72993874549866 . . N Augmentations: 10 F1-score: 0.9943761367605225 Time:212.58393836021423 . . N Augmentations: 20 F1-score: 0.9950922306435893 Time:405.470721244812 . . N Augmentations: 30 F1-score: 0.9947694889306348 Time:592.3682796955109 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . Conclusion . As we have seen Knowledge Distillation allows us to train for more epochs without overfitting and improves the final performance of the model compared with not using it (and getting very close of the resnet34, which resulted in 0.9963 F1-score). An intuitive explanation can be that we are giving more information to the network in every step of the backpropagation by, not only providing one-hot labels, but also a probability of the image to be another class. With this we are introducing the idea of similarities between classes. . I see these types of compression approaches as very interesting and I would like to apply them in other domains, maybe to NLP or 3D data. So I cannot recommend more FasterAI library and infinitely thank the work done to Nathan Hubens. Also, if you want to learn more about how to make smaller and faster Neural Networks I encourage you to visit his blog. . Thanks for reading!üòÄ .",
            "url": "https://xanaga.github.io/posts_en/knowledgedistillation/fastai/computer%20vision/2022/04/24/Knowledge_Distillation.html",
            "relUrl": "/knowledgedistillation/fastai/computer%20vision/2022/04/24/Knowledge_Distillation.html",
            "date": " ‚Ä¢ Apr 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "ResNet for Traffic Sign Classification With PyTorch",
            "content": ". Introduction . In the Intelligent Systems course from my degree, we were asked to write a report about a Neural Network related topic. In class, we were introduced to the German Traffic Sign Recognition Benchmark dataset, so I decided to explore previous work of the community. This is how I discovered Pavel Surmenok which has a post dedicated to this subject, using Fastai! As Pavel&#39;s implementation was using the first version of Fastai and I wanted to get more familiar with this library I thought it would be a good idea to try to implement a version of his code using the newest version of Fastai (version 2.5.3 at the time of doing the notebook). So the objectives are the following: . Get hands-on experience with Fastai to apply the theory I know of deep learning. I think it is a great tool for rapid testing with impressive results. Start building things! | Best case scenario, being able to contribute to updating the work that Pavel presented in his blog and help anyone who wants to start with Fastai v2. | Try to explain things I didn&#39;t know before and I would like to find somewhere. The aim of this is to consolidate the knowledge and also help anyone that had the same questions. | . Imports . import fastai fastai.__version__ # check that we have Fastai v2 . &#39;2.5.6&#39; . Show the collapsed code to see all the imports we will need . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch import time import pandas as pd . . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. . Tip: It is not directly related to Fastai or Deep Learning, but knowing how to manage your data is VERY important, and Pavel makes it in an understandable and elegant way. If you don&#8217;t have a lot of confidence working with data this is a section worth looking at. . Annotations will be named tuples containing the name of the file and its corresponding label. You can find a complete tutorial on namedtuple() here, but basically, those are tuple-like immutable data structures with named fields. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column annotations.append(Annotation(filename, label)) return annotations . load_training_annotations will loop over all the files from all the classes and returns the annotations of all the training examples. . def load_training_annotations(source_path): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename)) return annotations . . Note: my_list.extend(iter) will add each element of an iterable,iter, to the list my_list. If we would have used append we would get a list of list of named tuples instead of a list of named tuples. . We have talked about the filenames, labels etc. but our actual data are images. We will use the copy_files function to organize the images in training, validation and all (training+validation) folders. . def copy_files(label, filenames, source, destination, move=False): # copy to the training or validation folders, for the all folder just rename func = os.rename if move else shutil.copyfile # make a directory for every label label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) # fill the directories with its corresponding files for filename in filenames: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) . Finally, we use all the above mentioned to build our own split_train_validation_sets function. . def split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) # annotations will be a list of Annotations(filename, label) annotations = load_training_annotations(source_path) # filenames will be a dictionary # keys: label # values: list of file names filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) # for every label calculate the validation_size and populate the directories for label, filenames in filenames.items(): filenames = sorted(filenames) # get the validation_size, it must be an integer! validation_size = int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, move=False) copy_files(label, train_filenames, source_path, train_path, move=True) copy_files(label, validation_filenames, source_path, validation_path, move=True) . Once we have all these functions all we have to do is call them with the appropriate paths. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Exploratory analisys . We have our data prepared so, first things first, let&#39;s take a look at it. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] classes . ClassId SignName . 0 0 | Speed limit (20km/h) | . 1 1 | Speed limit (30km/h) | . 2 2 | Speed limit (50km/h) | . 3 3 | Speed limit (60km/h) | . 4 4 | Speed limit (70km/h) | . 5 5 | Speed limit (80km/h) | . 6 6 | End of speed limit (80km/h) | . 7 7 | Speed limit (100km/h) | . 8 8 | Speed limit (120km/h) | . 9 9 | No passing | . 10 10 | No passing for vechiles over 3.5 metric tons | . 11 11 | Right-of-way at the next intersection | . 12 12 | Priority road | . 13 13 | Yield | . 14 14 | Stop | . 15 15 | No vechiles | . 16 16 | Vechiles over 3.5 metric tons prohibited | . 17 17 | No entry | . 18 18 | General caution | . 19 19 | Dangerous curve to the left | . 20 20 | Dangerous curve to the right | . 21 21 | Double curve | . 22 22 | Bumpy road | . 23 23 | Slippery road | . 24 24 | Road narrows on the right | . 25 25 | Road work | . 26 26 | Traffic signals | . 27 27 | Pedestrians | . 28 28 | Children crossing | . 29 29 | Bicycles crossing | . 30 30 | Beware of ice/snow | . 31 31 | Wild animals crossing | . 32 32 | End of all speed and passing limits | . 33 33 | Turn right ahead | . 34 34 | Turn left ahead | . 35 35 | Ahead only | . 36 36 | Go straight or right | . 37 37 | Go straight or left | . 38 38 | Keep right | . 39 39 | Keep left | . 40 40 | Roundabout mandatory | . 41 41 | End of no passing | . 42 42 | End of no passing by vechiles over 3.5 metric tons | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We can try to do histogram equalization to see if it improves our results, but in the end, it does not. If you want to try you can use the HistogramEqualization_item class as an item_tfms for the Fastai pipeline. . . Note: You can also try to implement histogram equalization in the way Pavel does in his notebook, but I could not see the changes visually, maybe something has changed in how Fastai opens images. . # Histogram equalization class HistogramEqualization_item(Transform): def init(self, prefix=None): self.prefix = prefix or &#39;&#39; def encodes(self, o): if type(o) == PILImage: ret = ImageOps.equalize(o) else: ret = o return ret def decodes(self, o): return o . . We could start inspecting the data as it is, but Fastai provides us with some useful functions to visualize our data. To use them we will have to integrate this data into Fastai datastructures. We could create a DataBunch or Datasets or go directly to Dataloaders. We will do the last one, but you can find a lot of information in tutotials and documentation. . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)]) . The item_tfms will be applied to every item in our dataset independently (here is where you have to add the HistogramEqualization_item transform if you want to try). Those are performed in the CPU every time an item is accessed. In our case, we only use Resize. Contrary, the batch_tfms will be applied to all the images in the batch at the same time. Those are done in the GPU and are very fast. You can find more information about aug_transforms in the documentation, but basically is a wrap of very useful transforms for image data. As you may note, we randomly resize again the images. The reason is that this is the resizing we want to apply to our images, the one we did previously in √¨tem_tmfs makes all the images the same size and square, to be able to perform batch_tfms in the GPU. These augmentations are key to having a good performance, and this applies to every deep learning project you will work on. . . Note: We normalize using imagenet_stats because our models will be pretrained on this dataset, which is way larger than the one we are working with. . Now we can use show_batch method to see some examples from our dataset. . data.show_batch(nrows=3, ncols=3) . We can also see how the augmentations affect our data by seeing how those affect to a single example. . data.show_batch(nrows=3, ncols=3, unique=True) . We can also check the length of our training and validation set. . len(data.train_ds), len(data.valid_ds) . (31919, 7290) . . Important: For the sake of the blog post lenght, we finish here our &quot;Exploratory analysis&quot;. Nevertheless, explore the data not only includes watching at the data (which is very important), but also the sizes, class distribution etc. A more complete analysis can be found in Pavel&#8217;s post, and you are encouraged to go further with the help of Fastai. . Training . Explore training . In this section, we will try to find the best training hyperparameters to train the model. To do so we will train on the train data and evaluate on the validation data. Then we will use these hyperparameters to train the model on the whole data available (training + validation) and we will test it with the test data. . The metrics we will track will be accuracy and F1-score, because we have seen that the dataset shows class imbalance. The F1-score metric is more robust and informative about the model performance when we are working with this kind of dataset. Concerning the model, we will use a resnet34 pretrained on imagenet. The data, model, and metrics to track will be wrapped up together in a learner object. . Note: We are passing the weight decay as an argument to the Learner(). By doing so, we will use it when calling both lr_find() and fit_one_cycle() methods in the Explore training section. . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) wd = 5e-4 learn = cnn_learner(data, resnet34, metrics=[accuracy,f1_score_mult], wd=wd) . learn.summary() # Display information about the learner obj . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 566,400 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . . Important: Our model is frozen up to parameter group #2 (the whole model except the last layer), this means that when running backpropagation the gradients won&#8217;t be calculated for those layers, hence their weights will remain unchanged. . One of the most important hyperparameters in deep learning is the learning rate. To find a good one we can use lr_find method, which takes a batch and runs it through the network with incrementally bigger learning rates recording the loss for each run. The idea is to take the learning rate where the graph of the loss is the steepest. You can try to find it visually or let Fastai give you a suggestion (at least in this case it tend to suggest smaller learning rates). . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . learn.lr_find() . SuggestedLRs(valley=0.0020892962347716093) . Train the model with the learning rate selected for one epoch. In this epoch, we will only update the weights of the last layer, as the rest of the model is frozen (with the pretrained weights from imagenet). This will allow our last layer to &quot;catch up&quot; with the other ones and not to start randomly. The fit_one_cycle method uses the 1 cycle policy, which is a learning rate schedule policy consisting in starting from a low learning rate in the firsts epochs, increasing it up to a maximum, to decrease it again for the last epochs. A better explanation can be found in this post from Sylvain Gugger. . learn.fit_one_cycle(1, lr_max=0.005) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.875589 | 0.453181 | 0.853498 | 0.848488 | 01:30 | . Then we unfreeze the model to train it completely. With this, gradients will propagate through the whole network updating all the weights. . learn.unfreeze() learn.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 21,834,048 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have a different training scenario, so probably our learning rate has changed. So we repeat the process of finding the learning rate and training for some epochs. We can use slice when specifying the learning rate to allows us to use &quot;discriminative layer training&quot;, which consists of using different learning rates for different layers. Usually, we use a smaller learning rate for the earlier layers, as they are well trained to detect general features on a lot of data from imagenet, and a bigger one for the last layers, as they have to change more to capture the more specific features from our dataset. . learn.lr_find() . SuggestedLRs(valley=9.120108734350652e-05) . learn.fit_one_cycle(3, lr_max=0.0001) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.296145 | 0.232131 | 0.929218 | 0.925996 | 01:38 | . 1 | 0.155398 | 0.110068 | 0.970370 | 0.967639 | 01:38 | . 2 | 0.105242 | 0.086704 | 0.975034 | 0.972592 | 01:38 | . learn.lr_find() . SuggestedLRs(valley=1.2022644114040304e-05) . learn.fit_one_cycle(7, lr_max=slice(0.0001, 0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.117177 | 0.343552 | 0.925652 | 0.928776 | 01:38 | . 1 | 0.154971 | 0.221667 | 0.945816 | 0.942552 | 01:39 | . 2 | 0.117753 | 0.065744 | 0.982167 | 0.980668 | 01:38 | . 3 | 0.096865 | 0.046334 | 0.988615 | 0.988594 | 01:38 | . 4 | 0.065272 | 0.045523 | 0.986831 | 0.986001 | 01:38 | . 5 | 0.058440 | 0.033738 | 0.990535 | 0.990093 | 01:38 | . 6 | 0.055586 | 0.030030 | 0.990261 | 0.989890 | 01:38 | . Retrain on the whole dataset . Once we have tried different configurations and selected the one we think is best, it is time to train the model with all the data! . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) sz = 96 bs = 256 wd = 5e-4 . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], train=&#39;all&#39;) learn = cnn_learner(data, resnet34,metrics=[accuracy,f1_score_mult], wd=wd) . learn.fit_one_cycle(1, lr_max=0.01) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.542564 | 0.107461 | 0.964746 | 0.964709 | 01:46 | . learn.unfreeze() learn.fit_one_cycle(4, lr_max=slice(0.001,0.01)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.385180 | 0.500579 | 0.864060 | 0.859983 | 01:58 | . 1 | 0.210768 | 0.012989 | 0.995610 | 0.995585 | 01:58 | . 2 | 0.111806 | 0.003372 | 0.998903 | 0.998901 | 01:58 | . 3 | 0.063384 | 0.000662 | 0.999863 | 0.999863 | 01:58 | . learn.fit_one_cycle(6, lr_max=slice(0.0001,0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.059273 | 0.000613 | 0.999863 | 0.999863 | 01:58 | . 1 | 0.059253 | 0.001651 | 0.999451 | 0.999452 | 01:58 | . 2 | 0.058100 | 0.000631 | 0.999726 | 0.999726 | 01:58 | . 3 | 0.048571 | 0.000100 | 1.000000 | 1.000000 | 01:58 | . 4 | 0.042215 | 0.000125 | 1.000000 | 1.000000 | 01:58 | . 5 | 0.038188 | 0.000095 | 1.000000 | 1.000000 | 01:57 | . Test . For the test, we will use &quot;Test time augmentations&quot;. This technique is also used by Pavel and shows an increase in the performance of the model. It consists in, instead of just making the prediction of the given test image, making the prediction of the original and other augmented versions of this image using the augmentations used while training. The final prediction will be an average of all of them. This will take more time for each prediction but it will make them more robust. test_time_aug function will test the model with a different number of augmentations and it will return a Pandas Dataframe with the F1-scores and the time it has taken to run the inference of the test set. . def test_time_aug(learner, test_dataloader, metric, n_augs=[10], beta=0.1): res = [] for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learn.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds, y_true) res.append([aug, score, infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . As we did before, we need the test data to be in a DataLoader format. . true_test_labels = {a.filename: a.label for a in test_annotations} #get the annotations in a dictionary format class_indexes = data.vocab.o2i #dictionary from class to index test_img=get_image_files(&#39;./data/test&#39;) #list of the test image file paths filenames = [filepath.name for filepath in test_img] #get the names from the file paths labels = [str(true_test_labels[filename]) for filename in filenames] #list of the labels for each file y_true = np.array([class_indexes[label] for label in labels]) #array of the index for each label . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . len(test_dataloader.dataset) . 12630 . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(learn, test_dataloader, metric, n_augs=[0, 3, 5, 10, 15, 20, 30]) . N Augmentations: 0 F1-score: 0.994469328801153 Time:19.947819232940674 . . N Augmentations: 3 F1-score: 0.9943121917876697 Time:81.12456560134888 . . N Augmentations: 5 F1-score: 0.9950268203228465 Time:119.88853621482849 . . N Augmentations: 10 F1-score: 0.9959702387438432 Time:218.91360545158386 . . N Augmentations: 15 F1-score: 0.996130519244009 Time:317.34042143821716 . . N Augmentations: 20 F1-score: 0.996285695102012 Time:414.57182717323303 . . N Augmentations: 30 F1-score: 0.9963671397829769 Time:617.6512229442596 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . So we are getting a F1-score of 0.996367. Not bad at all! Who wants an autonomous ride?? ü•≥ üòÜ . Save the model . torch.save(learn.model.state_dict(), &#39;resnet34_weights_traffic.pth&#39;) .",
            "url": "https://xanaga.github.io/posts_en/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "relUrl": "/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "date": " ‚Ä¢ Apr 21, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "First Post",
            "content": "Finally, here we are‚Ä¶ . . The beginning . I have been self-learning about AI and Deep Learning since I started my degree more than three years ago. But lately, I had the feeling of being stuck, even a bit lost. Moreover, the idea of starting to share my work has always been in the back of my head. I think individually we can achieve cool things, but the big changes come from communities. We can find many examples just in the AI field, from Fastai to Hugging Face and many more. I want to be connected with those communities!üåç . Thinking about the best way to do it, I remembered what Jeremy Howard said about the importance of blogging in the second edition of one of the first courses I took. Then I discovered Fastpages, so the decision was taken, I would start blogging! . What you will find in this blog . With this work I want to share my work, the things I learn, and maybe my thoughts. It took me too long to start blogging because I wanted everything to be perfect, but this is not the idea. I will start my blog, it will be dynamic, things will change, there will be mistakes, and it never will be perfect. Beautiful, isn‚Äôt it? . Together with AI (which is my true loveüòç), I‚Äôm trying to get into Computer Graphics, VR/AR, and simulation. For that reason, I am trying to learn about Geometric Deep Learning, Neural Rendering, and also Unreal Engine! . Please remember this is a leaner‚Äôs blog, you are encouraged to help, give suggestions, and correct me if I am wrong. Let‚Äôs do this journey together! . Why not in Spanish? . As you may have noticed, I‚Äôm not a native English speaker. In fact, my mother tongue is Spanish. So, why not write in Spanish. Well, there are a couple of reasons: . I want to practice my English. Learn by doing! | In English, I have access to a broader audience, so more people can help or share their ideas. | And well‚Ä¶ I have never said there won‚Äôt be Spanish postsüòâ I think with different languages we can target different audiences. So if I see some content is redundant or irrelevant in English, I will make it in Spanish. | . So see you in the next one, adi√≥s!üëã .",
            "url": "https://xanaga.github.io/posts_en/general/2022/04/18/first-post.html",
            "relUrl": "/general/2022/04/18/first-post.html",
            "date": " ‚Ä¢ Apr 18, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, my name is Xavi! I am a Computational Maths Bachelor student, passionate about AI and Deep Learning since the first year of my degree. Specifically, I would like to apply the advances in this field to Computer Graphics, VR/AR, simulation, etc. Nevertheless, I love to explore new things, so my interests go from Brain Computer Interfaces to space exploration, ranging everything that includes technology and takes us closer to the future. I also love sports, discovering new countries and eating... I love food! With this blog, I hope to meet more people with my same interest so if you have any suggestions, doubts, ideas, or simply want to chat, don&#39;t think it twice, and drop me a line!üòÑ",
          "url": "https://xanaga.github.io/posts_en/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://xanaga.github.io/posts_en/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}