{
  
    
        "post0": {
            "title": "NLP with Transformers: Text Classification",
            "content": ". Introduction . This is the first post of a series, which will go through the &quot;NLP with Transformers&quot; book. I am following the reading group organized by Hugging Face, with the authors themselves! So I thought the best way to properly understand each chapter would be to try to apply the concepts to a different example (then the problems happenüòÇ). My idea is to post one of these every two weeks, before the reading group session. However, life gets in the way, so I cannot promise anything. Do not expect fancy or innovative things, the code will be similar to the one in the book. Of course I would like to go deeper, but time is limited‚è≥ and this is just to enhance my learning (and the learning of whoever wants to follow the book). Enough introduction, now let&#39;s go with my first dive into NLP! . . Warning: These posts are not attempting to instruct the reader and they probably will contain errors. Reading the corresponding chapter of the book is needed for a correct understanding. They will also contain questions to be asked in the reading group session. . The dataset . Instead of the emotion dataset we will use the paws. This contains multiple languages, so we specify the English version by using &quot;en&quot;. . from datasets import load_dataset paws = load_dataset(&quot;paws-x&quot;, &quot;en&quot;) . paws . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;], num_rows: 49401 }) test: Dataset({ features: [&#39;id&#39;, &#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;], num_rows: 2000 }) validation: Dataset({ features: [&#39;id&#39;, &#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;], num_rows: 2000 }) }) . So we have a dataset where each example is a pair of sentences, with a label. The label indicates whether or not one of the sentences is paraphrasing the other. 0 indicates the pair has a different meaning, while 1 indicates the pair is a paraphrase. Our goal will be to use a pre-trained DistillBert model to distinguish them. . Let&#39;s get a Pandas Dataframe to easily explore our data. . paws.set_format(type=&quot;pandas&quot;) df=paws[&quot;train&quot;][:] . df.head() . id sentence1 sentence2 label . 0 1 | In Paris , in October 1560 , he secretly met t... | In October 1560 , he secretly met with the Eng... | 0 | . 1 2 | The NBA season of 1975 -- 76 was the 30th seas... | The 1975 -- 76 season of the National Basketba... | 1 | . 2 3 | There are also specific discussions , public p... | There are also public discussions , profile sp... | 0 | . 3 4 | When comparable rates of flow can be maintaine... | The results are high when comparable flow rate... | 1 | . 4 5 | It is the seat of Zerendi District in Akmola R... | It is the seat of the district of Zerendi in A... | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Here we can see an example of a paraphrase: . print(df.iloc[1][&quot;sentence1&quot;] + &quot; n&quot;) print(df.iloc[1][&quot;sentence2&quot;] + &quot; n&quot;) print(&quot;Label: &quot; + str(df.iloc[1][&quot;label&quot;])) . The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association . The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA . Label: 1 . And here a &quot;non-paraphrase&quot; pair: . print(df.iloc[0][&quot;sentence1&quot;] + &quot; n&quot;) print(df.iloc[0][&quot;sentence2&quot;] + &quot; n&quot;) print(&quot;Label: &quot; + str(df.iloc[0][&quot;label&quot;])) . In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland . In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England . Label: 0 . Class distribution . Now we check if we have an unbalanced dataset. . . Note: Check your understanding: How many bars will the frequency chart show when checking the class distribution? . import matplotlib.pyplot as plt df[&quot;label&quot;].value_counts(ascending=True).plot.barh() plt.title(&quot;Frequency of Classes&quot;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; . We see that we have more examples from the negative class, but I would not consider it unbalanced. Nevertheless, it is good to keep it in mind. . How long are the sentences? . We will take the two sentences together as input to our model. Hence we will consider the length as the sum of the words in each sentence. This is just an approximation of the input size. Depending on the tokenization it may differ. . df[&quot;Words Per Pair&quot;] = df[&quot;sentence1&quot;].str.split().apply(len) + df[&quot;sentence2&quot;].str.split().apply(len) df.boxplot(&quot;Words Per Pair&quot;, by=&quot;label&quot;, grid=False, showfliers=False, color=&quot;black&quot;) plt.suptitle(&quot;&quot;) plt.xlabel(&quot;&quot;) plt.show() . /usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X)) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; In both cases we can see the number of words is between 40 and 50 on average, and it does not go beyond 70. . . Note: For applications using DistilBERT, the maximum context size is 512 tokens. . paws.reset_format() . Tokenization . We will jump directly into sub-word tokenization. . from transformers import AutoTokenizer model_ckpt = &quot;distilbert-base-uncased&quot; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) . We can get iformation about the tokenizer. . print(&quot;Vocabulary size: &quot; + str(tokenizer.vocab_size)) print(&quot;Model max length: &quot; + str(tokenizer.model_max_length)) print(&quot;Model input names: &quot; + str(tokenizer.model_input_names)) . Vocabulary size: 30522 Model max length: 512 Model input names: [&#39;input_ids&#39;, &#39;attention_mask&#39;] . The tokenizer function . As I introduced earlier, we will concatenate the two sentences and give them as input to our model. We can do it while we are tokenizing them, but because this is an example to gain understanding we will do it separately. . First we make a function to merge the two sentences. Pay attention that we insert [SEP] tokens to separate those. This way we let the model know that are two different sentences. . Question: Should we insert [SEP][CLS] instead? So we would have one classification token per each sentence? . Answer: Comming soon . import numpy as np def fuse_sentences(batch): s1 = np.core.defchararray.add(np.array(batch[&quot;sentence1&quot;]), np.array(&quot;[SEP]&quot;)) s2 = np.array(batch[&quot;sentence2&quot;]) return {&quot;sentences&quot;: np.core.defchararray.add( s1, s2 ) } . paws_fused = paws.map(fuse_sentences, batched=True, batch_size=None) . print(paws_fused[&quot;train&quot;].column_names) . [&#39;id&#39;, &#39;label&#39;, &#39;sentence1&#39;, &#39;sentence2&#39;, &#39;sentences&#39;] . print(paws_fused[&quot;train&quot;][0][&quot;sentence1&quot;] + &quot; n&quot;) print(paws_fused[&quot;train&quot;][0][&quot;sentence2&quot;] + &quot; n&quot;) print(paws_fused[&quot;train&quot;][0][&quot;sentences&quot;] + &quot; n&quot;) . In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland . In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England . In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .[SEP]In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England . . Now we define the function to tokenize the two sentences together. . def tokenize(batch): return tokenizer(batch[&quot;sentences&quot;], padding=True, truncation=True) . paws_encoded = paws_fused.map(tokenize, batched=True, batch_size=None) . print(paws_encoded[&quot;train&quot;].column_names) . [&#39;attention_mask&#39;, &#39;id&#39;, &#39;input_ids&#39;, &#39;label&#39;, &#39;sentence1&#39;, &#39;sentence2&#39;, &#39;sentences&#39;] . Let&#39;s check if the tokens have been inserted correctly. . tokenizer.convert_ids_to_tokens(paws_encoded[&quot;train&quot;][0][&quot;input_ids&quot;]) . [&#39;[CLS]&#39;, &#39;in&#39;, &#39;paris&#39;, &#39;,&#39;, &#39;in&#39;, &#39;october&#39;, &#39;1560&#39;, &#39;,&#39;, &#39;he&#39;, &#39;secretly&#39;, &#39;met&#39;, &#39;the&#39;, &#39;english&#39;, &#39;ambassador&#39;, &#39;,&#39;, &#39;nicolas&#39;, &#39;th&#39;, &#39;##rock&#39;, &#39;##mo&#39;, &#39;##rton&#39;, &#39;,&#39;, &#39;asking&#39;, &#39;him&#39;, &#39;for&#39;, &#39;a&#39;, &#39;passport&#39;, &#39;to&#39;, &#39;return&#39;, &#39;to&#39;, &#39;england&#39;, &#39;through&#39;, &#39;scotland&#39;, &#39;.&#39;, &#39;[SEP]&#39;, &#39;in&#39;, &#39;october&#39;, &#39;1560&#39;, &#39;,&#39;, &#39;he&#39;, &#39;secretly&#39;, &#39;met&#39;, &#39;with&#39;, &#39;the&#39;, &#39;english&#39;, &#39;ambassador&#39;, &#39;,&#39;, &#39;nicolas&#39;, &#39;th&#39;, &#39;##rock&#39;, &#39;##mo&#39;, &#39;##rton&#39;, &#39;,&#39;, &#39;in&#39;, &#39;paris&#39;, &#39;,&#39;, &#39;and&#39;, &#39;asked&#39;, &#39;him&#39;, &#39;for&#39;, &#39;a&#39;, &#39;passport&#39;, &#39;to&#39;, &#39;return&#39;, &#39;to&#39;, &#39;scotland&#39;, &#39;through&#39;, &#39;england&#39;, &#39;.&#39;, &#39;[SEP]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;] . The model . We import the pre-trained model from the Hub. It does not include the last autoregressive part, which uses the latent representations of the input to output a un-masked sentence. . from transformers import AutoModel model_ckpt = &quot;distilbert-base-uncased&quot; device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = AutoModel.from_pretrained(model_ckpt).to(device) . Classifier . First, will use simple linear regression on the latent representations of the inputs. . Here is a function to extract the last hidden state. Specifically we are interested in the [CLS] token, which will serve us as a &quot;summary&quot; of the input relations. . def extract_hidden_states(batch): # Place model inputs on the GPU inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names} # Extract last hidden states with torch.no_grad(): last_hidden_state = model(**inputs).last_hidden_state # Return vector for [CLS] token return {&quot;hidden_state&quot;: last_hidden_state[:,0].cpu().numpy()} . Question: Would it have more sense if we had two [CLS] tokens (one per sentence) and concatenate them? . Answer: Coming soon. . The inputs to the DistillBert and the labels have to be torch tensors, so we have to change the type of those columns. . tokenizer.model_input_names . [&#39;input_ids&#39;, &#39;attention_mask&#39;] . paws_encoded.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;label&quot;]) . paws_hidden = paws_encoded.map(extract_hidden_states, batched=True) . paws_hidden[&quot;train&quot;].column_names . [&#39;attention_mask&#39;, &#39;hidden_state&#39;, &#39;id&#39;, &#39;input_ids&#39;, &#39;label&#39;, &#39;sentence1&#39;, &#39;sentence2&#39;, &#39;sentences&#39;] . Bridge to Scikit-Learn . Convert the output of the model to Numpy arrays. This is because the Scikit-Learn library works with those, not with PyTorch tensors. . . Note: Check your understanding: What will be the dimensions of X_train as defined in the following code cell? . import numpy as np X_train = np.array(paws_hidden[&quot;train&quot;][&quot;hidden_state&quot;]) X_valid = np.array(paws_hidden[&quot;validation&quot;][&quot;hidden_state&quot;]) y_train = np.array(paws_hidden[&quot;train&quot;][&quot;label&quot;]) y_valid = np.array(paws_hidden[&quot;validation&quot;][&quot;label&quot;]) X_train.shape, X_valid.shape . ((49401, 768), (2000, 768)) . . Dimensionality Reduction . A good practice is to take a look to the vectors that we have representing each pair of sentences. But they are 768 dimensional! Dimensionality reduction will allow us to project them into a lower dimensional space. . from umap import UMAP from sklearn.preprocessing import MinMaxScaler import pandas as pd # Scale features to [0,1] range X_scaled = MinMaxScaler().fit_transform(X_train) # Initialize and fit UMAP mapper = UMAP(n_components=2, metric=&quot;cosine&quot;).fit(X_scaled) # Create a DataFrame of 2D embeddings df_emb = pd.DataFrame(mapper.embedding_, columns=[&quot;X&quot;, &quot;Y&quot;]) df_emb[&quot;label&quot;] = y_train df_emb.head() . X Y label . 0 -1.060788 | 4.195936 | 0 | . 1 -5.594001 | 14.602888 | 1 | . 2 2.748996 | 3.122407 | 0 | . 3 6.510664 | 7.842440 | 1 | . 4 5.438829 | 0.523094 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Now we can visualize how the data points are distibuted in a 2D space for each class. . fig, axes = plt.subplots(1, 2, figsize=(7,5)) axes = axes.flatten() cmaps = [&quot;Blues&quot;, &quot;Reds&quot;] labels = paws[&quot;train&quot;].features[&quot;label&quot;].names for i, (label, cmap) in enumerate(zip(labels, cmaps)): df_emb_sub = df_emb.query(f&quot;label == {i}&quot;) axes[i].hexbin(df_emb_sub[&quot;X&quot;], df_emb_sub[&quot;Y&quot;], cmap=cmap, gridsize=20, linewidths=(0,)) axes[i].set_title(label) axes[i].set_xticks([]), axes[i].set_yticks([]) plt.tight_layout() plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; . Important: In our case, they look similar, but remember that it does not mean the two classes are not separable in higher dimensions. . Training the classifier . # We increase `max_iter` to guarantee convergence from sklearn.linear_model import LogisticRegression lr_clf = LogisticRegression(max_iter=3000) lr_clf.fit(X_train, y_train) . lr_clf.score(X_valid, y_valid) . 0.5785 . We can compare it with a dummy classifier. In our case the one that performed best was choosing the most frequent class in the training set. . from sklearn.dummy import DummyClassifier dummy_clf = DummyClassifier(strategy=&quot;most_frequent&quot;) dummy_clf.fit(X_train, y_train) dummy_clf.score(X_valid, y_valid) . 0.5685 . So our classifier does not perform really well. It is almost like the dummy one! Lastly, we can plot the confusion matrix of the predictions to see what is going on. . from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix def plot_confusion_matrix(y_preds, y_true, labels): cm = confusion_matrix(y_true, y_preds, normalize=&quot;true&quot;) fig, ax = plt.subplots(figsize=(6, 6)) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(cmap=&quot;Blues&quot;, values_format=&quot;.2f&quot;, ax=ax, colorbar=False) plt.title(&quot;Normalized confusion matrix&quot;) plt.show() y_preds = lr_clf.predict(X_valid) plot_confusion_matrix(y_preds, y_valid, labels) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; We see that our classifier tends to give the 0 label no matter what is the input, similarly to the dummy one ü§î . Finetuning . After this hit, let&#39;s see if we can perform better by using a fully connected NN as a classifier and backpropagating the gradient through all the model. . from transformers import AutoModelForSequenceClassification num_labels = 2 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = (AutoModelForSequenceClassification .from_pretrained(model_ckpt, num_labels=num_labels) .to(device)) . from sklearn.metrics import accuracy_score, f1_score def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) f1 = f1_score(labels, preds, average=&quot;weighted&quot;) acc = accuracy_score(labels, preds) return {&quot;accuracy&quot;: acc, &quot;f1&quot;: f1} . from huggingface_hub import notebook_login notebook_login() . Login successful Your token has been saved to /root/.huggingface/token . from transformers import Trainer, TrainingArguments batch_size = 64 logging_steps = len(paws_encoded[&quot;train&quot;]) // batch_size model_name = f&quot;{model_ckpt}-finetuned-paws&quot; training_args = TrainingArguments(output_dir=model_name, num_train_epochs=2, learning_rate=2e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01, evaluation_strategy=&quot;epoch&quot;, disable_tqdm=False, logging_steps=logging_steps, push_to_hub=True, log_level=&quot;error&quot;) . Question: Which loss are we using to train the model? How we could change it? . Answer: Comming soon . from transformers import Trainer trainer = Trainer(model=model, args=training_args, compute_metrics=compute_metrics, train_dataset=paws_encoded[&quot;train&quot;], eval_dataset=paws_encoded[&quot;validation&quot;], tokenizer=tokenizer) trainer.train(); . /content/notebooks/distilbert-base-uncased-finetuned-paws is already a clone of https://huggingface.co/XaviXva/distilbert-base-uncased-finetuned-paws. Make sure you pull the latest changes with `repo.git_pull()`. WARNING:huggingface_hub.repository:/content/notebooks/distilbert-base-uncased-finetuned-paws is already a clone of https://huggingface.co/XaviXva/distilbert-base-uncased-finetuned-paws. Make sure you pull the latest changes with `repo.git_pull()`. . . [1544/1544 20:05, Epoch 2/2] Epoch Training Loss Validation Loss Accuracy F1 . 1 | 0.671500 | 0.598167 | 0.678500 | 0.679950 | . 2 | 0.427800 | 0.385005 | 0.835500 | 0.836158 | . &lt;/div&gt; &lt;/div&gt; Several commits (4) will be pushed upstream. WARNING:huggingface_hub.repository:Several commits (4) will be pushed upstream. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; This looks more promising than the simple classifier. We can take a deeper look by checking the predictions and the confusion matrix on the validation set. . preds_output = trainer.predict(paws_encoded[&quot;validation&quot;]) . preds_output.metrics . {&#39;test_loss&#39;: 0.38500452041625977, &#39;test_accuracy&#39;: 0.8355, &#39;test_f1&#39;: 0.8361579553422098, &#39;test_runtime&#39;: 6.5148, &#39;test_samples_per_second&#39;: 306.991, &#39;test_steps_per_second&#39;: 4.912} . y_preds = np.argmax(preds_output.predictions, axis=1) plot_confusion_matrix(y_preds, y_valid, labels) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; . Note: It impresses me to achieve relatively good results with only two epochs of training. . So it definitely performs better. We can look inside the model to see what is exactly the classification head that has been added. . model . DistilBertForSequenceClassification( (distilbert): DistilBertModel( (embeddings): Embeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (transformer): Transformer( (layer): ModuleList( (0): TransformerBlock( (attention): MultiHeadSelfAttention( (dropout): Dropout(p=0.1, inplace=False) (q_lin): Linear(in_features=768, out_features=768, bias=True) (k_lin): Linear(in_features=768, out_features=768, bias=True) (v_lin): Linear(in_features=768, out_features=768, bias=True) (out_lin): Linear(in_features=768, out_features=768, bias=True) ) (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FFN( (dropout): Dropout(p=0.1, inplace=False) (lin1): Linear(in_features=768, out_features=3072, bias=True) (lin2): Linear(in_features=3072, out_features=768, bias=True) ) (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) (1): TransformerBlock( (attention): MultiHeadSelfAttention( (dropout): Dropout(p=0.1, inplace=False) (q_lin): Linear(in_features=768, out_features=768, bias=True) (k_lin): Linear(in_features=768, out_features=768, bias=True) (v_lin): Linear(in_features=768, out_features=768, bias=True) (out_lin): Linear(in_features=768, out_features=768, bias=True) ) (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FFN( (dropout): Dropout(p=0.1, inplace=False) (lin1): Linear(in_features=768, out_features=3072, bias=True) (lin2): Linear(in_features=3072, out_features=768, bias=True) ) (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) (2): TransformerBlock( (attention): MultiHeadSelfAttention( (dropout): Dropout(p=0.1, inplace=False) (q_lin): Linear(in_features=768, out_features=768, bias=True) (k_lin): Linear(in_features=768, out_features=768, bias=True) (v_lin): Linear(in_features=768, out_features=768, bias=True) (out_lin): Linear(in_features=768, out_features=768, bias=True) ) (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FFN( (dropout): Dropout(p=0.1, inplace=False) (lin1): Linear(in_features=768, out_features=3072, bias=True) (lin2): Linear(in_features=3072, out_features=768, bias=True) ) (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) (3): TransformerBlock( (attention): MultiHeadSelfAttention( (dropout): Dropout(p=0.1, inplace=False) (q_lin): Linear(in_features=768, out_features=768, bias=True) (k_lin): Linear(in_features=768, out_features=768, bias=True) (v_lin): Linear(in_features=768, out_features=768, bias=True) (out_lin): Linear(in_features=768, out_features=768, bias=True) ) (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FFN( (dropout): Dropout(p=0.1, inplace=False) (lin1): Linear(in_features=768, out_features=3072, bias=True) (lin2): Linear(in_features=3072, out_features=768, bias=True) ) (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) (4): TransformerBlock( (attention): MultiHeadSelfAttention( (dropout): Dropout(p=0.1, inplace=False) (q_lin): Linear(in_features=768, out_features=768, bias=True) (k_lin): Linear(in_features=768, out_features=768, bias=True) (v_lin): Linear(in_features=768, out_features=768, bias=True) (out_lin): Linear(in_features=768, out_features=768, bias=True) ) (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FFN( (dropout): Dropout(p=0.1, inplace=False) (lin1): Linear(in_features=768, out_features=3072, bias=True) (lin2): Linear(in_features=3072, out_features=768, bias=True) ) (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) (5): TransformerBlock( (attention): MultiHeadSelfAttention( (dropout): Dropout(p=0.1, inplace=False) (q_lin): Linear(in_features=768, out_features=768, bias=True) (k_lin): Linear(in_features=768, out_features=768, bias=True) (v_lin): Linear(in_features=768, out_features=768, bias=True) (out_lin): Linear(in_features=768, out_features=768, bias=True) ) (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FFN( (dropout): Dropout(p=0.1, inplace=False) (lin1): Linear(in_features=768, out_features=3072, bias=True) (lin2): Linear(in_features=3072, out_features=768, bias=True) ) (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) ) ) ) (pre_classifier): Linear(in_features=768, out_features=768, bias=True) (classifier): Linear(in_features=768, out_features=2, bias=True) (dropout): Dropout(p=0.2, inplace=False) ) . . So our head that we have added are: . pre_classifier | classifier | doropout | . from torch import nn head = nn.Sequential(model.pre_classifier, model.classifier, model.dropout) head . Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=2, bias=True) (2): Dropout(p=0.2, inplace=False) ) . num_counted_elements = 0 for param in head.parameters(): num_counted_elements += param.numel() print(&quot;The number of parameters of the head is: &quot; + str(num_counted_elements)) . The number of parameters of the head is: 592130 . So we have added a way more powerful classifier that the classical one. Moreover remember now we are backpropagating through all the model. So we are training even more parameters. . num_counted_elements = 0 for param in model.parameters(): num_counted_elements += param.numel() print(&quot;The number of parameters of the model is: &quot; + str(num_counted_elements)) . The number of parameters of the model is: 66955010 . Error analysis . For our error analysis we will use Binary Cross Entropy loss because we only have two classes. . from torch.nn.functional import binary_cross_entropy_with_logits def forward_pass_with_label(batch): # Place all input tensors on the same device as the model inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names} with torch.no_grad(): output = model(**inputs) pred_label = torch.argmax(output.logits, axis=-1) # note we use output.logits[:,:,1] loss = binary_cross_entropy_with_logits(output.logits[:,1], batch[&quot;label&quot;].float().to(device), reduction=&quot;none&quot;) # Place outputs on CPU for compatibility with other dataset columns return {&quot;loss&quot;: loss.cpu().numpy(), &quot;predicted_label&quot;: pred_label.cpu().numpy()} . # Convert our dataset back to PyTorch tensors paws_encoded.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;label&quot;]) # Compute loss values paws_encoded[&quot;validation&quot;] = paws_encoded[&quot;validation&quot;].map( forward_pass_with_label, batched=True, batch_size=16) . paws_encoded.set_format(&quot;pandas&quot;) cols = [&quot;sentence1&quot;,&quot;sentence2&quot;, &quot;label&quot;, &quot;predicted_label&quot;, &quot;loss&quot;] df_test = paws_encoded[&quot;validation&quot;][:][cols] . df_test.sort_values(&quot;loss&quot;, ascending=False).head(10) . sentence1 sentence2 label predicted_label loss . 45 ACVM is headquartered in Edinburgh and has off... | ACVM is based in Glasgow and has subsidiaries ... | 0 | 1 | 1.982430 | . 939 Another way to control the population of deers... | Another way to regulate the population of deer... | 1 | 0 | 1.923286 | . 1257 1i Productions is an American board game publi... | 1i Productions is an American board game , fou... | 0 | 1 | 1.902109 | . 357 The medals were presented by Barbara Kendall ,... | The medals were handed over by Carlo Croce , I... | 1 | 0 | 1.883666 | . 728 In other articles , it applauded the economic ... | In other articles it praised the economic idea... | 1 | 0 | 1.853208 | . 1317 The medals were presented by Barbara Kendall ,... | The medals have been presented by Carlo Croce ... | 1 | 0 | 1.831070 | . 1468 As a small composer in the French school , he ... | A minor composer in the French school , as a c... | 1 | 0 | 1.725615 | . 867 Following her success , Jane Campion hired Jon... | Following her success , Jane Campion Jones was... | 1 | 0 | 1.725041 | . 360 In November 1989 , Delaney became a member of ... | In November 1989 , Delaney became a member of ... | 0 | 1 | 1.698835 | . 1559 Brewarrina Shire comprises Brewarrina and the ... | Brewarrina Shire and the villages of Gongolgon... | 0 | 1 | 1.681595 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . We can see that the model confuses both classes similarly, as is also shown in the confusion matrix. . df_test.sort_values(&quot;loss&quot;, ascending=True).head(10) . sentence1 sentence2 label predicted_label loss . 784 The racial Rubber Bowl was used by the Nationa... | The historic Rubber Bowl was used by the Natio... | 0 | 0 | 0.113665 | . 16 The racial Rubber Bowl was used by the Nationa... | The historic Rubber Bowl was used by the Natio... | 0 | 0 | 0.115467 | . 20 Earl St Vincent was a British ship that was ca... | Earl St Vincent was a French ship that was cap... | 0 | 0 | 0.117033 | . 1440 The historic Rubber Bowl was used by the Natio... | The racial Rubber Bowl was used by the Nationa... | 0 | 0 | 0.117698 | . 1854 Mark Knowles and Daniel Nestor won the title ,... | Jonathan Erlich and Andy Ram won the title and... | 0 | 0 | 0.120314 | . 377 Ruby died in Woodland Hills , California and w... | Ruby died in Los Angeles and was buried in the... | 0 | 0 | 0.120729 | . 111 In 284 BC , King Qi met King Zhao of Qin in we... | In 284 BC , King Xi met with King Zhao of Qin ... | 0 | 0 | 0.121336 | . 786 Between 1940 and 1945 he served as Canada &#39;s f... | Between 1940 and 1945 , he served as Australia... | 0 | 0 | 0.122137 | . 1281 In 284 BC , King Qi met King Zhao of Qin in we... | In 284 BCE , King Xi met with King Zhao of Qin... | 0 | 0 | 0.122312 | . 1226 In the summer of 1924 he went to Smackover in ... | In the summer of 1924 , he went to Union Count... | 0 | 0 | 0.122760 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . Nevertheless, we also see that the predictions he is most confident about are when the sentences are not paraphrasing each other. This may be caused because of the small unbalance in our dataset. So we should account for that when using its predictions. . Saving and sharing . Time to share the model! How to share and get models and datasets from the hub is one of the parts I am most interested to learn about. These things are essential to work in today&#39;s ML industry and research! . trainer.push_to_hub(commit_message=&quot;Training completed!&quot;) . &lt;/div&gt; .",
            "url": "https://xanaga.github.io/posts_en/nlp/text%20classification/reading%20group/2022/10/30/NLP_with_Transformers_Text_Classification.html",
            "relUrl": "/nlp/text%20classification/reading%20group/2022/10/30/NLP_with_Transformers_Text_Classification.html",
            "date": " ‚Ä¢ Oct 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Image Data Augmentations the Fastai's way",
            "content": ". Introducction . In this notebook we will see how data augmentation can help us to increase the performance of our models, hence needed less training time and data. This is a version of a codelab from the Efficient Deep Learning book. The difference with the original one is that we will use a training schedule more used in Fastai which help us to perform even better. . Installation . Install the necessary packages: . Fastai | . Import the libs used in this notebook . Dataset and metrics visualization . APIs: . plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs) | . Metrics . This is a useful function which is part of the timeseriesAI (tsai). It will plot the metrics tracked by Fastai after training . @patch @delegates(subplots) def plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs): metrics = np.stack(self.values) names = self.metric_names[1:-1] n = len(names) - 1 if nrows is None and ncols is None: nrows = int(math.sqrt(n)) ncols = int(np.ceil(n / nrows)) elif nrows is None: nrows = int(np.ceil(n / ncols)) elif ncols is None: ncols = int(np.ceil(n / nrows)) figsize = figsize or (ncols * 6, nrows * 4) fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs) axs = [ax if i &lt; n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n] for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)): ax.plot(metrics[:, i], color=&#39;#1f77b4&#39; if i == 0 else &#39;#ff7f0e&#39;, label=&#39;valid&#39; if i &gt; 0 else &#39;train&#39;) ax.set_title(name if i &gt; 1 else &#39;losses&#39;) ax.legend(loc=&#39;best&#39;) plt.show() . Dataset creation . We use the oxford_flowers102 to demonstrate the sample efficiency improvements. The dataset has integer class labels for the flower images. To identify the flowers by name, use the following links: . Name=&gt;Label | Label=&gt;Visual | . path=untar_data(URLs.FLOWERS) path.ls() . . 100.00% [345243648/345236087 00:08&lt;00:00] (#4) [Path(&#39;/root/.fastai/data/oxford-102-flowers/train.txt&#39;),Path(&#39;/root/.fastai/data/oxford-102-flowers/valid.txt&#39;),Path(&#39;/root/.fastai/data/oxford-102-flowers/jpg&#39;),Path(&#39;/root/.fastai/data/oxford-102-flowers/test.txt&#39;)] . Let&#39;s gather all the information in a single dictionary, it will make things easier. . info = {} # Fill the dictionary with the training data with open(path/&#39;train.txt&#39;) as txtfile: line = txtfile.readline().split() while len(line) &gt; 0: info[path/line[0]] = [line[1], &#39;train&#39;] line = txtfile.readline().split() # Fill the dictionary with the training data with open(path/&#39;valid.txt&#39;) as txtfile: line = txtfile.readline().split() while len(line) &gt; 0: info[path/line[0]] = [line[1], &#39;valid&#39;] line = txtfile.readline().split() # Fill the dictionary with the test data with open(path/&#39;test.txt&#39;) as txtfile: line = txtfile.readline().split() while len(line) &gt; 0: info[path/line[0]] = [line[1], &#39;test&#39;] line = txtfile.readline().split() . Now, we create two auxiliary functions, one for getting the label and the other for getting the split. With those, we will create a Data Block, which defines all the necessary steps to pre-process our data. We can add here the resizing of the image and the normalizations using the statistics of the dataset with which our model was pre-trained. In our case, it is Imagenet. . With the Data Block we can create datasets and dataloaders, by using DataBlock.datasets or DataBlock.dataloaders respectively. . def get_label(file_path): return info[file_path][0] def get_split(file_path): return info[file_path][1]==&#39;valid&#39; def get_image_files_filtered(dir_path): return [dir_path/im for im in os.listdir(dir_path) if info[dir_path/im][1] != &#39;test&#39;] . IMG_SIZE = 264 dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files_filtered, splitter=FuncSplitter(get_split), get_y=get_label, item_tfms = Resize(IMG_SIZE), batch_tfms = Normalize.from_stats(*imagenet_stats)) . ds = dblock.datasets(path/&#39;jpg&#39;) ds.decode . &lt;bound method Datasets.decode of (#2040) [(PILImage mode=RGB size=694x500, TensorCategory(83)),(PILImage mode=RGB size=500x509, TensorCategory(72)),(PILImage mode=RGB size=627x500, TensorCategory(83)),(PILImage mode=RGB size=500x607, TensorCategory(81)),(PILImage mode=RGB size=667x501, TensorCategory(79)),(PILImage mode=RGB size=752x500, TensorCategory(74)),(PILImage mode=RGB size=667x500, TensorCategory(19)),(PILImage mode=RGB size=635x500, TensorCategory(27)),(PILImage mode=RGB size=666x500, TensorCategory(58)),(PILImage mode=RGB size=542x500, TensorCategory(47))...]&gt; . We see that the images are not resized, not even Tensors. This is because Fastai will only apply these transformations when getting the elements. In fact, the item_tfms will be applied to every item individually, and batch_tfms will be applied very efficiently to a whole batch. . We can check what will be the input of the model by creating the dataloaders. . dl = dblock.dataloaders(path/&#39;jpg&#39;) b=dl.one_batch() b[0].shape . torch.Size([64, 3, 264, 264]) . Moreover, Fastai allow us to nicely visualize a batch from our trainig or validation dataloaders. . dl.show_batch() . Model architecture . We use a ResNet50 model pre-trained on the ImageNet dataset. The oxford_flowers102 is a small dataset with 1020 training images and 1020 validation images. Such a small sample is insufficient to train a model from scratch. However, with the pre-trained weights, this dataset is more than enough to achieve decent predictions. The model is exposed through a model variable. . Fastai provides us with an easy way the pretrained architecture to the specific problem that we are treating. We will create a learner which wraps up the dataloaders, the models and the metrics to be tracked, as well as other configurations regarding the training such as dropout probability here ps, or the learning rate. . DROPOUT_RATE = 0.2 LEARNING_RATE = 0.0002 learn = vision_learner(dl, resnet50, metrics=accuracy, ps = DROPOUT_RATE, lr = LEARNING_RATE) learn.summary() . . Downloading: &#34;https://download.pytorch.org/models/resnet50-0676ba61.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth . Sequential (Input shape: 64 x 3 x 264 x 264) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 132 x 132 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 64 x 64 x 66 x 66 MaxPool2d Conv2d 4096 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 256 x 66 x 66 Conv2d 16384 False BatchNorm2d 512 True ReLU Conv2d 16384 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 64 x 66 x 66 Conv2d 16384 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 256 x 66 x 66 Conv2d 16384 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 64 x 64 x 66 x 66 Conv2d 16384 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 256 x 66 x 66 Conv2d 16384 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 64 x 128 x 66 x 66 Conv2d 32768 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU Conv2d 131072 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 65536 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 65536 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 65536 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 64 x 256 x 33 x 33 Conv2d 131072 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU Conv2d 524288 False BatchNorm2d 2048 True ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 512 x 17 x 17 Conv2d 524288 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 512 x 9 x 9 Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 2048 x 9 x 9 Conv2d 1048576 False BatchNorm2d 4096 True ReLU Conv2d 2097152 False BatchNorm2d 4096 True ____________________________________________________________________________ 64 x 512 x 9 x 9 Conv2d 1048576 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 2048 x 9 x 9 Conv2d 1048576 False BatchNorm2d 4096 True ReLU ____________________________________________________________________________ 64 x 512 x 9 x 9 Conv2d 1048576 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 2048 x 9 x 9 Conv2d 1048576 False BatchNorm2d 4096 True ReLU ____________________________________________________________________________ 64 x 2048 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 64 x 4096 Flatten BatchNorm1d 8192 True Dropout ____________________________________________________________________________ 64 x 512 Linear 2097152 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 64 x 102 Linear 52224 True ____________________________________________________________________________ Total params: 25,666,624 Total trainable params: 2,211,712 Total non-trainable params: 23,454,912 Optimizer used: &lt;function Adam at 0x7f07d68fbdd0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Quick Note: As we can see there are some discrepancies between our model and the one presented in the book, summarized in the picture below. . . Batch size: It is visible in our case. Why is that? Because the learner also wraps up the dataloadoader, it knows what is the batch size, by default 64. . | Head: As mentioned, Fastai does two things automatically for us. First, it decides where to &quot;cut&quot; the head of our model, and then it replaces it with a non-trained head, different for each architecture. This can be done automatically because resnet50 is a well-known architecture, but if we want to use another one we have tools that allow us to do this. In our case instead of just having a linear layer right after the last B x 2048 x 9 x 9 convolutional block, we have an adaptative pooling followed by two smaller linear layers. . | Trainable params: Those are all the parameters from the new custom head, as the rest of the model will be frozen. Despite our head seeming more complex than simply adding a linear layer (like in the book), we can see it reduces the number of trainable parameters from 16,920,678 to 2,211,712, so we have x7 fewer parameters to train. Efficient architectures will be further discussed in Chapter 4. . | Non-Trainable params: The discrepancy in those numbers is caused by how Pytorch and Tensorflow count the number of parameters of BatchNorm layers, explained in Chapter 2. . | . To make it as simmilar as the codelab from the &quot;Efficient Deep Learning Book&quot; the following code will create a learner with an identical model architecture as the one presented there, being the head a single linear layer. Although, you are encouraged to try with both settings! . custom_head = nn.Sequential(nn.Flatten(), nn.Dropout(DROPOUT_RATE), nn.Linear(165888, 102)).to(device) learn = vision_learner(dl, resnet50, metrics=accuracy, ps = DROPOUT_RATE, lr = LEARNING_RATE, custom_head = custom_head) learn.summary() . . Sequential (Input shape: 64 x 3 x 264 x 264) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 132 x 132 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 64 x 64 x 66 x 66 MaxPool2d Conv2d 4096 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 256 x 66 x 66 Conv2d 16384 False BatchNorm2d 512 True ReLU Conv2d 16384 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 64 x 66 x 66 Conv2d 16384 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 256 x 66 x 66 Conv2d 16384 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 64 x 64 x 66 x 66 Conv2d 16384 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 256 x 66 x 66 Conv2d 16384 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 64 x 128 x 66 x 66 Conv2d 32768 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU Conv2d 131072 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 65536 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 65536 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 64 x 128 x 33 x 33 Conv2d 65536 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 512 x 33 x 33 Conv2d 65536 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 64 x 256 x 33 x 33 Conv2d 131072 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU Conv2d 524288 False BatchNorm2d 2048 True ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 256 x 17 x 17 Conv2d 262144 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 1024 x 17 x 17 Conv2d 262144 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 64 x 512 x 17 x 17 Conv2d 524288 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 512 x 9 x 9 Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 2048 x 9 x 9 Conv2d 1048576 False BatchNorm2d 4096 True ReLU Conv2d 2097152 False BatchNorm2d 4096 True ____________________________________________________________________________ 64 x 512 x 9 x 9 Conv2d 1048576 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 2048 x 9 x 9 Conv2d 1048576 False BatchNorm2d 4096 True ReLU ____________________________________________________________________________ 64 x 512 x 9 x 9 Conv2d 1048576 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 2048 x 9 x 9 Conv2d 1048576 False BatchNorm2d 4096 True ReLU ____________________________________________________________________________ 64 x 165888 Flatten Dropout ____________________________________________________________________________ 64 x 102 Linear 16920678 True ____________________________________________________________________________ Total params: 40,428,710 Total trainable params: 16,973,798 Total non-trainable params: 23,454,912 Optimizer used: &lt;function Adam at 0x7f07d68fbdd0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Training Setup . def train(learner, epochs=100): # Save the best weights during training learner.add_cb(SaveModelCallback(monitor=&#39;accuracy&#39;, comp=np.greater, fname=&#39;best_model_acc&#39;)) learner.fit(epochs) . Experiments . baseline . dl = dblock.dataloaders(path/&#39;jpg&#39;, bs=24) learn = vision_learner(dl, resnet50, metrics=accuracy, ps = DROPOUT_RATE, cbs=SaveModelCallback(monitor=&#39;accuracy&#39;, comp=np.greater, fname=&#39;best_model_acc&#39;)) learn.lr_find() . SuggestedLRs(valley=0.0008317637839354575) . learn.fit_one_cycle(10, lr_max=0.001) . epoch train_loss valid_loss accuracy time . 0 | 4.840006 | 2.875439 | 0.359804 | 00:23 | . 1 | 2.437588 | 0.948147 | 0.758824 | 00:24 | . 2 | 1.112089 | 0.672860 | 0.843137 | 00:23 | . 3 | 0.533977 | 0.618589 | 0.851961 | 00:24 | . 4 | 0.260300 | 0.556569 | 0.862745 | 00:24 | . 5 | 0.135446 | 0.530464 | 0.868627 | 00:24 | . 6 | 0.073677 | 0.507725 | 0.874510 | 00:24 | . 7 | 0.046662 | 0.490954 | 0.883333 | 00:24 | . 8 | 0.028372 | 0.484241 | 0.878431 | 00:24 | . 9 | 0.022570 | 0.482903 | 0.873529 | 00:24 | . Better model found at epoch 0 with accuracy value: 0.3598039150238037. Better model found at epoch 1 with accuracy value: 0.7588235139846802. Better model found at epoch 2 with accuracy value: 0.843137264251709. Better model found at epoch 3 with accuracy value: 0.8519607782363892. Better model found at epoch 4 with accuracy value: 0.8627451062202454. Better model found at epoch 5 with accuracy value: 0.8686274290084839. Better model found at epoch 6 with accuracy value: 0.8745098114013672. Better model found at epoch 7 with accuracy value: 0.8833333253860474. . learn.load(&#39;best_model_acc&#39;) learn.unfreeze() learn.lr_find() . SuggestedLRs(valley=5.248074739938602e-05) . learn.fit_one_cycle(15, lr_max=slice(1e-5, 1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.020012 | 0.486254 | 0.876471 | 00:28 | . 1 | 0.015546 | 0.460560 | 0.884314 | 00:29 | . 2 | 0.012389 | 0.435500 | 0.889216 | 00:28 | . 3 | 0.013358 | 0.441101 | 0.898039 | 00:28 | . 4 | 0.011623 | 0.420933 | 0.900000 | 00:28 | . 5 | 0.010642 | 0.434887 | 0.887255 | 00:28 | . 6 | 0.009612 | 0.423229 | 0.884314 | 00:28 | . 7 | 0.008490 | 0.406901 | 0.895098 | 00:28 | . 8 | 0.007193 | 0.401632 | 0.894118 | 00:28 | . 9 | 0.005040 | 0.392224 | 0.895098 | 00:28 | . 10 | 0.003902 | 0.388078 | 0.895098 | 00:28 | . 11 | 0.003452 | 0.378735 | 0.900000 | 00:28 | . 12 | 0.003294 | 0.378925 | 0.904902 | 00:28 | . 13 | 0.003102 | 0.371340 | 0.903922 | 00:28 | . 14 | 0.002509 | 0.375474 | 0.900980 | 00:28 | . Better model found at epoch 0 with accuracy value: 0.8764705657958984. Better model found at epoch 1 with accuracy value: 0.884313702583313. Better model found at epoch 2 with accuracy value: 0.8892157077789307. Better model found at epoch 3 with accuracy value: 0.8980392217636108. Better model found at epoch 4 with accuracy value: 0.8999999761581421. Better model found at epoch 12 with accuracy value: 0.9049019813537598. . learn.load(&#39;best_model_acc&#39;) learn.lr_find() . /usr/local/lib/python3.7/dist-packages/fastai/learner.py:56: UserWarning: Saved filed doesn&#39;t contain an optimizer state. elif with_opt: warn(&#34;Saved filed doesn&#39;t contain an optimizer state.&#34;) . SuggestedLRs(valley=3.630780702224001e-05) . learn.fit_one_cycle(10, lr_max=slice(1e-7, 1e-6)) . epoch train_loss valid_loss accuracy time . 0 | 0.002154 | 0.377533 | 0.897059 | 00:29 | . 1 | 0.002287 | 0.368967 | 0.903922 | 00:29 | . 2 | 0.002264 | 0.371477 | 0.903922 | 00:28 | . 3 | 0.002178 | 0.371252 | 0.900000 | 00:28 | . 4 | 0.002411 | 0.375013 | 0.900000 | 00:28 | . 5 | 0.002336 | 0.375906 | 0.896078 | 00:28 | . 6 | 0.002331 | 0.373897 | 0.905882 | 00:28 | . 7 | 0.002454 | 0.375649 | 0.900000 | 00:28 | . 8 | 0.002235 | 0.375499 | 0.902941 | 00:28 | . 9 | 0.001994 | 0.375757 | 0.901961 | 00:28 | . Better model found at epoch 0 with accuracy value: 0.8970588445663452. Better model found at epoch 1 with accuracy value: 0.9039215445518494. Better model found at epoch 6 with accuracy value: 0.9058823585510254. . learn.recorder.plot_metrics() . basicaug-hflip-rotate-0.1 . Fastai also provides a combination of transformations that can be applied to our data. Those are used with the aug_transforms function. . By setting the correct arguments (shown in the code) we can replicate the behavior of just using horizontal flipping and a random rotation with a probability 0&#39;1. Although, it is very easy and straightforward to combine different augmentations including lighting and more complex affine transformations. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files_filtered, splitter=FuncSplitter(get_split), get_y=get_label, item_tfms = Resize(IMG_SIZE), batch_tfms = [*aug_transforms(mult=2.5), Normalize.from_stats(*imagenet_stats)]) dl = dblock.dataloaders(path/&#39;jpg&#39;, bs=24) . We can check how an image will be augmented before going into the network using the show_batch function with the attribute unique . dl.show_batch(unique=True) . Just as an example of how different transformation can be applied we will create a toy dataloader. You can skip this cell if you want. . learn = vision_learner(dl, resnet50, metrics=accuracy, ps = DROPOUT_RATE, wd=5e-4, cbs=SaveModelCallback(monitor=&#39;accuracy&#39;, comp=np.greater, fname=&#39;best_model_acc&#39;)) learn.lr_find() . SuggestedLRs(valley=0.0020892962347716093) . learn.fit_one_cycle(15, lr_max=0.001) . epoch train_loss valid_loss accuracy time . 0 | 5.362812 | 3.990877 | 0.114706 | 00:25 | . 1 | 3.666749 | 1.515508 | 0.636275 | 00:24 | . 2 | 2.252049 | 0.835759 | 0.783333 | 00:25 | . 3 | 1.345704 | 0.586028 | 0.849020 | 00:25 | . 4 | 0.878627 | 0.514670 | 0.862745 | 00:25 | . 5 | 0.580246 | 0.450729 | 0.872549 | 00:25 | . 6 | 0.430550 | 0.451317 | 0.882353 | 00:25 | . 7 | 0.327473 | 0.426270 | 0.888235 | 00:25 | . 8 | 0.280518 | 0.391178 | 0.896078 | 00:25 | . 9 | 0.224007 | 0.385979 | 0.894118 | 00:25 | . 10 | 0.177495 | 0.332147 | 0.910784 | 00:25 | . 11 | 0.160216 | 0.317414 | 0.915686 | 00:25 | . 12 | 0.136054 | 0.311331 | 0.916667 | 00:25 | . 13 | 0.125442 | 0.304116 | 0.918627 | 00:25 | . 14 | 0.120099 | 0.308151 | 0.916667 | 00:25 | . Better model found at epoch 0 with accuracy value: 0.1147058829665184. Better model found at epoch 1 with accuracy value: 0.636274516582489. Better model found at epoch 2 with accuracy value: 0.7833333611488342. Better model found at epoch 3 with accuracy value: 0.8490195870399475. Better model found at epoch 4 with accuracy value: 0.8627451062202454. Better model found at epoch 5 with accuracy value: 0.8725489974021912. Better model found at epoch 6 with accuracy value: 0.8823529481887817. Better model found at epoch 7 with accuracy value: 0.8882352709770203. Better model found at epoch 8 with accuracy value: 0.8960784077644348. Better model found at epoch 10 with accuracy value: 0.9107843041419983. Better model found at epoch 11 with accuracy value: 0.9156862497329712. Better model found at epoch 12 with accuracy value: 0.9166666865348816. Better model found at epoch 13 with accuracy value: 0.9186274409294128. . learn.recorder.plot_metrics() . learn = vision_learner(dl, resnet50, metrics=accuracy, ps = DROPOUT_RATE, wd=5e-4, cbs=SaveModelCallback(monitor=&#39;accuracy&#39;, comp=np.greater, fname=&#39;best_model_acc&#39;)) learn.load(&#39;best_model_acc&#39;) learn.unfreeze() learn.lr_find() . /usr/local/lib/python3.7/dist-packages/fastai/learner.py:56: UserWarning: Saved filed doesn&#39;t contain an optimizer state. elif with_opt: warn(&#34;Saved filed doesn&#39;t contain an optimizer state.&#34;) . SuggestedLRs(valley=1.5848931980144698e-06) . learn.fit_one_cycle(10, lr_max=slice(1e-5, 5e-5)) . epoch train_loss valid_loss accuracy time . 0 | 0.164312 | 0.306754 | 0.913725 | 00:30 | . 1 | 0.129729 | 0.307294 | 0.916667 | 00:30 | . 2 | 0.124749 | 0.301241 | 0.917647 | 00:30 | . 3 | 0.116227 | 0.293344 | 0.925490 | 00:30 | . 4 | 0.103746 | 0.300047 | 0.923529 | 00:30 | . 5 | 0.103349 | 0.290642 | 0.932353 | 00:30 | . 6 | 0.119844 | 0.282914 | 0.929412 | 00:30 | . 7 | 0.114174 | 0.283348 | 0.926471 | 00:30 | . 8 | 0.092939 | 0.284805 | 0.926471 | 00:30 | . 9 | 0.085321 | 0.282970 | 0.926471 | 00:30 | . Better model found at epoch 0 with accuracy value: 0.9137254953384399. Better model found at epoch 1 with accuracy value: 0.9166666865348816. Better model found at epoch 2 with accuracy value: 0.9176470637321472. Better model found at epoch 3 with accuracy value: 0.9254902005195618. Better model found at epoch 5 with accuracy value: 0.9323529601097107. . learn = vision_learner(dl, resnet50, metrics=accuracy, ps = DROPOUT_RATE, wd=5e-4, cbs=SaveModelCallback(monitor=&#39;accuracy&#39;, comp=np.greater, fname=&#39;best_model_acc&#39;)) learn.load(&#39;best_model_acc&#39;) learn.lr_find() . /usr/local/lib/python3.7/dist-packages/fastai/learner.py:56: UserWarning: Saved filed doesn&#39;t contain an optimizer state. elif with_opt: warn(&#34;Saved filed doesn&#39;t contain an optimizer state.&#34;) . SuggestedLRs(valley=2.75422871709452e-06) . learn.fit_one_cycle(10, lr_max=slice(5e-7, 1e-6)) . epoch train_loss valid_loss accuracy time . 0 | 0.111675 | 0.289659 | 0.928431 | 00:25 | . 1 | 0.101360 | 0.286623 | 0.925490 | 00:25 | . 2 | 0.094691 | 0.285140 | 0.930392 | 00:25 | . 3 | 0.085991 | 0.285435 | 0.929412 | 00:25 | . 4 | 0.107634 | 0.291907 | 0.927451 | 00:25 | . 5 | 0.101778 | 0.281488 | 0.931373 | 00:25 | . 6 | 0.103773 | 0.282004 | 0.932353 | 00:25 | . 7 | 0.092038 | 0.280995 | 0.926471 | 00:25 | . 8 | 0.115465 | 0.281418 | 0.925490 | 00:25 | . 9 | 0.113581 | 0.286373 | 0.926471 | 00:25 | . Better model found at epoch 0 with accuracy value: 0.9284313917160034. Better model found at epoch 2 with accuracy value: 0.9303921461105347. Better model found at epoch 5 with accuracy value: 0.9313725233078003. Better model found at epoch 6 with accuracy value: 0.9323529601097107. .",
            "url": "https://xanaga.github.io/posts_en/data_augmentation/vision/efficient_deeplearning/2022/06/09/Image_Data_Augmentations_the_Fastai's_way.html",
            "relUrl": "/data_augmentation/vision/efficient_deeplearning/2022/06/09/Image_Data_Augmentations_the_Fastai's_way.html",
            "date": " ‚Ä¢ Jun 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Improving quantization by normalizing weights",
            "content": ". Introduction . I have hidden all the code from the previous post so we can focus on the experiments but you can find it if you download the notebook or open it in colab. . Collecting the Dataset . Visualize the dataset . Remember the model class definition. . class CNN(nn.Module): def __init__(self, dropout_rate=0.0, norm_w=True): super(CNN, self).__init__() self.norm_w = norm_w # Create the layers normalizing or not the weigths if self.norm_w: self.conv_layer1 = nn.utils.weight_norm(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3), name=&#39;weight&#39;) self.conv_layer2 = nn.utils.weight_norm(nn.Conv2d(32, 64, 3), name=&#39;weight&#39;) self.fc = nn.utils.weight_norm(nn.Linear(in_features= 64*5*5, out_features=10), name=&#39;weight&#39;) else: self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) self.conv_layer2 = nn.Conv2d(32, 64, 3) self.fc = nn.Linear(in_features= 64*5*5, out_features=10) # input dimensions are Bx1x28x28 (BxCxHxW) self.batch_norm1 = nn.BatchNorm2d(32) self.relu = nn.ReLU() self.pool = nn.MaxPool2d(kernel_size=2,stride=2) self.batch_norm2 = nn.BatchNorm2d(64) self.flat = nn.Flatten() self.drop = nn.Dropout(p=dropout_rate) def forward(self, x): # Block 1 out = self.conv_layer1(x) out = self.batch_norm1(out) out = self.relu(out) out = self.pool(out) # Block 2 out = self.conv_layer2(out) out = self.batch_norm2(out) out = self.relu(out) out = self.pool(out) # Flatten the output using BxC*H*W out = self.flat(out) out = self.drop(out) out = self.fc(out) return out def add_quant(self): &#39;&#39;&#39; Returns a new model with added quantization layers &#39;&#39;&#39; return nn.Sequential(torch.quantization.QuantStub(), self, torch.quantization.DeQuantStub()) . Training without PyTorch Model Quantization . Helper functions for Pytorch Quantizantion and evaluation . Chain everything together in a single train function . Experiments . Let&#39;s simulate that it is our first time facing this problem, so we haven&#39;t thought about normalizing the weights. . model_15, _ = train_model_and_quantize(epochs=15, norm_w=False) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 2.53 s - Loss: 0.2702 - Categorical_Accuracy: 0.9447 - Val_Loss: 0.0466 - Categorical_Val_Accuracy: 0.9848 Epoch 2 Time: 2.21 s - Loss: 0.0523 - Categorical_Accuracy: 0.9837 - Val_Loss: 0.0416 - Categorical_Val_Accuracy: 0.9869 Epoch 3 Time: 2.21 s - Loss: 0.0358 - Categorical_Accuracy: 0.9885 - Val_Loss: 0.0286 - Categorical_Val_Accuracy: 0.9904 Epoch 4 Time: 2.20 s - Loss: 0.0291 - Categorical_Accuracy: 0.9907 - Val_Loss: 0.0337 - Categorical_Val_Accuracy: 0.9888 Epoch 5 Time: 2.20 s - Loss: 0.0250 - Categorical_Accuracy: 0.9920 - Val_Loss: 0.0335 - Categorical_Val_Accuracy: 0.9909 Epoch 6 Time: 2.22 s - Loss: 0.0204 - Categorical_Accuracy: 0.9937 - Val_Loss: 0.0306 - Categorical_Val_Accuracy: 0.9898 Epoch 7 Time: 2.26 s - Loss: 0.0183 - Categorical_Accuracy: 0.9945 - Val_Loss: 0.0419 - Categorical_Val_Accuracy: 0.9863 Epoch 8 Time: 2.23 s - Loss: 0.0168 - Categorical_Accuracy: 0.9946 - Val_Loss: 0.0379 - Categorical_Val_Accuracy: 0.9900 Epoch 9 Time: 2.23 s - Loss: 0.0176 - Categorical_Accuracy: 0.9941 - Val_Loss: 0.0436 - Categorical_Val_Accuracy: 0.9882 Epoch 10 Time: 2.23 s - Loss: 0.0159 - Categorical_Accuracy: 0.9945 - Val_Loss: 0.0376 - Categorical_Val_Accuracy: 0.9889 Epoch 11 Time: 2.21 s - Loss: 0.0159 - Categorical_Accuracy: 0.9949 - Val_Loss: 0.0433 - Categorical_Val_Accuracy: 0.9881 Epoch 12 Time: 2.21 s - Loss: 0.0132 - Categorical_Accuracy: 0.9954 - Val_Loss: 0.0428 - Categorical_Val_Accuracy: 0.9894 Epoch 13 Time: 2.21 s - Loss: 0.0143 - Categorical_Accuracy: 0.9956 - Val_Loss: 0.0443 - Categorical_Val_Accuracy: 0.9884 Epoch 14 Time: 2.22 s - Loss: 0.0113 - Categorical_Accuracy: 0.9962 - Val_Loss: 0.0397 - Categorical_Val_Accuracy: 0.9896 Epoch 15 Time: 2.23 s - Loss: 0.0132 - Categorical_Accuracy: 0.9961 - Val_Loss: 0.0371 - Categorical_Val_Accuracy: 0.9895 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9895 Eval time: 5.13s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.6951 Eval time: 6.34s . As we can see, the drop in the performance of the quantized model is remarkable. What might have happened? Why the results are way better when using Keras? . We could try training for just 1 epoch and see what happend . model_1, _ = train_model_and_quantize(epochs=1, norm_w=False) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 2.34 s - Loss: 0.2260 - Categorical_Accuracy: 0.9505 - Val_Loss: 0.0466 - Categorical_Val_Accuracy: 0.9838 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9838 Eval time: 5.25s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.9424 Eval time: 6.03s . The result of the quantization is better now. What is happening? . The first thing that comes to my mind is taking a look at the weightsüîç . import matplotlib.pyplot as plt import numpy as np %matplotlib inline def vis_model(model): w = [] for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): w.extend(np.array(m.weight.data.cpu()).flatten()) plt.hist(w, density=True, bins=128) plt.ylabel(&#39;Ocurrences&#39;) plt.xlabel(&#39;Weight&#39;) . vis_model(model_15) . vis_model(model_1) . So the weights of the model trained for 15 epochs are larger, may that be what is causing the problem? . We can see how weights are distributed in the Keras model and we will see the wights are smaller (more similar to the model trained for one epoch). . . The explanation may be that with larger weights the minimum and the maximum are further apart so the &quot;resolution&quot; of the quantization is worse. One quick fix could be clamping the weights, so they can not be larger or smaller than a certain threshold. But this will make that our model just learn in the few first iterations. It will produce a histogram like this one: . . A better option would be normalizing the weights so let&#39;s try it. . model_15_norm, _ = train_model_and_quantize(epochs=15, norm_w=True) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 2.44 s - Loss: 0.1988 - Categorical_Accuracy: 0.9498 - Val_Loss: 0.0593 - Categorical_Val_Accuracy: 0.9806 Epoch 2 Time: 2.36 s - Loss: 0.0509 - Categorical_Accuracy: 0.9848 - Val_Loss: 0.0391 - Categorical_Val_Accuracy: 0.9870 Epoch 3 Time: 2.51 s - Loss: 0.0383 - Categorical_Accuracy: 0.9880 - Val_Loss: 0.0392 - Categorical_Val_Accuracy: 0.9871 Epoch 4 Time: 2.34 s - Loss: 0.0325 - Categorical_Accuracy: 0.9901 - Val_Loss: 0.0304 - Categorical_Val_Accuracy: 0.9897 Epoch 5 Time: 2.37 s - Loss: 0.0279 - Categorical_Accuracy: 0.9909 - Val_Loss: 0.0394 - Categorical_Val_Accuracy: 0.9877 Epoch 6 Time: 2.39 s - Loss: 0.0237 - Categorical_Accuracy: 0.9928 - Val_Loss: 0.0339 - Categorical_Val_Accuracy: 0.9896 Epoch 7 Time: 2.33 s - Loss: 0.0208 - Categorical_Accuracy: 0.9933 - Val_Loss: 0.0284 - Categorical_Val_Accuracy: 0.9899 Epoch 8 Time: 2.34 s - Loss: 0.0171 - Categorical_Accuracy: 0.9948 - Val_Loss: 0.0386 - Categorical_Val_Accuracy: 0.9869 Epoch 9 Time: 2.35 s - Loss: 0.0154 - Categorical_Accuracy: 0.9950 - Val_Loss: 0.0476 - Categorical_Val_Accuracy: 0.9849 Epoch 10 Time: 2.34 s - Loss: 0.0147 - Categorical_Accuracy: 0.9952 - Val_Loss: 0.0313 - Categorical_Val_Accuracy: 0.9903 Epoch 11 Time: 2.34 s - Loss: 0.0116 - Categorical_Accuracy: 0.9961 - Val_Loss: 0.0375 - Categorical_Val_Accuracy: 0.9885 Epoch 12 Time: 2.35 s - Loss: 0.0124 - Categorical_Accuracy: 0.9960 - Val_Loss: 0.0399 - Categorical_Val_Accuracy: 0.9896 Epoch 13 Time: 2.34 s - Loss: 0.0099 - Categorical_Accuracy: 0.9969 - Val_Loss: 0.0283 - Categorical_Val_Accuracy: 0.9918 Epoch 14 Time: 2.36 s - Loss: 0.0083 - Categorical_Accuracy: 0.9973 - Val_Loss: 0.0290 - Categorical_Val_Accuracy: 0.9913 Epoch 15 Time: 2.43 s - Loss: 0.0067 - Categorical_Accuracy: 0.9977 - Val_Loss: 0.0337 - Categorical_Val_Accuracy: 0.9907 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9907 Eval time: 5.51s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.9838 Eval time: 6.13s . We get better results! Lower loss in performance while being able to reduce significantly the size of the model. . And as we can see the weights are now normalized. . vis_model(model_15_norm) . Conclusion . After these experiments, the main takeaways would be the following: . We can look at the network&#39;s weights and see what is happening. It is a good practice and provides some understanding of how the model is being/has being trained. . | The values that our weight take are really important in quantization. . | We have to be careful with the weights not getting too large in our network. As we reward the outputs with higher values is feasible that the model tries to make its weights as big as possible. . | Normalizing the weight can lead to a better quantized models. Nevertheless, it can increase the training time with the same number of epochs. But it often converges faster, so fewer epochs are needed. You will have to study your problem carefully. No one said it was easy! . | .",
            "url": "https://xanaga.github.io/posts_en/quantization/efficient_deeplearning/2022/05/30/Improving_quantization_by_normalizing_weights.html",
            "relUrl": "/quantization/efficient_deeplearning/2022/05/30/Improving_quantization_by_normalizing_weights.html",
            "date": " ‚Ä¢ May 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Quantizing a Deep Learning Model",
            "content": ". Introduction . As the description says introduced, this is a version of a codelab from the &quot;Efficient Deep Learning&quot; book. The book uses TensorFlow so I thought it would be a good idea to try to implement the same ideas with PyTorch. This has the two main objectives: . Encourage others who are more familiar with PyTorch to read the book, as I find efficient deep learning a very important topic. Also, I think the work done by the authors is outstanding so I recommend it to everyone. . | Allow me to practice the concepts of chapter 2. . | . So I will try to make things as similar as possible to the original notebook in case anyone wants to use it to follow the book. I will try not to repeat the explanations made in that notebook and focuse on explaining the changes regarding the PyTorch implementation. . Moreover, there is another post that explores some difficulties I found while doing the PyTorch implementation, so I encourage you to read it as well. . Collecting the Dataset . import numpy as np import time import os import torch import torchvision from torch import nn, optim from torchsummary import summary from tqdm.notebook import tqdm import torch.utils.data as data_utils def process_x(x): &quot;&quot;&quot;Process the given tensors of images.&quot;&quot;&quot; x = x.type(torch.float32) # The original data is in [0.0, 255.0]. # This normalization helps in making them lie between [-1.0, 1.0]. x /= 127.5 x -= 1.0 # Now we have [NxWxH], add one dimension for the channels # We add it to dim 1 because Pytorch image format is [CxWxH] x = torch.unsqueeze(x, dim=1) return x def load_data(ds=torchvision.datasets.MNIST): &quot;&quot;&quot;Returns the processed dataset.&quot;&quot;&quot; training, test = ds(&#39;./data/mnist&#39;, download=True), ds(&#39;./data/mnist&#39;, download=True, train=False) train_images, train_labels = process_x(training.data), training.targets test_images, test_labels = process_x(test.data), test.targets return (train_images, train_labels), (test_images, test_labels) (train_x, train_y), (test_x, test_y) = load_data() . We will have to specify which device we want to use: . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; . Visualize the dataset . The part of visualising the dataset will be the same as in the original notebook. . %matplotlib inline import matplotlib.pyplot as plt def collect_samples(x, y, num_classes=10, num_per_class=5): sampled_x = None sampled_x_idx = 0 for class_idx in range(num_classes): num_collected = 0 for idx in range(len(x)): if num_collected == num_per_class: break if y[idx] != class_idx: continue if sampled_x is not None: sampled_x = np.concatenate((sampled_x, np.expand_dims(x[idx], 0))) else: sampled_x = np.expand_dims(x[idx], 0) sampled_x_idx = sampled_x_idx + 1 num_collected = num_collected + 1 return sampled_x def show_images(images, num_rows = 1, titles = None): &quot;&quot;&quot;Display a list of images in a single figure with matplotlib. &quot;&quot;&quot; # assert((titles is None)or (len(images) == len(titles))) num_images = len(images) num_cols = num_images // num_rows # plot images fig, axes = plt.subplots(num_rows, num_cols, figsize=(1.25 * num_rows, 1.75 * num_cols)) for i in range(num_images): ax = axes[i%num_cols, i//num_cols] ax.axis(&#39;off&#39;) ax.imshow(np.squeeze(images[i]), cmap=&#39;Greys&#39;) # ax.add_axes((0, 0, 1, 1)) if i % num_cols == 0: ax.set_title(&#39;{}&#39;.format(int(i / num_rows))) # plt.tight_layout() plt.show() show_images(collect_samples(train_x, train_y, num_per_class=10), num_rows=10) . . For creating the model we will use a class. Using Sequential() would be more similar to how it is done with Keras, but I think using the class will allow for more flexibility. . You will see a norm_w argument passed to the constructor. It is used for normalizing the weights of the layers, as it improves significantly the effectivity of the quantization. This is further detailed in the complementary post. . class CNN(nn.Module): def __init__(self, dropout_rate=0.0, norm_w=True): super(CNN, self).__init__() self.norm_w = norm_w # Create the layers normalizing or not the weigths if self.norm_w: self.conv_layer1 = nn.utils.weight_norm(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3), name=&#39;weight&#39;) self.conv_layer2 = nn.utils.weight_norm(nn.Conv2d(32, 64, 3), name=&#39;weight&#39;) self.fc = nn.utils.weight_norm(nn.Linear(in_features= 64*5*5, out_features=10), name=&#39;weight&#39;) else: self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) self.conv_layer2 = nn.Conv2d(32, 64, 3) self.fc = nn.Linear(in_features= 64*5*5, out_features=10) # input dimensions are Bx1x28x28 (BxCxHxW) self.batch_norm1 = nn.BatchNorm2d(32) self.relu = nn.ReLU() self.pool = nn.MaxPool2d(kernel_size=2,stride=2) self.batch_norm2 = nn.BatchNorm2d(64) self.flat = nn.Flatten() self.drop = nn.Dropout(p=dropout_rate) def forward(self, x): # Block 1 out = self.conv_layer1(x) out = self.batch_norm1(out) out = self.relu(out) out = self.pool(out) # Block 2 out = self.conv_layer2(out) out = self.batch_norm2(out) out = self.relu(out) out = self.pool(out) # Flatten the output using BxC*H*W out = self.flat(out) out = self.drop(out) out = self.fc(out) return out def add_quant(self): &#39;&#39;&#39; Returns a new model with added quantization layers &#39;&#39;&#39; return nn.Sequential(torch.quantization.QuantStub(), self, torch.quantization.DeQuantStub()) . toy_model = CNN() summary(toy_model, input_size=(1,28,28),device=&quot;cpu&quot;) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - . Quick Notes: . The BatchNorm2d in PyTorch appears as it has half of the parameters than the Keras one. This is because Keras count the accumulated means and accumulated std as non-trainable parameters, while Pytorch simply hide them. So PyTorch only counts as Batch Normalization parmeters the scale and the offset. This also explains the difference in the 192 non-trainable parameters. | Forward/backward pass size (MB) refears to the expected memory to run the model. | . Training without PyTorch Model Quantization . As PyTorch does not have a training loop using .fit() like Keras does, we will have to implement our own. It is not a plain PyTorch traiining loop as we will print some metrics and save them for the later plots. . Note:This could be done in fewer lines of code using Fastai or PyTorch Lightning, and for tracking is better to use platforms such as Weights &amp; Biases. But as is not the point of this post we will continue as it is. . def train_model(model, epochs, bz=128, device=&#39;cuda&#39;): # Save the model training information model_history = {&quot;loss&quot;:[], &quot;sparse_categorical_accuracy&quot;: [], &quot;val_loss&quot;:[], &quot;val_sparse_categorical_accuracy&quot;:[]} # Show the model summary model.to(device) summary(model, input_size=(1,28,28), device=device) # Define the training hyperparameters optimizer = optim.Adam(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() # Create the training and validation dataloders train = data_utils.TensorDataset(train_x, train_y) train_loader = data_utils.DataLoader(train, batch_size=bz, shuffle=True) val = data_utils.TensorDataset(test_x, test_y) val_loader = data_utils.DataLoader(val, batch_size=bz, shuffle=False) for epoch in range(epochs): # loop over the number of epochs start = time.time() print(f&#39;Epoch {epoch+1}&#39;) train_loss = 0.0 train_correct = 0.0 val_loss = 0.0 val_correct = 0.0 # Loop over the trainig set for data in tqdm(train_loader): # Move the data to the corresponding device x, y = data x, y = x.to(device), y.to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(x) loss = criterion(outputs, y) loss.backward() optimizer.step() train_loss += outputs.shape[0] * loss.item() train_correct += (outputs.argmax(1) == y).detach().cpu().sum() # Log the training loss and metrics train_loss = train_loss/float(len(train_x)) train_correct = train_correct/float(len(train_x)) model_history[&quot;loss&quot;].append(train_loss) model_history[&quot;sparse_categorical_accuracy&quot;].append(train_correct) #Loop over the validation set: model.eval() with torch.inference_mode(): for data in val_loader: x, y = data x, y = x.to(device), y.to(device) outputs = model(x) loss = criterion(outputs, y) val_loss += outputs.shape[0] * loss.item() val_correct += (outputs.argmax(1) == y).detach().cpu().sum() # Log the training loss and metrics val_loss = val_loss/float(len(test_x)) val_correct = val_correct/float(len(test_x)) model_history[&quot;val_loss&quot;].append(val_loss) model_history[&quot;val_sparse_categorical_accuracy&quot;].append(val_correct) model.train() end = time.time() # Print information for the epoch print(f&quot;Time: {(end-start):.2f} s - &quot;, f&quot;Loss: {train_loss:.4f} - Categorical_Accuracy: {train_correct:.4f} - &quot;, f&quot;Val_Loss: {val_loss:.4f} - Categorical_Val_Accuracy: {val_correct:.4f}&quot;) print(&#39;Finished Training&#39;) return model, model_history . no_quant = CNN() basic_mnist_model, basic_mnist_model_history = train_model(no_quant, epochs=15, device=device) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 4.67 s - Loss: 0.1699 - Categorical_Accuracy: 0.9557 - Val_Loss: 0.0609 - Categorical_Val_Accuracy: 0.9810 Epoch 2 Time: 4.54 s - Loss: 0.0498 - Categorical_Accuracy: 0.9845 - Val_Loss: 0.0487 - Categorical_Val_Accuracy: 0.9865 Epoch 3 Time: 4.56 s - Loss: 0.0388 - Categorical_Accuracy: 0.9876 - Val_Loss: 0.0431 - Categorical_Val_Accuracy: 0.9877 Epoch 4 Time: 4.55 s - Loss: 0.0300 - Categorical_Accuracy: 0.9909 - Val_Loss: 0.0472 - Categorical_Val_Accuracy: 0.9855 Epoch 5 Time: 4.57 s - Loss: 0.0245 - Categorical_Accuracy: 0.9925 - Val_Loss: 0.0317 - Categorical_Val_Accuracy: 0.9903 Epoch 6 Time: 4.55 s - Loss: 0.0226 - Categorical_Accuracy: 0.9930 - Val_Loss: 0.0365 - Categorical_Val_Accuracy: 0.9879 Epoch 7 Time: 4.52 s - Loss: 0.0186 - Categorical_Accuracy: 0.9943 - Val_Loss: 0.0333 - Categorical_Val_Accuracy: 0.9891 Epoch 8 Time: 4.52 s - Loss: 0.0165 - Categorical_Accuracy: 0.9949 - Val_Loss: 0.0339 - Categorical_Val_Accuracy: 0.9899 Epoch 9 Time: 4.55 s - Loss: 0.0143 - Categorical_Accuracy: 0.9952 - Val_Loss: 0.0352 - Categorical_Val_Accuracy: 0.9893 Epoch 10 Time: 4.55 s - Loss: 0.0122 - Categorical_Accuracy: 0.9964 - Val_Loss: 0.0273 - Categorical_Val_Accuracy: 0.9915 Epoch 11 Time: 4.62 s - Loss: 0.0109 - Categorical_Accuracy: 0.9964 - Val_Loss: 0.0298 - Categorical_Val_Accuracy: 0.9908 Epoch 12 Time: 4.54 s - Loss: 0.0089 - Categorical_Accuracy: 0.9971 - Val_Loss: 0.0278 - Categorical_Val_Accuracy: 0.9919 Epoch 13 Time: 4.69 s - Loss: 0.0082 - Categorical_Accuracy: 0.9973 - Val_Loss: 0.0365 - Categorical_Val_Accuracy: 0.9893 Epoch 14 Time: 5.18 s - Loss: 0.0082 - Categorical_Accuracy: 0.9972 - Val_Loss: 0.0389 - Categorical_Val_Accuracy: 0.9889 Epoch 15 Time: 5.89 s - Loss: 0.0075 - Categorical_Accuracy: 0.9975 - Val_Loss: 0.0309 - Categorical_Val_Accuracy: 0.9915 Finished Training . plt.plot( basic_mnist_model_history[&#39;sparse_categorical_accuracy&#39;], linestyle=&#39;dashed&#39;) plt.plot( basic_mnist_model_history[&#39;val_sparse_categorical_accuracy&#39;], linestyle=&#39;dotted&#39;) plt.title(&#39;Model Accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;Train Accuracy&#39;, &#39;Test Accuracy&#39;], loc=&#39;lower right&#39;) plt.show() . Helper functions for Pytorch Quantizantion and evaluation . These two functions will compute the size of our model. As it is mentioned in the book, it is an important metric to take into account when evaluanting models (not just the score). . Note:We won&#39;t use the first one, it is just informative, you can use it for non-quantized models. It is an aproximation and will be a bit lower than the real size. . def model_size(model): param_size = 0 for param in model.parameters(): param_size += param.nelement() * param.element_size() buffer_size = 0 for buffer in model.buffers(): buffer_size += buffer.nelement() * buffer.element_size() model_size = (param_size + buffer_size) / 1024 def real_model_size(model): torch.save(model.state_dict(), &quot;temp.p&quot;) size = os.path.getsize(&quot;temp.p&quot;)/1024 os.remove(&#39;temp.p&#39;) return size . The following evaluation functions use the same logic as the ones presented in the book. . def pytorch_model_eval(model, test_images, test_labels, quantized): &quot;&quot;&quot;Evaluate the generated model.&quot;&quot;&quot; num_correct = 0 num_total = 0 model.eval() with torch.inference_mode(): for idx in range(len(test_images)): num_total = num_total + 1 if quantized: input = test_images[idx:idx+1].to(&quot;cpu&quot;) else: input = test_images[idx:idx+1].to(device) output = model(input) # The returned output is a tensor of logits, so we find the maximum in that # tensor, and see if it matches the label. if output.argmax(1) == test_labels[idx]: num_correct = num_correct + 1 print(&#39;Accuracy:&#39;, num_correct * 1.0 / num_total) . The quantize_and_eval function includes the code needed for quantizing the model, which is inspired in the official PyTorch post which I recommend reading. . To do so we do the following steps: . If present, remove weight normalization from the layers as is not supported in quantization. . | Change the model to the CPU and put it into evaluation mode. . | Set the backend to fbgemm to run the model on a computer with x86 architecture. If we want the model to run on mobile devices which normally have arm architecture, you need to use qnnpack for the backend. . | Fuse: this consist in converting a list of modules into a single module. The output will be the same, but PyTorch will optimize some operations for us. This will save memory and make the model run faster. But it is optional and not all layers are supported. In our case we will fuse our two convolutional blocks, conv+batch_norm+(relu). You can read more about fusion and which layers can be fused here. . | Add the stubs layers at the beginning and the end of the model. Those will be converted after calibration to quantize the inputs and outputs. . | Set the configuration, we will use the default. . | Calibrate: We take a sample of the training data that represents the data on which we will evaluate, to teach the model how to quantize the inputs and activations. Although, here we are using directly the test data so it could be considered a bit like cheating. But as is not the point of the notebook we will leave it as it is and keep that in mind for real-world applications. . | Convert: where the actual quantization is made. . | . ! mkdir -p &#39;pytorch_models&#39; def quantize_and_eval(model, model_name, quantized_export, test_dataset_x, test_dataset_y): &quot;&quot;&quot;Helper method to eval a model (quantizing it or not).&quot;&quot;&quot; # Remove the normalization from the weighted layers as is not suported in quantization if model.norm_w: for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): nn.utils.remove_weight_norm(m) if quantized_export: # Some operations not suported on GPU model.to(&quot;cpu&quot;) model.eval() # Backend for using a serever backend = &quot;fbgemm&quot; &quot;&quot;&quot;Fuse - Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules &quot;&quot;&quot; torch.quantization.fuse_modules(model, [[&#39;conv_layer1&#39;,&#39;batch_norm1&#39;,&#39;relu&#39;], [&#39;conv_layer2&#39;, &#39;batch_norm2&#39;]], inplace=True) &quot;&quot;&quot;Insert stubs&quot;&quot;&quot; model = model.add_quant() &quot;&quot;&quot;Prepare&quot;&quot;&quot; model.qconfig = torch.quantization.get_default_qconfig(backend) torch.quantization.prepare(model, inplace=True) &quot;&quot;&quot;Calibrate&quot;&quot;&quot; with torch.inference_mode(): for idx in range(min(len(test_dataset_x), 1000)): x = test_dataset_x[idx:idx+1].to(&quot;cpu&quot;) model(x) &quot;&quot;&quot;Convert&quot;&quot;&quot; torch.quantization.convert(model, inplace=True) model_name = &#39;{}_{}.pt&#39;.format( model_name, (&#39;quantized&#39; if quantized_export else &#39;float&#39;)) print(&#39;Model Name: {}, Quantized: {}&#39;.format(model_name, quantized_export)) print(&#39;Model Size: {:.2f} KB&#39;.format(real_model_size(model))) torch.save(model.state_dict(), os.path.join(&#39;pytorch_models&#39;, model_name)) # Evaluate the model. start = time.time() pytorch_model_eval(model, test_dataset_x, test_dataset_y, quantized_export) end = time.time() print(&#39;Eval time: {:.2f}s&#39;.format(end-start)) . Chain everything together in a single train function . Now we only need to put all the pieces together!üéâüéâ . import os import numpy as np from copy import deepcopy def train_model_and_quantize(batch_size=128, epochs=100, model_name=&#39;mnist_model&#39;, norm_w=True): model = CNN(norm_w=norm_w).to(device) model, model_history = train_model(model, epochs, bz=batch_size) print(&quot;Running Final Evaluation&quot;) # Convert and evaluate both (floating point and quantized models). q_model = deepcopy(model) quantize_and_eval(model, model_name, False, test_x, test_y) quantize_and_eval(q_model, model_name, True, test_x, test_y) return model, model_history . mnist_model, mnist_model_history = train_model_and_quantize(epochs=15) . - Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 26, 26] 320 BatchNorm2d-2 [-1, 32, 26, 26] 64 ReLU-3 [-1, 32, 26, 26] 0 MaxPool2d-4 [-1, 32, 13, 13] 0 Conv2d-5 [-1, 64, 11, 11] 18,496 BatchNorm2d-6 [-1, 64, 11, 11] 128 ReLU-7 [-1, 64, 11, 11] 0 MaxPool2d-8 [-1, 64, 5, 5] 0 Flatten-9 [-1, 1600] 0 Dropout-10 [-1, 1600] 0 Linear-11 [-1, 10] 16,010 ================================================================ Total params: 35,018 Trainable params: 35,018 Non-trainable params: 0 - Input size (MB): 0.00 Forward/backward pass size (MB): 0.75 Params size (MB): 0.13 Estimated Total Size (MB): 0.89 - Epoch 1 Time: 4.73 s - Loss: 0.1884 - Categorical_Accuracy: 0.9522 - Val_Loss: 0.0486 - Categorical_Val_Accuracy: 0.9840 Epoch 2 Time: 4.57 s - Loss: 0.0508 - Categorical_Accuracy: 0.9849 - Val_Loss: 0.0457 - Categorical_Val_Accuracy: 0.9854 Epoch 3 Time: 4.63 s - Loss: 0.0391 - Categorical_Accuracy: 0.9880 - Val_Loss: 0.0392 - Categorical_Val_Accuracy: 0.9858 Epoch 4 Time: 4.63 s - Loss: 0.0321 - Categorical_Accuracy: 0.9898 - Val_Loss: 0.0359 - Categorical_Val_Accuracy: 0.9885 Epoch 5 Time: 4.61 s - Loss: 0.0276 - Categorical_Accuracy: 0.9915 - Val_Loss: 0.0286 - Categorical_Val_Accuracy: 0.9912 Epoch 6 Time: 4.65 s - Loss: 0.0224 - Categorical_Accuracy: 0.9936 - Val_Loss: 0.0275 - Categorical_Val_Accuracy: 0.9912 Epoch 7 Time: 4.64 s - Loss: 0.0200 - Categorical_Accuracy: 0.9938 - Val_Loss: 0.0337 - Categorical_Val_Accuracy: 0.9899 Epoch 8 Time: 4.69 s - Loss: 0.0171 - Categorical_Accuracy: 0.9945 - Val_Loss: 0.0331 - Categorical_Val_Accuracy: 0.9883 Epoch 9 Time: 4.69 s - Loss: 0.0153 - Categorical_Accuracy: 0.9951 - Val_Loss: 0.0272 - Categorical_Val_Accuracy: 0.9911 Epoch 10 Time: 4.60 s - Loss: 0.0149 - Categorical_Accuracy: 0.9951 - Val_Loss: 0.0310 - Categorical_Val_Accuracy: 0.9901 Epoch 11 Time: 4.61 s - Loss: 0.0128 - Categorical_Accuracy: 0.9958 - Val_Loss: 0.0267 - Categorical_Val_Accuracy: 0.9916 Epoch 12 Time: 4.61 s - Loss: 0.0094 - Categorical_Accuracy: 0.9971 - Val_Loss: 0.0315 - Categorical_Val_Accuracy: 0.9904 Epoch 13 Time: 4.60 s - Loss: 0.0086 - Categorical_Accuracy: 0.9975 - Val_Loss: 0.0277 - Categorical_Val_Accuracy: 0.9910 Epoch 14 Time: 4.62 s - Loss: 0.0081 - Categorical_Accuracy: 0.9974 - Val_Loss: 0.0358 - Categorical_Val_Accuracy: 0.9899 Epoch 15 Time: 4.63 s - Loss: 0.0081 - Categorical_Accuracy: 0.9973 - Val_Loss: 0.0318 - Categorical_Val_Accuracy: 0.9906 Finished Training Running Final Evaluation Model Name: mnist_model_float.pt, Quantized: False Model Size: 142.11 KB Accuracy: 0.9906 Eval time: 7.14s . /usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. reduce_range will be deprecated in a future release of PyTorch.&#34; . Model Name: mnist_model_quantized.pt, Quantized: True Model Size: 41.67 KB Accuracy: 0.9845 Eval time: 8.91s . plt.plot( mnist_model_history[&#39;sparse_categorical_accuracy&#39;], linestyle=&#39;dashed&#39;) plt.plot( mnist_model_history[&#39;val_sparse_categorical_accuracy&#39;], linestyle=&#39;dotted&#39;) plt.title(&#39;Model Accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;Train Accuracy&#39;, &#39;Test Accuracy&#39;], loc=&#39;lower right&#39;) plt.show() .",
            "url": "https://xanaga.github.io/posts_en/quantization/efficient_deeplearning/2022/05/29/Quantizing_A_Deep_Learning_Model_PyTorch.html",
            "relUrl": "/quantization/efficient_deeplearning/2022/05/29/Quantizing_A_Deep_Learning_Model_PyTorch.html",
            "date": " ‚Ä¢ May 29, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Knowledge Distillation for Traffic Sign Recognition",
            "content": ". Introduction . It seems that the current trend in Deep Learning is to have bigger and more models. This makes it difficult for users to use them, fit them into small devices, get fast results etc. This is why I see model compression, knowledge distillation, and these kinds of techniques as one of the more interesting and useful topics in deep learning. After all, if you want to apply deep learning in VR/AR you need to fit them into small devices. Also for simulations and computer graphics is better to have optimized and fast models. In addition, this makes AI more affordable to everyone, democratizing access to this technology. With this objective in mind I tried to get similar results as in my previous post, but using a resnet18 instead of a resnet34. Before continuing reading this post I encourage you to take a look at the previous one. Nevertheless, I was not successful at all. Looking for some solutions I discovered FasterAI by Nathan Hubens, which is an awesome library that implements those compression techniques based on Fastai. So my objectives with this notebook are: . Try to implement Knowledge Distillation technique using FasterAI to start getting familiar with the library. I would like to apply this model compression to other projects too (but that will not be covered in this notebook) | Encourage anyone who is thinking about implementing these types of approaches that may seem complicated to try this library. | . Imports . First we will upgrade the fastai version used in Colab, by deafult it is the first version. . ! pip install -Uqq fastai # upgrade fastai on colab . import fastai fastai.__version__ . &#39;2.5.6&#39; . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.data.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . This will be analogous to the Download section in the previous post. Despite that, I have added the possibility to only train on some classes of the dataset. Because of using Knowledge Distillation we should be able to classify signals that never have been seen in the training set (or at least give an informed guess). For this notebook, we will use all the classes, but you are encouraged to try to eliminate some and see how it behaves! . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . c=list(range(43)) # List the classes you want to train on . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. It is analogous to the process made in the resnet34 notebook, with some minor changes to handle the cases where not all the classes are selected. A more detailed explanation can be found in the previous post. . We have changed the read_annotations function to allow us to filter and read only the annotations of the classes we are interested in. This is done by adding the if inside the for loop. So the classes will have to be passed as an argument in all the following functions. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename, classes=None): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column if classes is None or label in classes: # We only read the annotations of the classes we are interested annotations.append(Annotation(filename, label)) return annotations def load_training_annotations(source_path, classes=None): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename, classes)) return annotations def copy_files(label, filenames, source, destination, classes=None, move=False): func = os.rename if move else shutil.copyfile label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) for filename in filenames: if classes is None or int(os.path.basename(label_path)) in classes: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) def split_train_validation_sets(source_path, train_path, validation_path, all_path, classes, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) annotations = load_training_annotations(source_path) filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) for label, filenames in filenames.items(): filenames = sorted(filenames) validation_size =int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, classes, move=False) copy_files(label, train_filenames, source_path, train_path, classes, move=True) copy_files(label, validation_filenames, source_path, validation_path, classes, move=True) . Due to the changes we have made in the previous functions we only have to add the information about the classes in the last step, passing it to the split_train_validation_sets function. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, c, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Prepare the data . Data preparation is even more similar to the previous post. We will only add a vocab when creating the dataset. This is because if we don&#39;t specify it Fastai will take as vocab the classes that appear in our data, which would be problematic if we want to restrict the problem to only a subset of classes. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], vocab=[str(i) for i in range(43)]) . We can see that our vocab will have all the classes, no matter which classes we use to train. . data.vocab . [&#39;0&#39;, &#39;1&#39;, &#39;10&#39;, &#39;11&#39;, &#39;12&#39;, &#39;13&#39;, &#39;14&#39;, &#39;15&#39;, &#39;16&#39;, &#39;17&#39;, &#39;18&#39;, &#39;19&#39;, &#39;2&#39;, &#39;20&#39;, &#39;21&#39;, &#39;22&#39;, &#39;23&#39;, &#39;24&#39;, &#39;25&#39;, &#39;26&#39;, &#39;27&#39;, &#39;28&#39;, &#39;29&#39;, &#39;3&#39;, &#39;30&#39;, &#39;31&#39;, &#39;32&#39;, &#39;33&#39;, &#39;34&#39;, &#39;35&#39;, &#39;36&#39;, &#39;37&#39;, &#39;38&#39;, &#39;39&#39;, &#39;4&#39;, &#39;40&#39;, &#39;41&#39;, &#39;42&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . Knowledge Distillation . This is a technique proposed by Geoffrey E. Hinton as a way to extract the knowledge from big models and use it to train simpler architectures. Moreover, it also has proven to provide what seems to be more robust and general learning which is always desirable. We have considered this approach because of the increasing size of the models not only in computer vision tasks but also in Natural Language Processing (NLP), which makes those difficult to be used in inference for very time demanding tasks or even impossible to fit in embedded systems. . The high-level idea is to, instead of training the models with one-hot encoded vectors, train them using a ‚Äúsofter version‚Äù of those that stores some information about the similarity between images. Is a concept similar to word embeddings in NLP. These labels are obtained from (normally) a bigger model which is already trained and used only for inference. . Here is where we start using FasterAI library. Nevertheless, I found problems loading a model into a learner with Fastai. As KnowledgeDistillation function expects to receive a learner I had to copy and paste this function and modify it by adding the from_learner parameter. In this way, I can set it to false and use as a teacher a plain PyTorch model. Nevertheless, the correct way of using it would be as straightforward as installing the library: . pip install fasterai . And import the things you need: . from fasterai import ... . sz = 96 bs = 256 wd = 5e-3 f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) . To act as the teacher we will use the model we trained in the previous post which can be found in this repo. You can download it from there or train your own model using the previous notebook. Once you have the model trained you will have to set the path_to_model variable. . If you are working in Colab you can drag and drop the model into the Colab&#39;s file explorer or save the model in your drive and mount it (just by clicking the Mount Drive button in the files tab, on the left in Colab). By mounting the drive you will be able to reach the model file by navigating across your drive directories. . path_to_model = #your_path . The teacher model &#128188; &#128207; . First, we prepare the teacher model. To do so, we will get the architecture of our model with random weights and then we will fill it with the trained weights. . Important: To do that, is necessary that the random filled architecture has exactly the same parameters as the ones in the pretrained ones. This is why we use create_cnn_model fuction of Fastai, which given a model architecture it creates a model (which is a Sequential object) with the head and the body that we choose. In the previous notebook we created the Learner with the default settings, so now we will use those defaults too. For each architecture, Fastai has a way of cutting the model, and for all of them it uses the same default head, you can check the source code! . If you would want to create the model with a different head and body you would have to use the cut and custom_head arguments. You can find more information about it in the documentation. . The to_device method will place it into GPU or CPU depending on the hardware available. . teacher = create_cnn_model(resnet34, 43, pretrained=False).to(device) #get the model architecture teacher.load_state_dict(torch.load(path_to_model+&#39;resnet34_weights.pth&#39;)) #load the trained wheights teacher . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=43, bias=False) ) ) . . . Note: The teacher is a Sequential object with two modules. The module (0) is the body, and it is the original architecture of the model (in our case resnet34), which can be pretrained (not in our case). The second module, the (1): Sequential, is the head which is initialized using kaiming_normal initialization by default(it can be changed using the init argument), so it is not pretrained. The layers of this head are the default used in Fastai. The decision about where to cut the original model is taken according to Fastai metadata for each architecture, but as we have said it is customizable using the cut argument. . Our teacher model is almost ready! The only thing that last is that, as we don&#39;t want to propagate gradients through our teacher (we only want it for inference) we should make sure it is frozen. . The following PyTorch code does exactly that. . for param in teacher.parameters(): param.requires_grad = False . . Note: Inside the KnowledgeDistillation callback we use the teacher in eval() mode, so the gradients would not be calculated even if the model is not frozen, but is better to make sure. . The student Learner &#128215; . Now let&#39;s prepare our student network. As before we use the create_cnn_model, but with a pretrained resnet18 architecture, to build the model that we will pass to the Learner constructor. . student_model=create_cnn_model(resnet18, 43).to(device) #get the model architecture and pretrained wheights . Then, we could do the following: . bad_student = Learner(data, student_model, metrics=[accuracy,f1_score_mult]) bad_student.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 11,725,888 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . But this would unfreeze the whole student! We don&#39;t want that, we would like to have the earlier pretrained layers of the model (the body) frozen and the rest (the head) to be trainable. The reason is that, by default, Learner does not freeze any parameter group. . So let&#39;s freeze the body. To do that we use the freeze_to(n) method where n is the number of parameter groups we want to freeze. We take n=1 as we only want the body to be frozen. . bad_student.freeze_to(1) bad_student.summary() . /usr/local/lib/python3.7/dist-packages/fastai/optimizer.py:22: UserWarning: Freezing 1 groups; model has 1; whole model is frozen. warn(f&#34;Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.&#34;) . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 False ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 False ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 12,672 Total non-trainable params: 11,713,216 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have frozen the entire model! ü•∂ . Note: The BatchNorm2d layers are never frozen, this is why we have some trainable parameters. If we look at the warning at the top of the output, we can see the problem. We only have one parameter group which is the entire model. . So, how are those parameters groups chosen? By the argument splitter, which by default is trainable_parameters. This will return all the trainable parameters of the model. In our case, they are all trainable, so we will only get one parameter group, hence the whole student will be frozen. . The solution is to pass splitter=default_split. This will split the parameter groups in body and head, just as we want! . . Important: In the Explore Training section we will use lr_find(), which does not accept any weight decay nor callback arguments (it takes the ones defined when creating the Learner). Then the correct thing to do is to pass as an argument to the learner the KnowledgeDistillation callback, as we do with the weight decay. By doing so we have thesame training scenario when doing lr_find() and fit_one_cycle(). . loss = partial(SoftTarget, T=20) kd = KnowledgeDistillation(teacher, loss, from_learner=False) . student = Learner(data, student_model, metrics=[accuracy,f1_score_mult], splitter=default_split, wd=wd, cbs=kd) student.freeze_to(1) #student.summary() . . Warning: If you use student.summary() it will raise the following error: Exception occured in KnowledgeDistillation when calling event after_loss: cross_entropy_loss(): argument &#8217;target&#8217; (position 2) must be Tensor, not NoneType. . Although, it only affect to the summary method. So we can continue and if we really want to check the learner we can remove and add back the callback, as the collapsed code shows below. . print(&#39;See in which position the KnowledgeDistillation is:&#39;) print(student.cbs) print() student.remove_cb(cb=student.cbs[3]) # We could also do cb=kd print(student.summary()) # Obviously you won&#39;t see the KnowledgeDistillation callback at the end student.add_cb(cb=kd) # Add back the callback print() print(&#39;Check that you have the same list as before the summary:&#39;) print(student.cbs) . See in which position the KnowledgeDistillation is: [TrainEvalCallback, Recorder, ProgressCallback, KnowledgeDistillation] . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 558,976 Total non-trainable params: 11,166,912 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback Check that you have the same list as before the summary: [TrainEvalCallback, Recorder, ProgressCallback, KnowledgeDistillation] . . Training . In the training process is where FasterAI magic comes in üßô . Only by adding the KnowledgeDistillation callback when fitting the model, we will be able to use this technique with almost the same code as in the previous post. But, as we setted those parameters when creating the Learner(), we don&#39;t even need to do that!üéâ . Explore training . We will follow the same procedure as is the previous post for finding the hyperparameters that better train our model. . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . . Note: Remember that the weight decay and the Knowledge Distillation Callback where setted in the Learner() definition. . student.lr_find() # wd and callback included . SuggestedLRs(valley=0.004365158267319202) . student.fit_one_cycle(1, lr_max=0.001) # wd and cbs are setted in Learner() . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 15.465137 | 13.668188 | 0.605898 | 0.563930 | 01:35 | . student.unfreeze() . student.lr_find() . SuggestedLRs(valley=0.00019054606673307717) . student.fit_one_cycle(9, lr_max=slice(0.0001, 0.001)) #, wd=wd, cbs=kd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 9.225066 | 6.410910 | 0.833745 | 0.821246 | 01:41 | . 1 | 4.279030 | 1.822614 | 0.960905 | 0.957461 | 01:36 | . 2 | 2.453992 | 1.222131 | 0.974760 | 0.972952 | 01:37 | . 3 | 1.889181 | 0.925809 | 0.982579 | 0.981159 | 01:37 | . 4 | 1.645638 | 0.775705 | 0.988889 | 0.988514 | 01:37 | . 5 | 1.506031 | 0.679071 | 0.989575 | 0.989230 | 01:37 | . 6 | 1.422567 | 0.668275 | 0.990809 | 0.990585 | 01:36 | . 7 | 1.355428 | 0.612182 | 0.991632 | 0.991374 | 01:36 | . 8 | 1.323943 | 0.623958 | 0.991495 | 0.991259 | 01:37 | . student.lr_find() . SuggestedLRs(valley=1.5848931980144698e-06) . student.fit_one_cycle(6, lr_max=slice(0.00001, 0.0001))#, wd=wd, cbs=kd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 1.322057 | 0.606176 | 0.991770 | 0.991487 | 01:37 | . 1 | 1.320874 | 0.618872 | 0.990535 | 0.990247 | 01:36 | . 2 | 1.300413 | 0.615932 | 0.992730 | 0.992584 | 01:36 | . 3 | 1.300726 | 0.611663 | 0.992455 | 0.992312 | 01:36 | . 4 | 1.290397 | 0.592799 | 0.992455 | 0.992310 | 01:37 | . 5 | 1.286910 | 0.597255 | 0.992455 | 0.992296 | 01:37 | . Retrain on the whole dataset . Once we know which configuration is better we can train the model on all the data available (training+validation). . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], vocab=[str(i) for i in range(43)], train=&#39;all&#39;) . teacher = create_cnn_model(resnet34, 43, pretrained=False).to(device) teacher.load_state_dict(torch.load(path_to_model+&#39;resnet34_weights.pth&#39;)) student_model=create_cnn_model(resnet18, 43).to(device) student = Learner(data, student_model, metrics=[accuracy,f1_score_mult], splitter=default_split, wd=wd, cbs=kd) student.freeze_to(1) . student.fit_one_cycle(1, lr_max=0.001) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 13.978484 | 11.242065 | 0.715364 | 0.683073 | 01:49 | . student.unfreeze() student.fit_one_cycle(10, lr_max=slice(0.0001, 0.001)) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 9.187653 | 5.441027 | 0.893416 | 0.883363 | 01:54 | . 1 | 3.874919 | 1.078969 | 0.987106 | 0.987011 | 01:53 | . 2 | 2.228036 | 0.545888 | 0.997257 | 0.997255 | 01:53 | . 3 | 1.754988 | 0.441869 | 0.998080 | 0.998076 | 01:52 | . 4 | 1.543820 | 0.505094 | 0.998903 | 0.998900 | 01:53 | . 5 | 1.451698 | 0.324294 | 0.999726 | 0.999725 | 01:52 | . 6 | 1.336454 | 0.319012 | 0.999863 | 0.999863 | 01:51 | . 7 | 1.282818 | 0.278213 | 1.000000 | 1.000000 | 01:51 | . 8 | 1.239342 | 0.259717 | 1.000000 | 1.000000 | 01:50 | . 9 | 1.236298 | 0.250886 | 1.000000 | 1.000000 | 01:51 | . student.fit_one_cycle(6, lr_max=0.0001) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 1.270205 | 0.312592 | 0.999726 | 0.999725 | 01:50 | . 1 | 1.336736 | 0.343301 | 0.999863 | 0.999863 | 01:51 | . 2 | 1.285208 | 0.289410 | 0.999863 | 0.999863 | 01:50 | . 3 | 1.217801 | 0.267841 | 0.999863 | 0.999863 | 01:51 | . 4 | 1.179426 | 0.231982 | 1.000000 | 1.000000 | 01:51 | . 5 | 1.172143 | 0.221064 | 1.000000 | 1.000000 | 01:50 | . Test . Like we did in the previous post we will use Test Time Augmentations, and we will use the same function to plot how the performance and inference time behave. . Note: We have added to the test_time_aug function the mask argument. This will allow us to compute the F1-score only for some classes. For example, only for the ones we have trained on. . def test_time_aug(learner, test_dataloader, y_true, metric, n_augs=[10], beta=0.1,mask=None): res = [] if mask is None: mask=list(range(len(y_true))) learner.eval() for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learner.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds[mask], y_true[mask]) res.append([aug, score,infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . true_test_labels = {a.filename: a.label for a in test_annotations} class_indexes = data.vocab.o2i test_img=get_image_files(&#39;./data/test&#39;) filenames = [filepath.name for filepath in test_img] labels = [str(true_test_labels[filename]) for filename in filenames] y_true = np.array([class_indexes[label] for label in labels]) . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . The following code will create the mask used for computing the F1-score. You only have to specify in which classes you want to focus on the interest_classes variable. We will focus on all the classes that we trained with (which are all the classes). . interest_classes=c interest_idx=[data.vocab.o2i[str(cl)] for cl in interest_classes] mask=np.isin(y_true, interest_idx, invert=False) . . Note: If you set invert to True you will get the F1-score only on the images you have not seen in the training. If we try to use our learner for as it is we will get an error, produced by the KnowledgeDistallation callback. But we don&#39;t need it anymore! (sorry Nathan, you&#39;ve been a hero üòî). Let&#39;s remove it. . student.cbs . (#4) [TrainEvalCallback,Recorder,ProgressCallback,KnowledgeDistillation] . student.cbs[3] . KnowledgeDistillation . student.remove_cb(cb=student.cbs[3]) student.cbs . (#3) [TrainEvalCallback,Recorder,ProgressCallback] . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(student, test_dataloader, y_true, metric, n_augs=[0, 5, 10, 20, 30]) . N Augmentations: 0 F1-score: 0.9922140340549594 Time:24.077036380767822 . . N Augmentations: 5 F1-score: 0.9933502950176439 Time:117.72993874549866 . . N Augmentations: 10 F1-score: 0.9943761367605225 Time:212.58393836021423 . . N Augmentations: 20 F1-score: 0.9950922306435893 Time:405.470721244812 . . N Augmentations: 30 F1-score: 0.9947694889306348 Time:592.3682796955109 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . Conclusion . As we have seen Knowledge Distillation allows us to train for more epochs without overfitting and improves the final performance of the model compared with not using it (and getting very close of the resnet34, which resulted in 0.9963 F1-score). An intuitive explanation can be that we are giving more information to the network in every step of the backpropagation by, not only providing one-hot labels, but also a probability of the image to be another class. With this we are introducing the idea of similarities between classes. . I see these types of compression approaches as very interesting and I would like to apply them in other domains, maybe to NLP or 3D data. So I cannot recommend more FasterAI library and infinitely thank the work done to Nathan Hubens. Also, if you want to learn more about how to make smaller and faster Neural Networks I encourage you to visit his blog. . Thanks for reading!üòÄ .",
            "url": "https://xanaga.github.io/posts_en/knowledgedistillation/fastai/computer%20vision/2022/04/24/Knowledge_Distillation.html",
            "relUrl": "/knowledgedistillation/fastai/computer%20vision/2022/04/24/Knowledge_Distillation.html",
            "date": " ‚Ä¢ Apr 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "ResNet for Traffic Sign Classification With PyTorch",
            "content": ". Introduction . In the Intelligent Systems course from my degree, we were asked to write a report about a Neural Network related topic. In class, we were introduced to the German Traffic Sign Recognition Benchmark dataset, so I decided to explore previous work of the community. This is how I discovered Pavel Surmenok which has a post dedicated to this subject, using Fastai! As Pavel&#39;s implementation was using the first version of Fastai and I wanted to get more familiar with this library I thought it would be a good idea to try to implement a version of his code using the newest version of Fastai (version 2.5.3 at the time of doing the notebook). So the objectives are the following: . Get hands-on experience with Fastai to apply the theory I know of deep learning. I think it is a great tool for rapid testing with impressive results. Start building things! | Best case scenario, being able to contribute to updating the work that Pavel presented in his blog and help anyone who wants to start with Fastai v2. | Try to explain things I didn&#39;t know before and I would like to find somewhere. The aim of this is to consolidate the knowledge and also help anyone that had the same questions. | . Imports . import fastai fastai.__version__ # check that we have Fastai v2 . &#39;2.5.6&#39; . Show the collapsed code to see all the imports we will need . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch import time import pandas as pd . . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. . Tip: It is not directly related to Fastai or Deep Learning, but knowing how to manage your data is VERY important, and Pavel makes it in an understandable and elegant way. If you don&#8217;t have a lot of confidence working with data this is a section worth looking at. . Annotations will be named tuples containing the name of the file and its corresponding label. You can find a complete tutorial on namedtuple() here, but basically, those are tuple-like immutable data structures with named fields. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column annotations.append(Annotation(filename, label)) return annotations . load_training_annotations will loop over all the files from all the classes and returns the annotations of all the training examples. . def load_training_annotations(source_path): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename)) return annotations . . Note: my_list.extend(iter) will add each element of an iterable,iter, to the list my_list. If we would have used append we would get a list of list of named tuples instead of a list of named tuples. . We have talked about the filenames, labels etc. but our actual data are images. We will use the copy_files function to organize the images in training, validation and all (training+validation) folders. . def copy_files(label, filenames, source, destination, move=False): # copy to the training or validation folders, for the all folder just rename func = os.rename if move else shutil.copyfile # make a directory for every label label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) # fill the directories with its corresponding files for filename in filenames: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) . Finally, we use all the above mentioned to build our own split_train_validation_sets function. . def split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) # annotations will be a list of Annotations(filename, label) annotations = load_training_annotations(source_path) # filenames will be a dictionary # keys: label # values: list of file names filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) # for every label calculate the validation_size and populate the directories for label, filenames in filenames.items(): filenames = sorted(filenames) # get the validation_size, it must be an integer! validation_size = int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, move=False) copy_files(label, train_filenames, source_path, train_path, move=True) copy_files(label, validation_filenames, source_path, validation_path, move=True) . Once we have all these functions all we have to do is call them with the appropriate paths. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Exploratory analisys . We have our data prepared so, first things first, let&#39;s take a look at it. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] classes . ClassId SignName . 0 0 | Speed limit (20km/h) | . 1 1 | Speed limit (30km/h) | . 2 2 | Speed limit (50km/h) | . 3 3 | Speed limit (60km/h) | . 4 4 | Speed limit (70km/h) | . 5 5 | Speed limit (80km/h) | . 6 6 | End of speed limit (80km/h) | . 7 7 | Speed limit (100km/h) | . 8 8 | Speed limit (120km/h) | . 9 9 | No passing | . 10 10 | No passing for vechiles over 3.5 metric tons | . 11 11 | Right-of-way at the next intersection | . 12 12 | Priority road | . 13 13 | Yield | . 14 14 | Stop | . 15 15 | No vechiles | . 16 16 | Vechiles over 3.5 metric tons prohibited | . 17 17 | No entry | . 18 18 | General caution | . 19 19 | Dangerous curve to the left | . 20 20 | Dangerous curve to the right | . 21 21 | Double curve | . 22 22 | Bumpy road | . 23 23 | Slippery road | . 24 24 | Road narrows on the right | . 25 25 | Road work | . 26 26 | Traffic signals | . 27 27 | Pedestrians | . 28 28 | Children crossing | . 29 29 | Bicycles crossing | . 30 30 | Beware of ice/snow | . 31 31 | Wild animals crossing | . 32 32 | End of all speed and passing limits | . 33 33 | Turn right ahead | . 34 34 | Turn left ahead | . 35 35 | Ahead only | . 36 36 | Go straight or right | . 37 37 | Go straight or left | . 38 38 | Keep right | . 39 39 | Keep left | . 40 40 | Roundabout mandatory | . 41 41 | End of no passing | . 42 42 | End of no passing by vechiles over 3.5 metric tons | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We can try to do histogram equalization to see if it improves our results, but in the end, it does not. If you want to try you can use the HistogramEqualization_item class as an item_tfms for the Fastai pipeline. . . Note: You can also try to implement histogram equalization in the way Pavel does in his notebook, but I could not see the changes visually, maybe something has changed in how Fastai opens images. . # Histogram equalization class HistogramEqualization_item(Transform): def init(self, prefix=None): self.prefix = prefix or &#39;&#39; def encodes(self, o): if type(o) == PILImage: ret = ImageOps.equalize(o) else: ret = o return ret def decodes(self, o): return o . . We could start inspecting the data as it is, but Fastai provides us with some useful functions to visualize our data. To use them we will have to integrate this data into Fastai datastructures. We could create a DataBunch or Datasets or go directly to Dataloaders. We will do the last one, but you can find a lot of information in tutotials and documentation. . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)]) . The item_tfms will be applied to every item in our dataset independently (here is where you have to add the HistogramEqualization_item transform if you want to try). Those are performed in the CPU every time an item is accessed. In our case, we only use Resize. Contrary, the batch_tfms will be applied to all the images in the batch at the same time. Those are done in the GPU and are very fast. You can find more information about aug_transforms in the documentation, but basically is a wrap of very useful transforms for image data. As you may note, we randomly resize again the images. The reason is that this is the resizing we want to apply to our images, the one we did previously in √¨tem_tmfs makes all the images the same size and square, to be able to perform batch_tfms in the GPU. These augmentations are key to having a good performance, and this applies to every deep learning project you will work on. . . Note: We normalize using imagenet_stats because our models will be pretrained on this dataset, which is way larger than the one we are working with. . Now we can use show_batch method to see some examples from our dataset. . data.show_batch(nrows=3, ncols=3) . We can also see how the augmentations affect our data by seeing how those affect to a single example. . data.show_batch(nrows=3, ncols=3, unique=True) . We can also check the length of our training and validation set. . len(data.train_ds), len(data.valid_ds) . (31919, 7290) . . Important: For the sake of the blog post lenght, we finish here our &quot;Exploratory analysis&quot;. Nevertheless, explore the data not only includes watching at the data (which is very important), but also the sizes, class distribution etc. A more complete analysis can be found in Pavel&#8217;s post, and you are encouraged to go further with the help of Fastai. . Training . Explore training . In this section, we will try to find the best training hyperparameters to train the model. To do so we will train on the train data and evaluate on the validation data. Then we will use these hyperparameters to train the model on the whole data available (training + validation) and we will test it with the test data. . The metrics we will track will be accuracy and F1-score, because we have seen that the dataset shows class imbalance. The F1-score metric is more robust and informative about the model performance when we are working with this kind of dataset. Concerning the model, we will use a resnet34 pretrained on imagenet. The data, model, and metrics to track will be wrapped up together in a learner object. . Note: We are passing the weight decay as an argument to the Learner(). By doing so, we will use it when calling both lr_find() and fit_one_cycle() methods in the Explore training section. . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) wd = 5e-4 learn = cnn_learner(data, resnet34, metrics=[accuracy,f1_score_mult], wd=wd) . learn.summary() # Display information about the learner obj . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 566,400 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . . Important: Our model is frozen up to parameter group #2 (the whole model except the last layer), this means that when running backpropagation the gradients won&#8217;t be calculated for those layers, hence their weights will remain unchanged. . One of the most important hyperparameters in deep learning is the learning rate. To find a good one we can use lr_find method, which takes a batch and runs it through the network with incrementally bigger learning rates recording the loss for each run. The idea is to take the learning rate where the graph of the loss is the steepest. You can try to find it visually or let Fastai give you a suggestion (at least in this case it tend to suggest smaller learning rates). . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . learn.lr_find() . SuggestedLRs(valley=0.0020892962347716093) . Train the model with the learning rate selected for one epoch. In this epoch, we will only update the weights of the last layer, as the rest of the model is frozen (with the pretrained weights from imagenet). This will allow our last layer to &quot;catch up&quot; with the other ones and not to start randomly. The fit_one_cycle method uses the 1 cycle policy, which is a learning rate schedule policy consisting in starting from a low learning rate in the firsts epochs, increasing it up to a maximum, to decrease it again for the last epochs. A better explanation can be found in this post from Sylvain Gugger. . learn.fit_one_cycle(1, lr_max=0.005) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.875589 | 0.453181 | 0.853498 | 0.848488 | 01:30 | . Then we unfreeze the model to train it completely. With this, gradients will propagate through the whole network updating all the weights. . learn.unfreeze() learn.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 21,834,048 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have a different training scenario, so probably our learning rate has changed. So we repeat the process of finding the learning rate and training for some epochs. We can use slice when specifying the learning rate to allows us to use &quot;discriminative layer training&quot;, which consists of using different learning rates for different layers. Usually, we use a smaller learning rate for the earlier layers, as they are well trained to detect general features on a lot of data from imagenet, and a bigger one for the last layers, as they have to change more to capture the more specific features from our dataset. . learn.lr_find() . SuggestedLRs(valley=9.120108734350652e-05) . learn.fit_one_cycle(3, lr_max=0.0001) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.296145 | 0.232131 | 0.929218 | 0.925996 | 01:38 | . 1 | 0.155398 | 0.110068 | 0.970370 | 0.967639 | 01:38 | . 2 | 0.105242 | 0.086704 | 0.975034 | 0.972592 | 01:38 | . learn.lr_find() . SuggestedLRs(valley=1.2022644114040304e-05) . learn.fit_one_cycle(7, lr_max=slice(0.0001, 0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.117177 | 0.343552 | 0.925652 | 0.928776 | 01:38 | . 1 | 0.154971 | 0.221667 | 0.945816 | 0.942552 | 01:39 | . 2 | 0.117753 | 0.065744 | 0.982167 | 0.980668 | 01:38 | . 3 | 0.096865 | 0.046334 | 0.988615 | 0.988594 | 01:38 | . 4 | 0.065272 | 0.045523 | 0.986831 | 0.986001 | 01:38 | . 5 | 0.058440 | 0.033738 | 0.990535 | 0.990093 | 01:38 | . 6 | 0.055586 | 0.030030 | 0.990261 | 0.989890 | 01:38 | . Retrain on the whole dataset . Once we have tried different configurations and selected the one we think is best, it is time to train the model with all the data! . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) sz = 96 bs = 256 wd = 5e-4 . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], train=&#39;all&#39;) learn = cnn_learner(data, resnet34,metrics=[accuracy,f1_score_mult], wd=wd) . learn.fit_one_cycle(1, lr_max=0.01) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.542564 | 0.107461 | 0.964746 | 0.964709 | 01:46 | . learn.unfreeze() learn.fit_one_cycle(4, lr_max=slice(0.001,0.01)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.385180 | 0.500579 | 0.864060 | 0.859983 | 01:58 | . 1 | 0.210768 | 0.012989 | 0.995610 | 0.995585 | 01:58 | . 2 | 0.111806 | 0.003372 | 0.998903 | 0.998901 | 01:58 | . 3 | 0.063384 | 0.000662 | 0.999863 | 0.999863 | 01:58 | . learn.fit_one_cycle(6, lr_max=slice(0.0001,0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.059273 | 0.000613 | 0.999863 | 0.999863 | 01:58 | . 1 | 0.059253 | 0.001651 | 0.999451 | 0.999452 | 01:58 | . 2 | 0.058100 | 0.000631 | 0.999726 | 0.999726 | 01:58 | . 3 | 0.048571 | 0.000100 | 1.000000 | 1.000000 | 01:58 | . 4 | 0.042215 | 0.000125 | 1.000000 | 1.000000 | 01:58 | . 5 | 0.038188 | 0.000095 | 1.000000 | 1.000000 | 01:57 | . Test . For the test, we will use &quot;Test time augmentations&quot;. This technique is also used by Pavel and shows an increase in the performance of the model. It consists in, instead of just making the prediction of the given test image, making the prediction of the original and other augmented versions of this image using the augmentations used while training. The final prediction will be an average of all of them. This will take more time for each prediction but it will make them more robust. test_time_aug function will test the model with a different number of augmentations and it will return a Pandas Dataframe with the F1-scores and the time it has taken to run the inference of the test set. . def test_time_aug(learner, test_dataloader, metric, n_augs=[10], beta=0.1): res = [] for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learn.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds, y_true) res.append([aug, score, infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . As we did before, we need the test data to be in a DataLoader format. . true_test_labels = {a.filename: a.label for a in test_annotations} #get the annotations in a dictionary format class_indexes = data.vocab.o2i #dictionary from class to index test_img=get_image_files(&#39;./data/test&#39;) #list of the test image file paths filenames = [filepath.name for filepath in test_img] #get the names from the file paths labels = [str(true_test_labels[filename]) for filename in filenames] #list of the labels for each file y_true = np.array([class_indexes[label] for label in labels]) #array of the index for each label . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . len(test_dataloader.dataset) . 12630 . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(learn, test_dataloader, metric, n_augs=[0, 3, 5, 10, 15, 20, 30]) . N Augmentations: 0 F1-score: 0.994469328801153 Time:19.947819232940674 . . N Augmentations: 3 F1-score: 0.9943121917876697 Time:81.12456560134888 . . N Augmentations: 5 F1-score: 0.9950268203228465 Time:119.88853621482849 . . N Augmentations: 10 F1-score: 0.9959702387438432 Time:218.91360545158386 . . N Augmentations: 15 F1-score: 0.996130519244009 Time:317.34042143821716 . . N Augmentations: 20 F1-score: 0.996285695102012 Time:414.57182717323303 . . N Augmentations: 30 F1-score: 0.9963671397829769 Time:617.6512229442596 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . So we are getting a F1-score of 0.996367. Not bad at all! Who wants an autonomous ride?? ü•≥ üòÜ . Save the model . torch.save(learn.model.state_dict(), &#39;resnet34_weights_traffic.pth&#39;) .",
            "url": "https://xanaga.github.io/posts_en/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "relUrl": "/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "date": " ‚Ä¢ Apr 21, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "First Post",
            "content": "Finally, here we are‚Ä¶ . . The beginning . I have been self-learning about AI and Deep Learning since I started my degree more than three years ago. But lately, I had the feeling of being stuck, even a bit lost. Moreover, the idea of starting to share my work has always been in the back of my head. I think individually we can achieve cool things, but the big changes come from communities. We can find many examples just in the AI field, from Fastai to Hugging Face and many more. I want to be connected with those communities!üåç . Thinking about the best way to do it, I remembered what Jeremy Howard said about the importance of blogging in the second edition of one of the first courses I took. Then I discovered Fastpages, so the decision was taken, I would start blogging! . What you will find in this blog . With this work I want to share my work, the things I learn, and maybe my thoughts. It took me too long to start blogging because I wanted everything to be perfect, but this is not the idea. I will start my blog, it will be dynamic, things will change, there will be mistakes, and it never will be perfect. Beautiful, isn‚Äôt it? . Together with AI (which is my true loveüòç), I‚Äôm trying to get into Computer Graphics, VR/AR, and simulation. For that reason, I am trying to learn about Geometric Deep Learning, Neural Rendering, and also Unreal Engine! . Please remember this is a leaner‚Äôs blog, you are encouraged to help, give suggestions, and correct me if I am wrong. Let‚Äôs do this journey together! . Why not in Spanish? . As you may have noticed, I‚Äôm not a native English speaker. In fact, my mother tongue is Spanish. So, why not write in Spanish. Well, there are a couple of reasons: . I want to practice my English. Learn by doing! | In English, I have access to a broader audience, so more people can help or share their ideas. | And well‚Ä¶ I have never said there won‚Äôt be Spanish postsüòâ I think with different languages we can target different audiences. So if I see some content is redundant or irrelevant in English, I will make it in Spanish. | . So see you in the next one, adi√≥s!üëã .",
            "url": "https://xanaga.github.io/posts_en/general/2022/04/18/first-post.html",
            "relUrl": "/general/2022/04/18/first-post.html",
            "date": " ‚Ä¢ Apr 18, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, my name is Xavi! I am a Computational Maths Bachelor student, passionate about AI and Deep Learning since the first year of my degree. Specifically, I would like to apply the advances in this field to Computer Graphics, VR/AR, simulation, etc. Nevertheless, I love to explore new things, so my interests go from Brain Computer Interfaces to space exploration, ranging everything that includes technology and takes us closer to the future. I also love sports, discovering new countries and eating... I love food! With this blog, I hope to meet more people with my same interest so if you have any suggestions, doubts, ideas, or simply want to chat, don&#39;t think it twice, and drop me a line!üòÑ",
          "url": "https://xanaga.github.io/posts_en/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://xanaga.github.io/posts_en/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}