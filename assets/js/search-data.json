{
  
    
        "post0": {
            "title": "Knowledge Distillation for Traffic Sign Recognition",
            "content": ". Introduction . It seems that the current trend in Deep Learning is to have bigger and more models. This makes it difficult for users to use them, fit them into small devices, get fast results etc. This is why I see model compression, knowledge distillation, and these kinds of techniques as one of the more interesting and useful topics in deep learning. After all, if you want to apply deep learning in VR/AR you need to fit them into small devices. Also for simulations and computer graphics is better to have optimized and fast models. In addition, this makes AI more affordable to everyone, democratizing access to this technology. With this objective in mind I tried to get similar results as in my previous post, but using a resnet18 instead of a resnet34. Before continuing reading this post I encourage you to take a look at the previous one. Nevertheless, I was not successful at all. Looking for some solutions I discovered FasterAI by Nathan Hubens, which is an awesome library that implements those compression techniques based on Fastai. So my objectives with this notebook are: . Try to implement Knowledge Distillation technique using FasterAI to start getting familiar with the library. I would like to apply this model compression to other projects too (but that will not be covered in this notebook) | Encourage anyone who is thinking about implementing these types of approaches that may seem complicated to try this library. | . Imports . First we will upgrade the fastai version used in Colab, by deafult it is the first version. . ! pip install -Uqq fastai # upgrade fastai on colab . %reload_ext autoreload %autoreload 2 %matplotlib inline . import fastai fastai.__version__ . &#39;2.5.6&#39; . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.data.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . This will be analogous to the Download section in the previous post. Despite that, I have added the possibility to only train on some classes of the dataset. Because of using Knowledge Distillation we should be able to classify signals that never have been seen in the training set (or at least give an informed guess). For this notebook, we will use all the classes, but you are encouraged to try to eliminate some and see how it behaves! . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . c=list(range(43)) # List the classes you want to train on . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. It is analogous to the process made in the resnet34 notebook, with some minor changes to handle the cases where not all the classes are selected. A more detailed explanation can be found in the previous post. . We have changed the read_annotations function to allow us to filter and read only the annotations of the classes we are interested in. This is done by adding the if inside the for loop. So the classes will have to be passed as an argument in all the following functions. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename, classes=None): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column if classes is None or label in classes: # We only read the annotations of the classes we are interested annotations.append(Annotation(filename, label)) return annotations def load_training_annotations(source_path, classes=None): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename, classes)) return annotations def copy_files(label, filenames, source, destination, classes=None, move=False): func = os.rename if move else shutil.copyfile label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) for filename in filenames: if classes is None or int(os.path.basename(label_path)) in classes: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) def split_train_validation_sets(source_path, train_path, validation_path, all_path, classes, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) annotations = load_training_annotations(source_path) filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) for label, filenames in filenames.items(): filenames = sorted(filenames) validation_size =int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, classes, move=False) copy_files(label, train_filenames, source_path, train_path, classes, move=True) copy_files(label, validation_filenames, source_path, validation_path, classes, move=True) . Due to the changes we have made in the previous functions we only have to add the information about the classes in the last step, passing it to the split_train_validation_sets function. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, c, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Prepare the data . Data preparation is even more similar to the previous post. We will only add a vocab when creating the dataset. This is because if we don&#39;t specify it Fastai will take as vocab the classes that appear in our data, which would be problematic if we want to restrict the problem to only a subset of classes. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], vocab=[str(i) for i in range(43)]) . We can see that our vocab will have all the classes, no matter which classes we use to train. . data.vocab . [&#39;0&#39;, &#39;1&#39;, &#39;10&#39;, &#39;11&#39;, &#39;12&#39;, &#39;13&#39;, &#39;14&#39;, &#39;15&#39;, &#39;16&#39;, &#39;17&#39;, &#39;18&#39;, &#39;19&#39;, &#39;2&#39;, &#39;20&#39;, &#39;21&#39;, &#39;22&#39;, &#39;23&#39;, &#39;24&#39;, &#39;25&#39;, &#39;26&#39;, &#39;27&#39;, &#39;28&#39;, &#39;29&#39;, &#39;3&#39;, &#39;30&#39;, &#39;31&#39;, &#39;32&#39;, &#39;33&#39;, &#39;34&#39;, &#39;35&#39;, &#39;36&#39;, &#39;37&#39;, &#39;38&#39;, &#39;39&#39;, &#39;4&#39;, &#39;40&#39;, &#39;41&#39;, &#39;42&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . Knowledge Distillation . This is a technique proposed by Geoffrey E. Hinton as a way to extract the knowledge from big models and use it to train simpler architectures. Moreover, it also has proven to provide what seems to be more robust and general learning which is always desirable. We have considered this approach because of the increasing size of the models not only in computer vision tasks but also in Natural Language Processing (NLP), which makes those difficult to be used in inference for very time demanding tasks or even impossible to fit in embedded systems. . The high-level idea is to, instead of training the models with one-hot encoded vectors, train them using a ‚Äúsofter version‚Äù of those that stores some information about the similarity between images. Is a concept similar to word embeddings in NLP. These labels are obtained from (normally) a bigger model which is already trained and used only for inference. . Here is where we start using FasterAI library. Nevertheless, I found problems loading a model into a learner with Fastai. As KnowledgeDistillation function expects to receive a learner I had to copy and paste this function and modify it by adding the from_learner parameter. In this way, I can set it to false and use as a teacher a plain PyTorch model. Nevertheless, the correct way of using it would be as straightforward as installing the library: . pip install fasterai . And import the things you need: . from fasterai import ... . sz = 96 bs = 256 wd = 5e-3 f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) . To act as the teacher we will use the model we trained in the previous post which can be found in this repo. You can download it from there or train your own model using the previous notebook. Once you have the model trained you will have to set the path_to_model variable. . If you are working in Colab you can drag and drop the model into the Colab&#39;s file explorer or save the model in your drive and mount it (just by clicking the Mount Drive button in the files tab, on the left in Colab). By mounting the drive you will be able to reach the model file by navigating across your drive directories. . path_to_model = #your_path . The teacher model &#128188; &#128207; . First, we prepare the teacher model. To do so, we will get the architecture of our model with random weights and then we will fill it with the trained weights. . Important: To do that, is necessary that the random filled architecture has exactly the same parameters as the ones in the pretrained ones. This is why we use create_cnn_model fuction of Fastai, which given a model architecture it creates a model (which is a Sequential object) with the head and the body that we choose. In the previous notebook we created the Learner with the default settings, so now we will use those defaults too. For each architecture, Fastai has a way of cutting the model, and for all of them it uses the same default head, you can check the source code! . If you would want to create the model with a different head and body you would have to use the cut and custom_head arguments. You can find more information about it in the documentation. . The to_device method will place it into GPU or CPU depending on the hardware available. . teacher = create_cnn_model(resnet34, 43, pretrained=False).to(device) #get the model architecture teacher.load_state_dict(torch.load(path_to_model+&#39;resnet34_weights.pth&#39;)) #load the trained wheights teacher . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=43, bias=False) ) ) . . . Note: The teacher is a Sequential object with two modules. The module (0) is the body, and it is the original architecture of the model (in our case resnet34), which can be pretrained (not in our case). The second module, the (1): Sequential, is the head which is initialized using kaiming_normal initialization by default(it can be changed using the init argument), so it is not pretrained. The layers of this head are the default used in Fastai. The decision about where to cut the original model is taken according to Fastai metadata for each architecture, but as we have said it is customizable using the cut argument. Our teacher model is almost ready! The only thing that last is that, as we don&#39;t want to propagate gradients through our teacher (we only want it for inference) we should make sure it is frozen. . The following PyTorch code does exactly that. . for param in teacher.parameters(): param.requires_grad = False . . Note: Inside the KnowledgeDistillation callback we use the teacher in eval() mode, so the gradients would not be calculated even if the model is not frozen, but is better to make sure. . The student Learner &#128215; . Now let&#39;s prepare our student network. As before we use the create_cnn_model, but with a pretrained resnet18 architecture, to build the model that we will pass to the Learner constructor. . student_model=create_cnn_model(resnet18, 43).to(device) #get the model architecture and pretrained wheights . Then, we could do the following: . bad_student = Learner(data, student_model, metrics=[accuracy,f1_score_mult]) bad_student.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 11,725,888 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . But this would unfreeze the whole student! We don&#39;t want that, we would like to have the earlier pretrained layers of the model (the body) frozen and the rest (the head) to be trainable. The reason is that, by default, Learner does not freeze any parameter group. . So let&#39;s freeze the body. To do that we use the freeze_to(n) method where n is the number of parameter groups we want to freeze. We take n=1 as we only want the body to be frozen. . bad_student.freeze_to(1) bad_student.summary() . /usr/local/lib/python3.7/dist-packages/fastai/optimizer.py:22: UserWarning: Freezing 1 groups; model has 1; whole model is frozen. warn(f&#34;Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.&#34;) . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 False ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 False ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 12,672 Total non-trainable params: 11,713,216 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have frozen the entire model! ü•∂ . Note: The BatchNorm2d layers are never frozen, this is why we have some trainable parameters. If we look at the warning at the top of the output, we can see the problem. We only have one parameter group which is the entire model. . So, how are those parameters groups chosen? By the argument splitter, which by default is trainable_parameters. This will return all the trainable parameters of the model. In our case, they are all trainable, so we will only get one parameter group, hence the whole student will be frozen. . The solution is to pass splitter=default_split. This will split the parameter groups in body and head, just as we want! . . Important: In the Explore Training section we will use lr_find(), which does not accept any weight decay nor callback arguments (it takes the ones defined when creating the Learner). Then the correct thing to do is to pass as an argument to the learner the KnowledgeDistillation callback, as we do with the weight decay. By doing so we have thesame training scenario when doing lr_find() and fit_one_cycle(). . loss = partial(SoftTarget, T=20) kd = KnowledgeDistillation(teacher, loss, from_learner=False) . student = Learner(data, student_model, metrics=[accuracy,f1_score_mult], splitter=default_split, wd=wd, cbs=kd) student.freeze_to(1) #student.summary() . . Warning: If you use student.summary() it will raise the following error: Exception occured in KnowledgeDistillation when calling event after_loss: cross_entropy_loss(): argument &#39;target&#39; (position 2) must be Tensor, not NoneType. Although, it only affect to the summary method. So we can continue and if we really want to check the learner we can remove and add back the callback, as the collapsed code shows below. . print(&#39;See in which position the KnowledgeDistillation is:&#39;) print(student.cbs) print() student.remove_cb(cb=student.cbs[3]) # We could also do cb=kd print(student.summary()) # Obviously you won&#39;t see the KnowledgeDistillation callback at the end student.add_cb(cb=kd) # Add back the callback print() print(&#39;Check that you have the same list as before the summary:&#39;) print(student.cbs) . . See in which position the KnowledgeDistillation is: [TrainEvalCallback, Recorder, ProgressCallback, KnowledgeDistillation] . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 11,725,888 Total trainable params: 558,976 Total non-trainable params: 11,166,912 Optimizer used: &lt;function Adam at 0x7fa08117a3b0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback Check that you have the same list as before the summary: [TrainEvalCallback, Recorder, ProgressCallback, KnowledgeDistillation] . Training . In the training process is where FasterAI magic comes in üßô . Only by adding the KnowledgeDistillation callback when fitting the model, we will be able to use this technique with almost the same code as in the previous post. But, as we setted those parameters when creating the Learner(), we don&#39;t even need to do that!üéâ . Explore training . We will follow the same procedure as is the previous post for finding the hyperparameters that better train our model. . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . . Note: Remember that the weight decay and the Knowledge Distillation Callback where setted in the Learner() definition. . student.lr_find() # wd and callback included . SuggestedLRs(valley=0.004365158267319202) . student.fit_one_cycle(1, lr_max=0.001) # wd and cbs are setted in Learner() . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 15.465137 | 13.668188 | 0.605898 | 0.563930 | 01:35 | . student.unfreeze() . student.lr_find() . SuggestedLRs(valley=0.00019054606673307717) . student.fit_one_cycle(9, lr_max=slice(0.0001, 0.001)) #, wd=wd, cbs=kd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 9.225066 | 6.410910 | 0.833745 | 0.821246 | 01:41 | . 1 | 4.279030 | 1.822614 | 0.960905 | 0.957461 | 01:36 | . 2 | 2.453992 | 1.222131 | 0.974760 | 0.972952 | 01:37 | . 3 | 1.889181 | 0.925809 | 0.982579 | 0.981159 | 01:37 | . 4 | 1.645638 | 0.775705 | 0.988889 | 0.988514 | 01:37 | . 5 | 1.506031 | 0.679071 | 0.989575 | 0.989230 | 01:37 | . 6 | 1.422567 | 0.668275 | 0.990809 | 0.990585 | 01:36 | . 7 | 1.355428 | 0.612182 | 0.991632 | 0.991374 | 01:36 | . 8 | 1.323943 | 0.623958 | 0.991495 | 0.991259 | 01:37 | . student.lr_find() . SuggestedLRs(valley=1.5848931980144698e-06) . student.fit_one_cycle(6, lr_max=slice(0.00001, 0.0001))#, wd=wd, cbs=kd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 1.322057 | 0.606176 | 0.991770 | 0.991487 | 01:37 | . 1 | 1.320874 | 0.618872 | 0.990535 | 0.990247 | 01:36 | . 2 | 1.300413 | 0.615932 | 0.992730 | 0.992584 | 01:36 | . 3 | 1.300726 | 0.611663 | 0.992455 | 0.992312 | 01:36 | . 4 | 1.290397 | 0.592799 | 0.992455 | 0.992310 | 01:37 | . 5 | 1.286910 | 0.597255 | 0.992455 | 0.992296 | 01:37 | . Retrain on the whole dataset . Once we know which configuration is better we can train the model on all the data available (training+validation). . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], vocab=[str(i) for i in range(43)], train=&#39;all&#39;) . teacher = create_cnn_model(resnet34, 43, pretrained=False).to(device) teacher.load_state_dict(torch.load(path_to_model+&#39;resnet34_weights.pth&#39;)) student_model=create_cnn_model(resnet18, 43).to(device) student = Learner(data, student_model, metrics=[accuracy,f1_score_mult], splitter=default_split, wd=wd, cbs=kd) student.freeze_to(1) . student.fit_one_cycle(1, lr_max=0.001) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 13.978484 | 11.242065 | 0.715364 | 0.683073 | 01:49 | . student.unfreeze() student.fit_one_cycle(10, lr_max=slice(0.0001, 0.001)) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 9.187653 | 5.441027 | 0.893416 | 0.883363 | 01:54 | . 1 | 3.874919 | 1.078969 | 0.987106 | 0.987011 | 01:53 | . 2 | 2.228036 | 0.545888 | 0.997257 | 0.997255 | 01:53 | . 3 | 1.754988 | 0.441869 | 0.998080 | 0.998076 | 01:52 | . 4 | 1.543820 | 0.505094 | 0.998903 | 0.998900 | 01:53 | . 5 | 1.451698 | 0.324294 | 0.999726 | 0.999725 | 01:52 | . 6 | 1.336454 | 0.319012 | 0.999863 | 0.999863 | 01:51 | . 7 | 1.282818 | 0.278213 | 1.000000 | 1.000000 | 01:51 | . 8 | 1.239342 | 0.259717 | 1.000000 | 1.000000 | 01:50 | . 9 | 1.236298 | 0.250886 | 1.000000 | 1.000000 | 01:51 | . student.fit_one_cycle(6, lr_max=0.0001) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 1.270205 | 0.312592 | 0.999726 | 0.999725 | 01:50 | . 1 | 1.336736 | 0.343301 | 0.999863 | 0.999863 | 01:51 | . 2 | 1.285208 | 0.289410 | 0.999863 | 0.999863 | 01:50 | . 3 | 1.217801 | 0.267841 | 0.999863 | 0.999863 | 01:51 | . 4 | 1.179426 | 0.231982 | 1.000000 | 1.000000 | 01:51 | . 5 | 1.172143 | 0.221064 | 1.000000 | 1.000000 | 01:50 | . Test . Like we did in the previous post we will use Test Time Augmentations, and we will use the same function to plot how the performance and inference time behave. . Note: We have added to the test_time_aug function the mask argument. This will allow us to compute the F1-score only for some classes. For example, only for the ones we have trained on. . def test_time_aug(learner, test_dataloader, y_true, metric, n_augs=[10], beta=0.1,mask=None): res = [] if mask is None: mask=list(range(len(y_true))) learner.eval() for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learner.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds[mask], y_true[mask]) res.append([aug, score,infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . true_test_labels = {a.filename: a.label for a in test_annotations} class_indexes = data.vocab.o2i test_img=get_image_files(&#39;./data/test&#39;) filenames = [filepath.name for filepath in test_img] labels = [str(true_test_labels[filename]) for filename in filenames] y_true = np.array([class_indexes[label] for label in labels]) . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . The following code will create the mask used for computing the F1-score. You only have to specify in which classes you want to focus on the interest_classes variable. We will focus on all the classes that we trained with (which are all the classes). . interest_classes=c interest_idx=[data.vocab.o2i[str(cl)] for cl in interest_classes] mask=np.isin(y_true, interest_idx, invert=False) . . Note: If you set invert to True you will get the F1-score only on the images you have not seen in the training. If we try to use our learner for as it is we will get an error, produced by the KnowledgeDistallation callback. But we don&#39;t need it anymore! (sorry Nathan, you&#39;ve been a hero üòî). Let&#39;s remove it. . student.cbs . (#4) [TrainEvalCallback,Recorder,ProgressCallback,KnowledgeDistillation] . student.cbs[3] . KnowledgeDistillation . student.remove_cb(cb=student.cbs[3]) student.cbs . (#3) [TrainEvalCallback,Recorder,ProgressCallback] . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(student, test_dataloader, y_true, metric, n_augs=[0, 5, 10, 20, 30]) . N Augmentations: 0 F1-score: 0.9922140340549594 Time:24.077036380767822 . . N Augmentations: 5 F1-score: 0.9933502950176439 Time:117.72993874549866 . . N Augmentations: 10 F1-score: 0.9943761367605225 Time:212.58393836021423 . . N Augmentations: 20 F1-score: 0.9950922306435893 Time:405.470721244812 . . N Augmentations: 30 F1-score: 0.9947694889306348 Time:592.3682796955109 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . Conclusion . As we have seen Knowledge Distillation allows us to train for more epochs without overfitting and improves the final performance of the model compared with not using it (and getting very close of the resnet34, which resulted in 0.9963 F1-score). An intuitive explanation can be that we are giving more information to the network in every step of the backpropagation by, not only providing one-hot labels, but also a probability of the image to be another class. With this we are introducing the idea of similarities between classes. . I see these types of compression approaches as very interesting and I would like to apply them in other domains, maybe to NLP or 3D data. So I cannot recommend more FasterAI library and infinitely thank the work done to Nathan Hubens. Also, if you want to learn more about how to make smaller and faster Neural Networks I encourage you to visit his blog. . Thanks for reading!üòÄ .",
            "url": "https://xanaga.github.io/posts_en/knowledgedistillation/fastai/computer%20vision/2022/04/24/Knowledge_Distillation.html",
            "relUrl": "/knowledgedistillation/fastai/computer%20vision/2022/04/24/Knowledge_Distillation.html",
            "date": " ‚Ä¢ Apr 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "ResNet for Traffic Sign Classification With PyTorch",
            "content": ". Introduction . In the Intelligent Systems course from my degree, we were asked to write a report about a Neural Network related topic. In class, we were introduced to the German Traffic Sign Recognition Benchmark dataset, so I decided to explore previous work of the community. This is how I discovered Pavel Surmenok which has a post dedicated to this subject, using Fastai! As Pavel&#39;s implementation was using the first version of Fastai and I wanted to get more familiar with this library I thought it would be a good idea to try to implement a version of his code using the newest version of Fastai (version 2.5.3 at the time of doing the notebook). So the objectives are the following: . Get hands-on experience with Fastai to apply the theory I know of deep learning. I think it is a great tool for rapid testing with impressive results. Start building things! | Best case scenario, being able to contribute to updating the work that Pavel presented in his blog and help anyone who wants to start with Fastai v2. | Try to explain things I didn&#39;t know before and I would like to find somewhere. The aim of this is to consolidate the knowledge and also help anyone that had the same questions. | . Imports . import fastai fastai.__version__ # check that we have Fastai v2 . &#39;2.5.6&#39; . Show the collapsed code to see all the imports we will need . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch import time import pandas as pd . . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. . Tip: It is not directly related to Fastai or Deep Learning, but knowing how to manage your data is VERY important, and Pavel makes it in an understandable and elegant way. If you don&#8217;t have a lot of confidence working with data this is a section worth looking at. . Annotations will be named tuples containing the name of the file and its corresponding label. You can find a complete tutorial on namedtuple() here, but basically, those are tuple-like immutable data structures with named fields. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column annotations.append(Annotation(filename, label)) return annotations . load_training_annotations will loop over all the files from all the classes and returns the annotations of all the training examples. . def load_training_annotations(source_path): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename)) return annotations . . Note: my_list.extend(iter) will add each element of an iterable,iter, to the list my_list. If we would have used append we would get a list of list of named tuples instead of a list of named tuples. . We have talked about the filenames, labels etc. but our actual data are images. We will use the copy_files function to organize the images in training, validation and all (training+validation) folders. . def copy_files(label, filenames, source, destination, move=False): # copy to the training or validation folders, for the all folder just rename func = os.rename if move else shutil.copyfile # make a directory for every label label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) # fill the directories with its corresponding files for filename in filenames: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) . Finally, we use all the above mentioned to build our own split_train_validation_sets function. . def split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) # annotations will be a list of Annotations(filename, label) annotations = load_training_annotations(source_path) # filenames will be a dictionary # keys: label # values: list of file names filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) # for every label calculate the validation_size and populate the directories for label, filenames in filenames.items(): filenames = sorted(filenames) # get the validation_size, it must be an integer! validation_size = int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, move=False) copy_files(label, train_filenames, source_path, train_path, move=True) copy_files(label, validation_filenames, source_path, validation_path, move=True) . Once we have all these functions all we have to do is call them with the appropriate paths. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Exploratory analisys . We have our data prepared so, first things first, let&#39;s take a look at it. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] classes . ClassId SignName . 0 0 | Speed limit (20km/h) | . 1 1 | Speed limit (30km/h) | . 2 2 | Speed limit (50km/h) | . 3 3 | Speed limit (60km/h) | . 4 4 | Speed limit (70km/h) | . 5 5 | Speed limit (80km/h) | . 6 6 | End of speed limit (80km/h) | . 7 7 | Speed limit (100km/h) | . 8 8 | Speed limit (120km/h) | . 9 9 | No passing | . 10 10 | No passing for vechiles over 3.5 metric tons | . 11 11 | Right-of-way at the next intersection | . 12 12 | Priority road | . 13 13 | Yield | . 14 14 | Stop | . 15 15 | No vechiles | . 16 16 | Vechiles over 3.5 metric tons prohibited | . 17 17 | No entry | . 18 18 | General caution | . 19 19 | Dangerous curve to the left | . 20 20 | Dangerous curve to the right | . 21 21 | Double curve | . 22 22 | Bumpy road | . 23 23 | Slippery road | . 24 24 | Road narrows on the right | . 25 25 | Road work | . 26 26 | Traffic signals | . 27 27 | Pedestrians | . 28 28 | Children crossing | . 29 29 | Bicycles crossing | . 30 30 | Beware of ice/snow | . 31 31 | Wild animals crossing | . 32 32 | End of all speed and passing limits | . 33 33 | Turn right ahead | . 34 34 | Turn left ahead | . 35 35 | Ahead only | . 36 36 | Go straight or right | . 37 37 | Go straight or left | . 38 38 | Keep right | . 39 39 | Keep left | . 40 40 | Roundabout mandatory | . 41 41 | End of no passing | . 42 42 | End of no passing by vechiles over 3.5 metric tons | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We can try to do histogram equalization to see if it improves our results, but in the end, it does not. If you want to try you can use the HistogramEqualization_item class as an item_tfms for the Fastai pipeline. . . Note: You can also try to implement histogram equalization in the way Pavel does in his notebook, but I could not see the changes visually, maybe something has changed in how Fastai opens images. . # Histogram equalization class HistogramEqualization_item(Transform): def init(self, prefix=None): self.prefix = prefix or &#39;&#39; def encodes(self, o): if type(o) == PILImage: ret = ImageOps.equalize(o) else: ret = o return ret def decodes(self, o): return o . . We could start inspecting the data as it is, but Fastai provides us with some useful functions to visualize our data. To use them we will have to integrate this data into Fastai datastructures. We could create a DataBunch or Datasets or go directly to Dataloaders. We will do the last one, but you can find a lot of information in tutotials and documentation. . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)]) . The item_tfms will be applied to every item in our dataset independently (here is where you have to add the HistogramEqualization_item transform if you want to try). Those are performed in the CPU every time an item is accessed. In our case, we only use Resize. Contrary, the batch_tfms will be applied to all the images in the batch at the same time. Those are done in the GPU and are very fast. You can find more information about aug_transforms in the documentation, but basically is a wrap of very useful transforms for image data. As you may note, we randomly resize again the images. The reason is that this is the resizing we want to apply to our images, the one we did previously in √¨tem_tmfs makes all the images the same size and square, to be able to perform batch_tfms in the GPU. These augmentations are key to having a good performance, and this applies to every deep learning project you will work on. . . Note: We normalize using imagenet_stats because our models will be pretrained on this dataset, which is way larger than the one we are working with. . Now we can use show_batch method to see some examples from our dataset. . data.show_batch(nrows=3, ncols=3) . We can also see how the augmentations affect our data by seeing how those affect to a single example. . data.show_batch(nrows=3, ncols=3, unique=True) . We can also check the length of our training and validation set. . len(data.train_ds), len(data.valid_ds) . (31919, 7290) . . Important: For the sake of the blog post lenght, we finish here our &quot;Exploratory analysis&quot;. Nevertheless, explore the data not only includes watching at the data (which is very important), but also the sizes, class distribution etc. A more complete analysis can be found in Pavel&#8217;s post, and you are encouraged to go further with the help of Fastai. . Training . Explore training . In this section, we will try to find the best training hyperparameters to train the model. To do so we will train on the train data and evaluate on the validation data. Then we will use these hyperparameters to train the model on the whole data available (training + validation) and we will test it with the test data. . The metrics we will track will be accuracy and F1-score, because we have seen that the dataset shows class imbalance. The F1-score metric is more robust and informative about the model performance when we are working with this kind of dataset. Concerning the model, we will use a resnet34 pretrained on imagenet. The data, model, and metrics to track will be wrapped up together in a learner object. . Note: We are passing the weight decay as an argument to the Learner(). By doing so, we will use it when calling both lr_find() and fit_one_cycle() methods in the Explore training section. . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) wd = 5e-4 learn = cnn_learner(data, resnet34, metrics=[accuracy,f1_score_mult], wd=wd) . learn.summary() # Display information about the learner obj . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 566,400 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . . Important: Our model is frozen up to parameter group #2 (the whole model except the last layer), this means that when running backpropagation the gradients won&#8217;t be calculated for those layers, hence their weights will remain unchanged. . One of the most important hyperparameters in deep learning is the learning rate. To find a good one we can use lr_find method, which takes a batch and runs it through the network with incrementally bigger learning rates recording the loss for each run. The idea is to take the learning rate where the graph of the loss is the steepest. You can try to find it visually or let Fastai give you a suggestion (at least in this case it tend to suggest smaller learning rates). . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . learn.lr_find() . SuggestedLRs(valley=0.0020892962347716093) . Train the model with the learning rate selected for one epoch. In this epoch, we will only update the weights of the last layer, as the rest of the model is frozen (with the pretrained weights from imagenet). This will allow our last layer to &quot;catch up&quot; with the other ones and not to start randomly. The fit_one_cycle method uses the 1 cycle policy, which is a learning rate schedule policy consisting in starting from a low learning rate in the firsts epochs, increasing it up to a maximum, to decrease it again for the last epochs. A better explanation can be found in this post from Sylvain Gugger. . learn.fit_one_cycle(1, lr_max=0.005) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.875589 | 0.453181 | 0.853498 | 0.848488 | 01:30 | . Then we unfreeze the model to train it completely. With this, gradients will propagate through the whole network updating all the weights. . learn.unfreeze() learn.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 21,834,048 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have a different training scenario, so probably our learning rate has changed. So we repeat the process of finding the learning rate and training for some epochs. We can use slice when specifying the learning rate to allows us to use &quot;discriminative layer training&quot;, which consists of using different learning rates for different layers. Usually, we use a smaller learning rate for the earlier layers, as they are well trained to detect general features on a lot of data from imagenet, and a bigger one for the last layers, as they have to change more to capture the more specific features from our dataset. . learn.lr_find() . SuggestedLRs(valley=9.120108734350652e-05) . learn.fit_one_cycle(3, lr_max=0.0001) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.296145 | 0.232131 | 0.929218 | 0.925996 | 01:38 | . 1 | 0.155398 | 0.110068 | 0.970370 | 0.967639 | 01:38 | . 2 | 0.105242 | 0.086704 | 0.975034 | 0.972592 | 01:38 | . learn.lr_find() . SuggestedLRs(valley=1.2022644114040304e-05) . learn.fit_one_cycle(7, lr_max=slice(0.0001, 0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.117177 | 0.343552 | 0.925652 | 0.928776 | 01:38 | . 1 | 0.154971 | 0.221667 | 0.945816 | 0.942552 | 01:39 | . 2 | 0.117753 | 0.065744 | 0.982167 | 0.980668 | 01:38 | . 3 | 0.096865 | 0.046334 | 0.988615 | 0.988594 | 01:38 | . 4 | 0.065272 | 0.045523 | 0.986831 | 0.986001 | 01:38 | . 5 | 0.058440 | 0.033738 | 0.990535 | 0.990093 | 01:38 | . 6 | 0.055586 | 0.030030 | 0.990261 | 0.989890 | 01:38 | . Retrain on the whole dataset . Once we have tried different configurations and selected the one we think is best, it is time to train the model with all the data! . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) sz = 96 bs = 256 wd = 5e-4 . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], train=&#39;all&#39;) learn = cnn_learner(data, resnet34,metrics=[accuracy,f1_score_mult], wd=wd) . learn.fit_one_cycle(1, lr_max=0.01) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.542564 | 0.107461 | 0.964746 | 0.964709 | 01:46 | . learn.unfreeze() learn.fit_one_cycle(4, lr_max=slice(0.001,0.01)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.385180 | 0.500579 | 0.864060 | 0.859983 | 01:58 | . 1 | 0.210768 | 0.012989 | 0.995610 | 0.995585 | 01:58 | . 2 | 0.111806 | 0.003372 | 0.998903 | 0.998901 | 01:58 | . 3 | 0.063384 | 0.000662 | 0.999863 | 0.999863 | 01:58 | . learn.fit_one_cycle(6, lr_max=slice(0.0001,0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.059273 | 0.000613 | 0.999863 | 0.999863 | 01:58 | . 1 | 0.059253 | 0.001651 | 0.999451 | 0.999452 | 01:58 | . 2 | 0.058100 | 0.000631 | 0.999726 | 0.999726 | 01:58 | . 3 | 0.048571 | 0.000100 | 1.000000 | 1.000000 | 01:58 | . 4 | 0.042215 | 0.000125 | 1.000000 | 1.000000 | 01:58 | . 5 | 0.038188 | 0.000095 | 1.000000 | 1.000000 | 01:57 | . Test . For the test, we will use &quot;Test time augmentations&quot;. This technique is also used by Pavel and shows an increase in the performance of the model. It consists in, instead of just making the prediction of the given test image, making the prediction of the original and other augmented versions of this image using the augmentations used while training. The final prediction will be an average of all of them. This will take more time for each prediction but it will make them more robust. test_time_aug function will test the model with a different number of augmentations and it will return a Pandas Dataframe with the F1-scores and the time it has taken to run the inference of the test set. . def test_time_aug(learner, test_dataloader, metric, n_augs=[10], beta=0.1): res = [] for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learn.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds, y_true) res.append([aug, score, infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . As we did before, we need the test data to be in a DataLoader format. . true_test_labels = {a.filename: a.label for a in test_annotations} #get the annotations in a dictionary format class_indexes = data.vocab.o2i #dictionary from class to index test_img=get_image_files(&#39;./data/test&#39;) #list of the test image file paths filenames = [filepath.name for filepath in test_img] #get the names from the file paths labels = [str(true_test_labels[filename]) for filename in filenames] #list of the labels for each file y_true = np.array([class_indexes[label] for label in labels]) #array of the index for each label . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . len(test_dataloader.dataset) . 12630 . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(learn, test_dataloader, metric, n_augs=[0, 3, 5, 10, 15, 20, 30]) . N Augmentations: 0 F1-score: 0.994469328801153 Time:19.947819232940674 . . N Augmentations: 3 F1-score: 0.9943121917876697 Time:81.12456560134888 . . N Augmentations: 5 F1-score: 0.9950268203228465 Time:119.88853621482849 . . N Augmentations: 10 F1-score: 0.9959702387438432 Time:218.91360545158386 . . N Augmentations: 15 F1-score: 0.996130519244009 Time:317.34042143821716 . . N Augmentations: 20 F1-score: 0.996285695102012 Time:414.57182717323303 . . N Augmentations: 30 F1-score: 0.9963671397829769 Time:617.6512229442596 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . So we are getting a F1-score of 0.996367. Not bad at all! Who wants an autonomous ride?? ü•≥ üòÜ . Save the model . torch.save(learn.model.state_dict(), &#39;resnet34_weights_traffic.pth&#39;) .",
            "url": "https://xanaga.github.io/posts_en/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "relUrl": "/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "date": " ‚Ä¢ Apr 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "First Post",
            "content": "Finally, here we are‚Ä¶ . . The beginning . I have been self-learning about AI and Deep Learning since I started my degree more than three years ago. But lately, I had the feeling of being stuck, even a bit lost. Moreover, the idea of starting to share my work has always been in the back of my head. I think individually we can achieve cool things, but the big changes come from communities. We can find many examples just in the AI field, from Fastai to Hugging Face and many more. I want to be connected with those communities!üåç . Thinking about the best way to do it, I remembered what Jeremy Howard said about the importance of blogging in the second edition of one of the first courses I took. Then I discovered Fastpages, so the decision was taken, I would start blogging! . What you will find in this blog . With this work I want to share my work, the things I learn, and maybe my thoughts. It took me too long to start blogging because I wanted everything to be perfect, but this is not the idea. I will start my blog, it will be dynamic, things will change, there will be mistakes, and it never will be perfect. Beautiful, isn‚Äôt it? . Together with AI (which is my true loveüòç), I‚Äôm trying to get into Computer Graphics, VR/AR, and simulation. For that reason, I am trying to learn about Geometric Deep Learning, Neural Rendering, and also Unreal Engine! . Please remember this is a leaner‚Äôs blog, you are encouraged to help, give suggestions, and correct me if I am wrong. Let‚Äôs do this journey together! . Why not in Spanish? . As you may have noticed, I‚Äôm not a native English speaker. In fact, my mother tongue is Spanish. So, why not write in Spanish. Well, there are a couple of reasons: . I want to practice my English. Learn by doing! | In English, I have access to a broader audience, so more people can help or share their ideas. | And well‚Ä¶ I have never said there won‚Äôt be Spanish postsüòâ I think with different languages we can target different audiences. So if I see some content is redundant or irrelevant in English, I will make it in Spanish. | . So see you in the next one, adi√≥s!üëã .",
            "url": "https://xanaga.github.io/posts_en/general/2022/04/18/first-post.html",
            "relUrl": "/general/2022/04/18/first-post.html",
            "date": " ‚Ä¢ Apr 18, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, my name is Xavi! I am a Computational Maths Bachelor student, passionate about AI and Deep Learning since the first year of my degree. Specifically, I would like to apply the advances in this field to Computer Graphics, VR/AR, simulation, etc. Nevertheless, I love to explore new things, so my interests go from Brain Computer Interfaces to space exploration, ranging everything that includes technology and takes us closer to the future. I also love sports, discovering new countries and eating... I love food! With this blog, I hope to meet more people with my same interest so if you have any suggestions, doubts, ideas, or simply want to chat, don&#39;t think it twice, and drop me a line!üòÑ",
          "url": "https://xanaga.github.io/posts_en/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://xanaga.github.io/posts_en/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}