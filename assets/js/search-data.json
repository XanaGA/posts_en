{
  
    
        "post0": {
            "title": "ResNet for Traffic Sign Classification With PyTorch",
            "content": ". Introduction . In the Intelligent Systems course from my degree, we were asked to write a report about a Neural Network related topic. In class, we were introduced to the German Traffic Sign Recognition Benchmark dataset, so I decided to explore previous work of the community. This is how I discovered Pavel Surmenok which has a post dedicated to this subject, using Fastai! As Pavel&#39;s implementation was using the first version of Fastai and I wanted to get more familiar with this library I thought it would be a good idea to try to implement a version of his code using the newest version of Fastai (version 2.5.3 at the time of doing the notebook). So the objectives are the following: . Get hands-on experience with Fastai to apply the theory I know of deep learning. I think it is a great tool for rapid testing with impressive results. Start building things! | Best case scenario, being able to contribute to updating the work that Pavel presented in his blog and help anyone who wants to start with Fastai v2. | Try to explain things I didn&#39;t know before and I would like to find somewhere. The aim of this is to consolidate the knowledge and also help anyone that had the same questions. | . Imports . import fastai fastai.__version__ # check that we have Fastai v2 . &#39;2.5.6&#39; . Show the collapsed code to see all the imports we will need . from fastai.imports import * from fastai.basics import * from fastai.vision.all import * from fastai.callback.all import * from fastai.vision.core import PILImage from PIL import ImageOps import matplotlib.pyplot as plt import csv from collections import defaultdict, namedtuple import os import shutil import pandas as pd from sklearn.metrics import confusion_matrix, f1_score import torch import time import pandas as pd . . We will use the following variable to tell Pytorch to allocate the tensors into the GPU if we have access to one. If not, all the processing will be made on the CPU (NOT recommended, it will be very slow) . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . Download the datasets . . Warning: Check the download links in case the dataset has been moved. For example, the links provided in Pavel&#8217;s blog post are no longer working. You can find all the versions released here. . # Download and unpack the training set and the test set ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P data ! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P data ! unzip data/GTSRB_Final_Training_Images.zip -d data ! unzip data/GTSRB_Final_Test_Images.zip -d data ! unzip data/GTSRB_Final_Test_GT.zip -d data # Move the test set to data/test ! mkdir data/test ! mv data/GTSRB/Final_Test/Images/*.ppm data/test # Download class names ! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P data . . The following are some functions and code used to organize and divide our data into training, validation and test set. . Tip: It is not directly related to Fastai or Deep Learning, but knowing how to manage your data is VERY important, and Pavel makes it in an understandable and elegant way. If you don&#8217;t have a lot of confidence working with data this is a section worth looking at. . Annotations will be named tuples containing the name of the file and its corresponding label. You can find a complete tutorial on namedtuple() here, but basically, those are tuple-like immutable data structures with named fields. . Annotation = namedtuple(&#39;Annotation&#39;, [&#39;filename&#39;, &#39;label&#39;]) def read_annotations(filename): annotations = [] with open(filename) as f: reader = csv.reader(f, delimiter=&#39;;&#39;) next(reader) # skip header # loop over all images in current annotations file for row in reader: filename = row[0] # filename is in the 0th column label = int(row[7]) # label is in the 7th column annotations.append(Annotation(filename, label)) return annotations . load_training_annotations will loop over all the files from all the classes and returns the annotations of all the training examples. . def load_training_annotations(source_path): annotations = [] for c in range(0,43): filename = os.path.join(source_path, format(c, &#39;05d&#39;), &#39;GT-&#39; + format(c, &#39;05d&#39;) + &#39;.csv&#39;) annotations.extend(read_annotations(filename)) return annotations . . Note: my_list.extend(iter) will add each element of an iterable,iter, to the list my_list. If we would have used append we would get a list of list of named tuples instead of a list of named tuples. . We have talked about the filenames, labels etc. but our actual data are images. We will use the copy_files function to organize the images in training, validation and all (training+validation) folders. . def copy_files(label, filenames, source, destination, move=False): # copy to the training or validation folders, for the all folder just rename func = os.rename if move else shutil.copyfile # make a directory for every label label_path = os.path.join(destination, str(label)) if not os.path.exists(label_path): os.makedirs(label_path) # fill the directories with its corresponding files for filename in filenames: destination_path = os.path.join(label_path, filename) if not os.path.exists(destination_path): func(os.path.join(source, format(label, &#39;05d&#39;), filename), destination_path) . Finally, we use all the above mentioned to build our own split_train_validation_sets function. . def split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction=0.2): &quot;&quot;&quot; Splits the GTSRB training set into training and validation sets. &quot;&quot;&quot; if not os.path.exists(train_path): os.makedirs(train_path) if not os.path.exists(validation_path): os.makedirs(validation_path) if not os.path.exists(all_path): os.makedirs(all_path) # annotations will be a list of Annotations(filename, label) annotations = load_training_annotations(source_path) # filenames will be a dictionary # keys: label # values: list of file names filenames = defaultdict(list) for annotation in annotations: filenames[annotation.label].append(annotation.filename) # for every label calculate the validation_size and populate the directories for label, filenames in filenames.items(): filenames = sorted(filenames) # get the validation_size, it must be an integer! validation_size = int(len(filenames) // 30 * validation_fraction) * 30 train_filenames = filenames[validation_size:] validation_filenames = filenames[:validation_size] copy_files(label, filenames, source_path, all_path, move=False) copy_files(label, train_filenames, source_path, train_path, move=True) copy_files(label, validation_filenames, source_path, validation_path, move=True) . Once we have all these functions all we have to do is call them with the appropriate paths. . path = &#39;data&#39; source_path = os.path.join(path, &#39;GTSRB/Final_Training/Images&#39;) train_path = os.path.join(path, &#39;train&#39;) validation_path = os.path.join(path, &#39;valid&#39;) all_path = os.path.join(path, &#39;all&#39;) validation_fraction = 0.2 split_train_validation_sets(source_path, train_path, validation_path, all_path, validation_fraction) test_annotations = read_annotations(&#39;data/GT-final_test.csv&#39;) . Exploratory analisys . We have our data prepared so, first things first, let&#39;s take a look at it. . classes = pd.read_csv(&#39;data/signnames.csv&#39;) class_names = {} for i, row in classes.iterrows(): class_names[str(row[0])] = row[1] classes . ClassId SignName . 0 0 | Speed limit (20km/h) | . 1 1 | Speed limit (30km/h) | . 2 2 | Speed limit (50km/h) | . 3 3 | Speed limit (60km/h) | . 4 4 | Speed limit (70km/h) | . 5 5 | Speed limit (80km/h) | . 6 6 | End of speed limit (80km/h) | . 7 7 | Speed limit (100km/h) | . 8 8 | Speed limit (120km/h) | . 9 9 | No passing | . 10 10 | No passing for vechiles over 3.5 metric tons | . 11 11 | Right-of-way at the next intersection | . 12 12 | Priority road | . 13 13 | Yield | . 14 14 | Stop | . 15 15 | No vechiles | . 16 16 | Vechiles over 3.5 metric tons prohibited | . 17 17 | No entry | . 18 18 | General caution | . 19 19 | Dangerous curve to the left | . 20 20 | Dangerous curve to the right | . 21 21 | Double curve | . 22 22 | Bumpy road | . 23 23 | Slippery road | . 24 24 | Road narrows on the right | . 25 25 | Road work | . 26 26 | Traffic signals | . 27 27 | Pedestrians | . 28 28 | Children crossing | . 29 29 | Bicycles crossing | . 30 30 | Beware of ice/snow | . 31 31 | Wild animals crossing | . 32 32 | End of all speed and passing limits | . 33 33 | Turn right ahead | . 34 34 | Turn left ahead | . 35 35 | Ahead only | . 36 36 | Go straight or right | . 37 37 | Go straight or left | . 38 38 | Keep right | . 39 39 | Keep left | . 40 40 | Roundabout mandatory | . 41 41 | End of no passing | . 42 42 | End of no passing by vechiles over 3.5 metric tons | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We can try to do histogram equalization to see if it improves our results, but in the end, it does not. If you want to try you can use the HistogramEqualization_item class as an item_tfms for the Fastai pipeline. . . Note: You can also try to implement histogram equalization in the way Pavel does in his notebook, but I could not see the changes visually, maybe something has changed in how Fastai opens images. . # Histogram equalization class HistogramEqualization_item(Transform): def init(self, prefix=None): self.prefix = prefix or &#39;&#39; def encodes(self, o): if type(o) == PILImage: ret = ImageOps.equalize(o) else: ret = o return ret def decodes(self, o): return o . . We could start inspecting the data as it is, but Fastai provides us with some useful functions to visualize our data. To use them we will have to integrate this data into Fastai datastructures. We could create a DataBunch or Datasets or go directly to Dataloaders. We will do the last one, but you can find a lot of information in tutotials and documentation. . sz = 96 data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)]) . The item_tfms will be applied to every item in our dataset independently (here is where you have to add the HistogramEqualization_item transform if you want to try). Those are performed in the CPU every time an item is accessed. In our case, we only use Resize. Contrary, the batch_tfms will be applied to all the images in the batch at the same time. Those are done in the GPU and are very fast. You can find more information about aug_transforms in the documentation, but basically is a wrap of very useful transforms for image data. As you may note, we randomly resize again the images. The reason is that this is the resizing we want to apply to our images, the one we did previously in √¨tem_tmfs makes all the images the same size and square, to be able to perform batch_tfms in the GPU. These augmentations are key to having a good performance, and this applies to every deep learning project you will work on. . . Note: We normalize using imagenet_stats because our models will be pretrained on this dataset, which is way larger than the one we are working with. . Now we can use show_batch method to see some examples from our dataset. . data.show_batch(nrows=3, ncols=3) . We can also see how the augmentations affect our data by seeing how those affect to a single example. . data.show_batch(nrows=3, ncols=3, unique=True) . We can also check the length of our training and validation set. . len(data.train_ds), len(data.valid_ds) . (31919, 7290) . . Important: For the sake of the blog post lenght, we finish here our &quot;Exploratory analysis&quot;. Nevertheless, explore the data not only includes watching at the data (which is very important), but also the sizes, class distribution etc. A more complete analysis can be found in Pavel&#8217;s post, and you are encouraged to go further with the help of Fastai. . Training . Explore training . In this section, we will try to find the best training hyperparameters to train the model. To do so we will train on the train data and evaluate on the validation data. Then we will use these hyperparameters to train the model on the whole data available (training + validation) and we will test it with the test data. . The metrics we will track will be accuracy and F1-score, because we have seen that the dataset shows class imbalance. The F1-score metric is more robust and informative about the model performance when we are working with this kind of dataset. Concerning the model, we will use a resnet34 pretrained on imagenet. The data, model, and metrics to track will be wrapped up together in a learner object. . Note: We are passing the weight decay as an argument to the Learner(). By doing so, we will use it when calling both lr_find() and fit_one_cycle() methods in the Explore training section. . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) wd = 5e-4 learn = cnn_learner(data, resnet34, metrics=[accuracy,f1_score_mult], wd=wd) . learn.summary() # Display information about the learner obj . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 566,400 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . . Important: Our model is frozen up to parameter group #2 (the whole model except the last layer), this means that when running backpropagation the gradients won&#8217;t be calculated for those layers, hence their weights will remain unchanged. . One of the most important hyperparameters in deep learning is the learning rate. To find a good one we can use lr_find method, which takes a batch and runs it through the network with incrementally bigger learning rates recording the loss for each run. The idea is to take the learning rate where the graph of the loss is the steepest. You can try to find it visually or let Fastai give you a suggestion (at least in this case it tend to suggest smaller learning rates). . . Warning: The following code cells aim to illustrate the procedure of how to find a good learning rate. In practice we made more experiments, so you are encouraged to change the values in these cells to see how the training behaves. Don&#8217;t be afraid to overfit as this will not be the final model. You can set a large number of iterations to see when it starts overfitting so you can train the final model in a more informed way in the next section. . learn.lr_find() . SuggestedLRs(valley=0.0020892962347716093) . Train the model with the learning rate selected for one epoch. In this epoch, we will only update the weights of the last layer, as the rest of the model is frozen (with the pretrained weights from imagenet). This will allow our last layer to &quot;catch up&quot; with the other ones and not to start randomly. The fit_one_cycle method uses the 1 cycle policy, which is a learning rate schedule policy consisting in starting from a low learning rate in the firsts epochs, increasing it up to a maximum, to decrease it again for the last epochs. A better explanation can be found in this post from Sylvain Gugger. . learn.fit_one_cycle(1, lr_max=0.005) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.875589 | 0.453181 | 0.853498 | 0.848488 | 01:30 | . Then we unfreeze the model to train it completely. With this, gradients will propagate through the whole network updating all the weights. . learn.unfreeze() learn.summary() . Sequential (Input shape: 256 x 3 x 96 x 96) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 256 x 64 x 48 x 48 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 256 x 64 x 24 x 24 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 256 x 128 x 12 x 12 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 256 x 256 x 6 x 6 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 256 x 512 x 3 x 3 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 256 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 256 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 256 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 256 x 43 Linear 22016 True ____________________________________________________________________________ Total params: 21,834,048 Total trainable params: 21,834,048 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fd938a645f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . . Now we have a different training scenario, so probably our learning rate has changed. So we repeat the process of finding the learning rate and training for some epochs. We can use slice when specifying the learning rate to allows us to use &quot;discriminative layer training&quot;, which consists of using different learning rates for different layers. Usually, we use a smaller learning rate for the earlier layers, as they are well trained to detect general features on a lot of data from imagenet, and a bigger one for the last layers, as they have to change more to capture the more specific features from our dataset. . learn.lr_find() . SuggestedLRs(valley=9.120108734350652e-05) . learn.fit_one_cycle(3, lr_max=0.0001) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.296145 | 0.232131 | 0.929218 | 0.925996 | 01:38 | . 1 | 0.155398 | 0.110068 | 0.970370 | 0.967639 | 01:38 | . 2 | 0.105242 | 0.086704 | 0.975034 | 0.972592 | 01:38 | . learn.lr_find() . SuggestedLRs(valley=1.2022644114040304e-05) . learn.fit_one_cycle(7, lr_max=slice(0.0001, 0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.117177 | 0.343552 | 0.925652 | 0.928776 | 01:38 | . 1 | 0.154971 | 0.221667 | 0.945816 | 0.942552 | 01:39 | . 2 | 0.117753 | 0.065744 | 0.982167 | 0.980668 | 01:38 | . 3 | 0.096865 | 0.046334 | 0.988615 | 0.988594 | 01:38 | . 4 | 0.065272 | 0.045523 | 0.986831 | 0.986001 | 01:38 | . 5 | 0.058440 | 0.033738 | 0.990535 | 0.990093 | 01:38 | . 6 | 0.055586 | 0.030030 | 0.990261 | 0.989890 | 01:38 | . Retrain on the whole dataset . Once we have tried different configurations and selected the one we think is best, it is time to train the model with all the data! . f1_score_mult = FBeta(beta=1, average=&#39;weighted&#39;) sz = 96 bs = 256 wd = 5e-4 . data = ImageDataLoaders.from_folder(path, item_tfms=[Resize(sz,order=0)], bs=256, batch_tfms=[*aug_transforms(do_flip=False, max_zoom=1.2, max_rotate=10, max_lighting=0.8, p_lighting=0.8), Normalize.from_stats(*imagenet_stats)], train=&#39;all&#39;) learn = cnn_learner(data, resnet34,metrics=[accuracy,f1_score_mult]) . learn.fit_one_cycle(1, lr_max=0.01) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.542564 | 0.107461 | 0.964746 | 0.964709 | 01:46 | . learn.unfreeze() learn.fit_one_cycle(4, lr_max=slice(0.001,0.01)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.385180 | 0.500579 | 0.864060 | 0.859983 | 01:58 | . 1 | 0.210768 | 0.012989 | 0.995610 | 0.995585 | 01:58 | . 2 | 0.111806 | 0.003372 | 0.998903 | 0.998901 | 01:58 | . 3 | 0.063384 | 0.000662 | 0.999863 | 0.999863 | 01:58 | . learn.fit_one_cycle(6, lr_max=slice(0.0001,0.001)) #, wd=wd) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.059273 | 0.000613 | 0.999863 | 0.999863 | 01:58 | . 1 | 0.059253 | 0.001651 | 0.999451 | 0.999452 | 01:58 | . 2 | 0.058100 | 0.000631 | 0.999726 | 0.999726 | 01:58 | . 3 | 0.048571 | 0.000100 | 1.000000 | 1.000000 | 01:58 | . 4 | 0.042215 | 0.000125 | 1.000000 | 1.000000 | 01:58 | . 5 | 0.038188 | 0.000095 | 1.000000 | 1.000000 | 01:57 | . Test . For the test, we will use &quot;Test time augmentations&quot;. This technique is also used by Pavel and shows an increase in the performance of the model. It consists in, instead of just making the prediction of the given test image, making the prediction of the original and other augmented versions of this image using the augmentations used while training. The final prediction will be an average of all of them. This will take more time for each prediction but it will make them more robust. test_time_aug function will test the model with a different number of augmentations and it will return a Pandas Dataframe with the F1-scores and the time it has taken to run the inference of the test set. . def test_time_aug(learner, test_dataloader, metric, n_augs=[10], beta=0.1): res = [] for aug in n_augs: if aug == 0: start = time.time() log_preds,_ = learner.get_preds(dl=test_dataloader) end = time.time() infer_time = end-start else: start = time.time() log_preds,_ = learn.tta(dl=test_dataloader, n=aug, beta=beta) end = time.time() infer_time = end-start preds = np.argmax(log_preds,1) score = metric(preds, y_true) res.append([aug, score, infer_time]) print(f&#39;N Augmentations: {aug} tF1-score: {score} tTime:{infer_time}&#39;) return pd.DataFrame(res, columns=[&#39;n_aug&#39;, &#39;score&#39;, &#39;time&#39;]) . As we did before, we need the test data to be in a DataLoader format. . true_test_labels = {a.filename: a.label for a in test_annotations} #get the annotations in a dictionary format class_indexes = data.vocab.o2i #dictionary from class to index test_img=get_image_files(&#39;./data/test&#39;) #list of the test image file paths filenames = [filepath.name for filepath in test_img] #get the names from the file paths labels = [str(true_test_labels[filename]) for filename in filenames] #list of the labels for each file y_true = np.array([class_indexes[label] for label in labels]) #array of the index for each label . test_dataloader=data.test_dl(test_img, bs=256, shuffle=False) . len(test_dataloader.dataset) . 12630 . metric = partial(f1_score, average=&#39;weighted&#39;) results = test_time_aug(learn, test_dataloader, metric, n_augs=[0, 3, 5, 10, 15, 20, 30]) . N Augmentations: 0 F1-score: 0.994469328801153 Time:19.947819232940674 . . N Augmentations: 3 F1-score: 0.9943121917876697 Time:81.12456560134888 . . N Augmentations: 5 F1-score: 0.9950268203228465 Time:119.88853621482849 . . N Augmentations: 10 F1-score: 0.9959702387438432 Time:218.91360545158386 . . N Augmentations: 15 F1-score: 0.996130519244009 Time:317.34042143821716 . . N Augmentations: 20 F1-score: 0.996285695102012 Time:414.57182717323303 . . N Augmentations: 30 F1-score: 0.9963671397829769 Time:617.6512229442596 . fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(results[&#39;n_aug&#39;], results[&#39;score&#39;], &#39;g-&#39;) ax2.plot(results[&#39;n_aug&#39;], results[&#39;time&#39;], &#39;b--&#39;) ax1.set_xlabel(&#39;Number Augmentations&#39;) ax1.set_ylabel(&#39;F1 score&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Time (s)&#39;, color=&#39;b&#39;) plt.show() . So we are getting a F1-score of 0.996367. Not bad at all! Who wants an autonomous ride?? ü•≥ üòÜ . Save the model . torch.save(learn.model.state_dict(), &#39;resnet34_weights.pth&#39;) .",
            "url": "https://xanaga.github.io/posts_en/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "relUrl": "/fastai/computer%20vision/traffic%20sign/2022/04/21/ResNet_for_Traffic_Sign_Classification_With_PyTorch.html",
            "date": " ‚Ä¢ Apr 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "First Post",
            "content": "Finally, here we are‚Ä¶ . . The beginning . I have been self-learning about AI and Deep Learning since I started my degree more than three years ago. But lately, I had the feeling of being stuck, even a bit lost. Moreover, the idea of starting to share my work has always been in the back of my head. I think individually we can achieve cool things, but the big changes come from communities. We can find many examples just in the AI field, from Fastai to Hugging Face and many more. I want to be connected with those communities!üåç . Thinking about the best way to do it, I remembered what Jeremy Howard said about the importance of blogging in the second edition of one of the first courses I took. Then I discovered Fastpages, so the decision was taken, I would start blogging! . What you will find in this blog . With this work I want to share my work, the things I learn, and maybe my thoughts. It took me too long to start blogging because I wanted everything to be perfect, but this is not the idea. I will start my blog, it will be dynamic, things will change, there will be mistakes, and it never will be perfect. Beautiful, isn‚Äôt it? . Together with AI (which is my true loveüòç), I‚Äôm trying to get into Computer Graphics, VR/AR, and simulation. For that reason, I am trying to learn about Geometric Deep Learning, Neural Rendering, and also Unreal Engine! . Please remember this is a leaner‚Äôs blog, you are encouraged to help, give suggestions, and correct me if I am wrong. Let‚Äôs do this journey together! . Why not in Spanish? . As you may have noticed, I‚Äôm not a native English speaker. In fact, my mother tongue is Spanish. So, why not write in Spanish. Well, there are a couple of reasons: . I want to practice my English. Learn by doing! | In English, I have access to a broader audience, so more people can help or share their ideas. | And well‚Ä¶ I have never said there won‚Äôt be Spanish postsüòâ I think with different languages we can target different audiences. So if I see some content is redundant or irrelevant in English, I will make it in Spanish. | . So see you in the next one, adi√≥s!üëã .",
            "url": "https://xanaga.github.io/posts_en/general/2022/04/18/first-post.html",
            "relUrl": "/general/2022/04/18/first-post.html",
            "date": " ‚Ä¢ Apr 18, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, my name is Xavi! I am a Computational Maths Bachelor student, passionate about AI and Deep Learning since the first year of my degree. Specifically, I would like to apply the advances in this field to Computer Graphics, VR/AR, simulation, etc. Nevertheless, I love to explore new things, so my interests go from Brain Computer Interfaces to space exploration, ranging everything that includes technology and takes us closer to the future. I also love sports, discovering new countries and eating... I love food! With this blog, I hope to meet more people with my same interest so if you have any suggestions, doubts, ideas, or simply want to chat, don&#39;t think it twice, and drop me a line!üòÑ",
          "url": "https://xanaga.github.io/posts_en/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://xanaga.github.io/posts_en/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}